Couldn't find 'C:\Users\frost\.ollama\id_ed25519'. Generating new private key.
Your new public key is: 

ssh-ed25519 AAAAC3NzaC1lZDI1NTE5AAAAIOLMnmJfu3hBOmLWnPXDT3jCKbTFEWiRvFh8gX8qofCd

time=2025-10-18T01:00:50.369-04:00 level=INFO source=routes.go:1511 msg="server config" env="map[CUDA_VISIBLE_DEVICES: GGML_VK_VISIBLE_DEVICES: GPU_DEVICE_ORDINAL: HIP_VISIBLE_DEVICES: HSA_OVERRIDE_GFX_VERSION: HTTPS_PROXY: HTTP_PROXY: NO_PROXY: OLLAMA_CONTEXT_LENGTH:4096 OLLAMA_DEBUG:INFO OLLAMA_FLASH_ATTENTION:false OLLAMA_GPU_OVERHEAD:0 OLLAMA_HOST:http://127.0.0.1:11434 OLLAMA_INTEL_GPU:false OLLAMA_KEEP_ALIVE:5m0s OLLAMA_KV_CACHE_TYPE: OLLAMA_LLM_LIBRARY: OLLAMA_LOAD_TIMEOUT:5m0s OLLAMA_MAX_LOADED_MODELS:0 OLLAMA_MAX_QUEUE:512 OLLAMA_MODELS:C:\\Users\\frost\\.ollama\\models OLLAMA_MULTIUSER_CACHE:false OLLAMA_NEW_ENGINE:false OLLAMA_NOHISTORY:false OLLAMA_NOPRUNE:false OLLAMA_NUM_PARALLEL:1 OLLAMA_ORIGINS:[http://localhost https://localhost http://localhost:* https://localhost:* http://127.0.0.1 https://127.0.0.1 http://127.0.0.1:* https://127.0.0.1:* http://0.0.0.0 https://0.0.0.0 http://0.0.0.0:* https://0.0.0.0:* app://* file://* tauri://* vscode-webview://* vscode-file://*] OLLAMA_REMOTES:[ollama.com] OLLAMA_SCHED_SPREAD:false ROCR_VISIBLE_DEVICES:]"
time=2025-10-18T01:00:50.370-04:00 level=INFO source=images.go:522 msg="total blobs: 0"
time=2025-10-18T01:00:50.370-04:00 level=INFO source=images.go:529 msg="total unused blobs removed: 0"
time=2025-10-18T01:00:50.370-04:00 level=INFO source=routes.go:1564 msg="Listening on 127.0.0.1:11434 (version 0.12.6)"
time=2025-10-18T01:00:50.371-04:00 level=INFO source=runner.go:80 msg="discovering available GPUs..."
time=2025-10-18T01:01:03.941-04:00 level=INFO source=types.go:112 msg="inference compute" id=GPU-b28645fa-aaf6-86a1-ab1b-ef8d61df29f2 library=CUDA compute=12.0 name=CUDA0 description="NVIDIA GeForce RTX 5070" libdirs=ollama,cuda_v12 driver=12.9 pci_id=01:00.0 type=discrete total="11.9 GiB" available="10.7 GiB"
time=2025-10-18T01:01:03.942-04:00 level=INFO source=routes.go:1605 msg="entering low vram mode" "total vram"="11.9 GiB" threshold="20.0 GiB"
[GIN] 2025/10/18 - 01:01:03 | 200 |            0s |       127.0.0.1 | GET      "/api/version"
[GIN] 2025/10/18 - 01:01:03 | 200 |            0s |       127.0.0.1 | GET      "/api/version"
[GIN] 2025/10/18 - 01:01:03 | 200 |      1.1734ms |       127.0.0.1 | GET      "/api/tags"
[GIN] 2025/10/18 - 01:01:55 | 200 |            0s |       127.0.0.1 | GET      "/api/version"
[GIN] 2025/10/18 - 01:03:34 | 200 |            0s |       127.0.0.1 | GET      "/"
[GIN] 2025/10/18 - 01:03:34 | 404 |            0s |       127.0.0.1 | GET      "/favicon.ico"
[GIN] 2025/10/18 - 01:04:56 | 200 |            0s |       127.0.0.1 | HEAD     "/"
time=2025-10-18T01:04:57.155-04:00 level=INFO source=download.go:177 msg="downloading 633fc5be925f in 16 136 MB part(s)"
time=2025-10-18T01:06:43.179-04:00 level=INFO source=download.go:177 msg="downloading fa8235e5b48f in 1 1.1 KB part(s)"
time=2025-10-18T01:06:44.444-04:00 level=INFO source=download.go:177 msg="downloading 542b217f179c in 1 148 B part(s)"
time=2025-10-18T01:06:45.696-04:00 level=INFO source=download.go:177 msg="downloading 8dde1baf1db0 in 1 78 B part(s)"
time=2025-10-18T01:06:46.863-04:00 level=INFO source=download.go:177 msg="downloading 23291dc44752 in 1 483 B part(s)"
[GIN] 2025/10/18 - 01:06:50 | 200 |         1m54s |       127.0.0.1 | POST     "/api/pull"
[GIN] 2025/10/18 - 01:06:50 | 200 |            0s |       127.0.0.1 | HEAD     "/"
time=2025-10-18T01:06:51.421-04:00 level=INFO source=download.go:177 msg="downloading f5074b1221da in 16 273 MB part(s)"
time=2025-10-18T01:09:11.478-04:00 level=INFO source=download.go:374 msg="f5074b1221da part 11 stalled; retrying. If this persists, press ctrl-c to exit, then 'ollama pull' to find a faster connection."
time=2025-10-18T01:10:43.937-04:00 level=INFO source=download.go:295 msg="f5074b1221da part 2 attempt 0 failed: unexpected EOF, retrying in 1s"
time=2025-10-18T01:18:25.472-04:00 level=INFO source=download.go:374 msg="f5074b1221da part 3 stalled; retrying. If this persists, press ctrl-c to exit, then 'ollama pull' to find a faster connection."
time=2025-10-18T01:22:03.471-04:00 level=INFO source=download.go:374 msg="f5074b1221da part 13 stalled; retrying. If this persists, press ctrl-c to exit, then 'ollama pull' to find a faster connection."
time=2025-10-18T01:23:20.471-04:00 level=INFO source=download.go:374 msg="f5074b1221da part 13 stalled; retrying. If this persists, press ctrl-c to exit, then 'ollama pull' to find a faster connection."
time=2025-10-18T01:24:05.483-04:00 level=INFO source=download.go:374 msg="f5074b1221da part 0 stalled; retrying. If this persists, press ctrl-c to exit, then 'ollama pull' to find a faster connection."
time=2025-10-18T01:25:03.470-04:00 level=INFO source=download.go:374 msg="f5074b1221da part 4 stalled; retrying. If this persists, press ctrl-c to exit, then 'ollama pull' to find a faster connection."
time=2025-10-18T01:28:11.471-04:00 level=INFO source=download.go:374 msg="f5074b1221da part 9 stalled; retrying. If this persists, press ctrl-c to exit, then 'ollama pull' to find a faster connection."
time=2025-10-18T01:28:27.104-04:00 level=INFO source=download.go:295 msg="f5074b1221da part 7 attempt 0 failed: unexpected EOF, retrying in 1s"
time=2025-10-18T01:43:15.481-04:00 level=INFO source=download.go:374 msg="f5074b1221da part 4 stalled; retrying. If this persists, press ctrl-c to exit, then 'ollama pull' to find a faster connection."
time=2025-10-18T01:47:19.196-04:00 level=INFO source=download.go:177 msg="downloading 43070e2d4e53 in 1 11 KB part(s)"
time=2025-10-18T01:47:20.339-04:00 level=INFO source=download.go:177 msg="downloading 1ff5b64b61b9 in 1 799 B part(s)"
time=2025-10-18T01:47:21.543-04:00 level=INFO source=download.go:177 msg="downloading ed11eda7790d in 1 30 B part(s)"
time=2025-10-18T01:47:22.678-04:00 level=INFO source=download.go:177 msg="downloading 1064e17101bd in 1 487 B part(s)"
[GIN] 2025/10/18 - 01:47:33 | 200 |        40m42s |       127.0.0.1 | POST     "/api/pull"
[GIN] 2025/10/18 - 01:47:33 | 200 |            0s |       127.0.0.1 | HEAD     "/"
time=2025-10-18T01:47:33.751-04:00 level=INFO source=download.go:177 msg="downloading 3a43f93b78ec in 16 239 MB part(s)"
time=2025-10-18T01:50:13.528-04:00 level=INFO source=download.go:177 msg="downloading 8c17c2ebb0ea in 1 7.0 KB part(s)"
time=2025-10-18T01:50:14.662-04:00 level=INFO source=download.go:177 msg="downloading 590d74a5569b in 1 4.8 KB part(s)"
time=2025-10-18T01:50:15.777-04:00 level=INFO source=download.go:177 msg="downloading 2e0493f67d0c in 1 59 B part(s)"
time=2025-10-18T01:50:16.922-04:00 level=INFO source=download.go:177 msg="downloading 7f6a57943a88 in 1 120 B part(s)"
time=2025-10-18T01:50:18.095-04:00 level=INFO source=download.go:177 msg="downloading 316526ac7323 in 1 529 B part(s)"
[GIN] 2025/10/18 - 01:50:25 | 200 |         2m52s |       127.0.0.1 | POST     "/api/pull"
[GIN] 2025/10/18 - 02:20:37 | 200 |            0s |       127.0.0.1 | HEAD     "/"
[GIN] 2025/10/18 - 02:20:38 | 200 |    543.8666ms |       127.0.0.1 | POST     "/api/create"
[GIN] 2025/10/18 - 02:20:38 | 200 |            0s |       127.0.0.1 | HEAD     "/"
[GIN] 2025/10/18 - 02:20:38 | 200 |     20.9325ms |       127.0.0.1 | POST     "/api/show"
time=2025-10-18T02:20:41.216-04:00 level=INFO source=runner.go:545 msg="failure during GPU discovery" OLLAMA_LIBRARY_PATH="[C:\\Users\\frost\\AppData\\Local\\Programs\\Ollama\\lib\\ollama C:\\Users\\frost\\AppData\\Local\\Programs\\Ollama\\lib\\ollama\\cuda_v12]" extra_envs=[] error="failed to finish discovery before timeout"
time=2025-10-18T02:20:41.216-04:00 level=WARN source=runner.go:347 msg="unable to refresh free memory, using old values"
llama_model_loader: loaded meta data with 25 key-value pairs and 291 tensors from C:\Users\frost\.ollama\models\blobs\sha256-f5074b1221da0f5a2910d33b642efa5b9eb58cfdddca1c79e16d7ad28aa2b31f (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = llama
llama_model_loader: - kv   1:                               general.name str              = Mistral-7B-Instruct-v0.3
llama_model_loader: - kv   2:                          llama.block_count u32              = 32
llama_model_loader: - kv   3:                       llama.context_length u32              = 32768
llama_model_loader: - kv   4:                     llama.embedding_length u32              = 4096
llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 14336
llama_model_loader: - kv   6:                 llama.attention.head_count u32              = 32
llama_model_loader: - kv   7:              llama.attention.head_count_kv u32              = 8
llama_model_loader: - kv   8:                       llama.rope.freq_base f32              = 1000000.000000
llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010
llama_model_loader: - kv  10:                          general.file_type u32              = 15
llama_model_loader: - kv  11:                           llama.vocab_size u32              = 32768
llama_model_loader: - kv  12:                 llama.rope.dimension_count u32              = 128
llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = llama
llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = default
llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,32768]   = ["<unk>", "<s>", "</s>", "[INST]", "[...
llama_model_loader: - kv  16:                      tokenizer.ggml.scores arr[f32,32768]   = [0.000000, 0.000000, 0.000000, 0.0000...
llama_model_loader: - kv  17:                  tokenizer.ggml.token_type arr[i32,32768]   = [2, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, ...
llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 1
llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 2
llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
llama_model_loader: - kv  21:               tokenizer.ggml.add_bos_token bool             = true
llama_model_loader: - kv  22:               tokenizer.ggml.add_eos_token bool             = false
llama_model_loader: - kv  23:                    tokenizer.chat_template str              = {{ bos_token }}{% for message in mess...
llama_model_loader: - kv  24:               general.quantization_version u32              = 2
llama_model_loader: - type  f32:   65 tensors
llama_model_loader: - type q4_K:  193 tensors
llama_model_loader: - type q6_K:   33 tensors
print_info: file format = GGUF V3 (latest)
print_info: file type   = Q4_K - Medium
print_info: file size   = 4.07 GiB (4.83 BPW) 
load: special_eos_id is not in special_eog_ids - the tokenizer config may be incorrect
load: printing all EOG tokens:
load:   - 2 ('</s>')
load: special tokens cache size = 771
load: token to piece cache size = 0.1731 MB
print_info: arch             = llama
print_info: vocab_only       = 1
print_info: model type       = ?B
print_info: model params     = 7.25 B
print_info: general.name     = Mistral-7B-Instruct-v0.3
print_info: vocab type       = SPM
print_info: n_vocab          = 32768
print_info: n_merges         = 0
print_info: BOS token        = 1 '<s>'
print_info: EOS token        = 2 '</s>'
print_info: UNK token        = 0 '<unk>'
print_info: LF token         = 781 '<0x0A>'
print_info: EOG token        = 2 '</s>'
print_info: max token length = 48
llama_model_load: vocab only - skipping tensors
time=2025-10-18T02:20:41.275-04:00 level=INFO source=cpu_windows.go:139 msg=packages count=1
time=2025-10-18T02:20:41.276-04:00 level=INFO source=cpu_windows.go:155 msg="efficiency cores detected" maxEfficiencyClass=1
time=2025-10-18T02:20:41.276-04:00 level=INFO source=cpu_windows.go:186 msg="" package=0 cores=12 efficiency=4 threads=20
time=2025-10-18T02:20:41.277-04:00 level=INFO source=server.go:400 msg="starting runner" cmd="C:\\Users\\frost\\AppData\\Local\\Programs\\Ollama\\ollama.exe runner --model C:\\Users\\frost\\.ollama\\models\\blobs\\sha256-f5074b1221da0f5a2910d33b642efa5b9eb58cfdddca1c79e16d7ad28aa2b31f --port 54729"
time=2025-10-18T02:20:41.286-04:00 level=INFO source=cpu_windows.go:139 msg=packages count=1
time=2025-10-18T02:20:41.286-04:00 level=INFO source=cpu_windows.go:155 msg="efficiency cores detected" maxEfficiencyClass=1
time=2025-10-18T02:20:41.286-04:00 level=INFO source=cpu_windows.go:186 msg="" package=0 cores=12 efficiency=4 threads=20
time=2025-10-18T02:20:41.286-04:00 level=INFO source=server.go:505 msg="system memory" total="63.8 GiB" free="43.8 GiB" free_swap="44.3 GiB"
time=2025-10-18T02:20:41.286-04:00 level=INFO source=memory.go:36 msg="new model will fit in available VRAM across minimum required GPUs, loading" model=C:\Users\frost\.ollama\models\blobs\sha256-f5074b1221da0f5a2910d33b642efa5b9eb58cfdddca1c79e16d7ad28aa2b31f library=CUDA parallel=1 required="5.4 GiB" gpus=1
time=2025-10-18T02:20:41.286-04:00 level=INFO source=server.go:545 msg=offload library=CUDA layers.requested=-1 layers.model=33 layers.offload=33 layers.split=[33] memory.available="[10.7 GiB]" memory.gpu_overhead="0 B" memory.required.full="5.4 GiB" memory.required.partial="5.4 GiB" memory.required.kv="512.0 MiB" memory.required.allocations="[5.4 GiB]" memory.weights.total="4.0 GiB" memory.weights.repeating="3.9 GiB" memory.weights.nonrepeating="105.0 MiB" memory.graph.full="296.0 MiB" memory.graph.partial="305.0 MiB"
time=2025-10-18T02:20:41.346-04:00 level=INFO source=runner.go:893 msg="starting go runner"
load_backend: loaded CPU backend from C:\Users\frost\AppData\Local\Programs\Ollama\lib\ollama\ggml-cpu-alderlake.dll
ggml_cuda_init: GGML_CUDA_FORCE_MMQ:    no
ggml_cuda_init: GGML_CUDA_FORCE_CUBLAS: no
ggml_cuda_init: found 1 CUDA devices:
  Device 0: NVIDIA GeForce RTX 5070, compute capability 12.0, VMM: yes, ID: GPU-b28645fa-aaf6-86a1-ab1b-ef8d61df29f2
load_backend: loaded CUDA backend from C:\Users\frost\AppData\Local\Programs\Ollama\lib\ollama\cuda_v12\ggml-cuda.dll
time=2025-10-18T02:20:41.488-04:00 level=INFO source=ggml.go:104 msg=system CPU.0.SSE3=1 CPU.0.SSSE3=1 CPU.0.AVX=1 CPU.0.AVX_VNNI=1 CPU.0.AVX2=1 CPU.0.F16C=1 CPU.0.FMA=1 CPU.0.BMI2=1 CPU.0.LLAMAFILE=1 CPU.1.LLAMAFILE=1 CUDA.0.ARCHS=500,520,600,610,700,750,800,860,870,890,900,1200 CUDA.0.USE_GRAPHS=1 CUDA.0.PEER_MAX_BATCH_SIZE=128 compiler=cgo(clang)
time=2025-10-18T02:20:41.489-04:00 level=INFO source=runner.go:929 msg="Server listening on 127.0.0.1:54729"
time=2025-10-18T02:20:41.493-04:00 level=INFO source=server.go:1272 msg="waiting for llama runner to start responding"
time=2025-10-18T02:20:41.492-04:00 level=INFO source=runner.go:828 msg=load request="{Operation:commit LoraPath:[] Parallel:1 BatchSize:512 FlashAttention:false KvSize:4096 KvCacheType: NumThreads:8 GPULayers:33[ID:GPU-b28645fa-aaf6-86a1-ab1b-ef8d61df29f2 Layers:33(0..32)] MultiUserCache:false ProjectorPath: MainGPU:0 UseMmap:false}"
time=2025-10-18T02:20:41.494-04:00 level=INFO source=server.go:1306 msg="waiting for server to become available" status="llm server loading model"
ggml_backend_cuda_device_get_memory utilizing NVML memory reporting free: 11680669696 total: 12820938752
llama_model_load_from_file_impl: using device CUDA0 (NVIDIA GeForce RTX 5070) (0000:01:00.0) - 11139 MiB free
llama_model_loader: loaded meta data with 25 key-value pairs and 291 tensors from C:\Users\frost\.ollama\models\blobs\sha256-f5074b1221da0f5a2910d33b642efa5b9eb58cfdddca1c79e16d7ad28aa2b31f (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = llama
llama_model_loader: - kv   1:                               general.name str              = Mistral-7B-Instruct-v0.3
llama_model_loader: - kv   2:                          llama.block_count u32              = 32
llama_model_loader: - kv   3:                       llama.context_length u32              = 32768
llama_model_loader: - kv   4:                     llama.embedding_length u32              = 4096
llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 14336
llama_model_loader: - kv   6:                 llama.attention.head_count u32              = 32
llama_model_loader: - kv   7:              llama.attention.head_count_kv u32              = 8
llama_model_loader: - kv   8:                       llama.rope.freq_base f32              = 1000000.000000
llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010
llama_model_loader: - kv  10:                          general.file_type u32              = 15
llama_model_loader: - kv  11:                           llama.vocab_size u32              = 32768
llama_model_loader: - kv  12:                 llama.rope.dimension_count u32              = 128
llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = llama
llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = default
llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,32768]   = ["<unk>", "<s>", "</s>", "[INST]", "[...
llama_model_loader: - kv  16:                      tokenizer.ggml.scores arr[f32,32768]   = [0.000000, 0.000000, 0.000000, 0.0000...
llama_model_loader: - kv  17:                  tokenizer.ggml.token_type arr[i32,32768]   = [2, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, ...
llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 1
llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 2
llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
llama_model_loader: - kv  21:               tokenizer.ggml.add_bos_token bool             = true
llama_model_loader: - kv  22:               tokenizer.ggml.add_eos_token bool             = false
llama_model_loader: - kv  23:                    tokenizer.chat_template str              = {{ bos_token }}{% for message in mess...
llama_model_loader: - kv  24:               general.quantization_version u32              = 2
llama_model_loader: - type  f32:   65 tensors
llama_model_loader: - type q4_K:  193 tensors
llama_model_loader: - type q6_K:   33 tensors
print_info: file format = GGUF V3 (latest)
print_info: file type   = Q4_K - Medium
print_info: file size   = 4.07 GiB (4.83 BPW) 
load: special_eos_id is not in special_eog_ids - the tokenizer config may be incorrect
load: printing all EOG tokens:
load:   - 2 ('</s>')
load: special tokens cache size = 771
load: token to piece cache size = 0.1731 MB
print_info: arch             = llama
print_info: vocab_only       = 0
print_info: n_ctx_train      = 32768
print_info: n_embd           = 4096
print_info: n_layer          = 32
print_info: n_head           = 32
print_info: n_head_kv        = 8
print_info: n_rot            = 128
print_info: n_swa            = 0
print_info: is_swa_any       = 0
print_info: n_embd_head_k    = 128
print_info: n_embd_head_v    = 128
print_info: n_gqa            = 4
print_info: n_embd_k_gqa     = 1024
print_info: n_embd_v_gqa     = 1024
print_info: f_norm_eps       = 0.0e+00
print_info: f_norm_rms_eps   = 1.0e-05
print_info: f_clamp_kqv      = 0.0e+00
print_info: f_max_alibi_bias = 0.0e+00
print_info: f_logit_scale    = 0.0e+00
print_info: f_attn_scale     = 0.0e+00
print_info: n_ff             = 14336
print_info: n_expert         = 0
print_info: n_expert_used    = 0
print_info: causal attn      = 1
print_info: pooling type     = 0
print_info: rope type        = 0
print_info: rope scaling     = linear
print_info: freq_base_train  = 1000000.0
print_info: freq_scale_train = 1
print_info: n_ctx_orig_yarn  = 32768
print_info: rope_finetuned   = unknown
print_info: model type       = 7B
print_info: model params     = 7.25 B
print_info: general.name     = Mistral-7B-Instruct-v0.3
print_info: vocab type       = SPM
print_info: n_vocab          = 32768
print_info: n_merges         = 0
print_info: BOS token        = 1 '<s>'
print_info: EOS token        = 2 '</s>'
print_info: UNK token        = 0 '<unk>'
print_info: LF token         = 781 '<0x0A>'
print_info: EOG token        = 2 '</s>'
print_info: max token length = 48
load_tensors: loading model tensors, this can take a while... (mmap = false)
load_tensors: offloading 32 repeating layers to GPU
load_tensors: offloading output layer to GPU
load_tensors: offloaded 33/33 layers to GPU
load_tensors:        CUDA0 model buffer size =  4097.52 MiB
load_tensors:          CPU model buffer size =    72.00 MiB
llama_init_from_model: model default pooling_type is [0], but [-1] was specified
llama_context: constructing llama_context
llama_context: n_seq_max     = 1
llama_context: n_ctx         = 4096
llama_context: n_ctx_per_seq = 4096
llama_context: n_batch       = 512
llama_context: n_ubatch      = 512
llama_context: causal_attn   = 1
llama_context: flash_attn    = disabled
llama_context: kv_unified    = false
llama_context: freq_base     = 1000000.0
llama_context: freq_scale    = 1
llama_context: n_ctx_per_seq (4096) < n_ctx_train (32768) -- the full capacity of the model will not be utilized
llama_context:  CUDA_Host  output buffer size =     0.14 MiB
llama_kv_cache:      CUDA0 KV buffer size =   512.00 MiB
llama_kv_cache: size =  512.00 MiB (  4096 cells,  32 layers,  1/1 seqs), K (f16):  256.00 MiB, V (f16):  256.00 MiB
llama_context:      CUDA0 compute buffer size =   300.01 MiB
llama_context:  CUDA_Host compute buffer size =    20.01 MiB
llama_context: graph nodes  = 1158
llama_context: graph splits = 2
time=2025-10-18T02:20:45.016-04:00 level=INFO source=server.go:1310 msg="llama runner started in 3.74 seconds"
time=2025-10-18T02:20:45.016-04:00 level=INFO source=sched.go:482 msg="loaded runners" count=1
time=2025-10-18T02:20:45.016-04:00 level=INFO source=server.go:1272 msg="waiting for llama runner to start responding"
time=2025-10-18T02:20:45.016-04:00 level=INFO source=server.go:1310 msg="llama runner started in 3.74 seconds"
[GIN] 2025/10/18 - 02:20:45 | 200 |    6.8370927s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/10/18 - 02:21:52 | 200 |            0s |       127.0.0.1 | GET      "/api/version"
[GIN] 2025/10/18 - 02:21:52 | 200 |            0s |       127.0.0.1 | GET      "/api/version"
[GIN] 2025/10/18 - 02:21:52 | 200 |      28.541ms |       127.0.0.1 | GET      "/api/tags"
[GIN] 2025/10/18 - 02:21:52 | 404 |       515.3µs |       127.0.0.1 | POST     "/api/show"
[GIN] 2025/10/18 - 02:22:45 | 200 |            0s |       127.0.0.1 | HEAD     "/"
[GIN] 2025/10/18 - 02:22:45 | 200 |     14.4044ms |       127.0.0.1 | POST     "/api/show"
[GIN] 2025/10/18 - 02:22:45 | 200 |      27.438ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/10/18 - 02:24:01 | 200 |     5.437035s |       127.0.0.1 | POST     "/api/chat"
time=2025-10-18T05:03:57.737-04:00 level=INFO source=runner.go:545 msg="failure during GPU discovery" OLLAMA_LIBRARY_PATH="[C:\\Users\\frost\\AppData\\Local\\Programs\\Ollama\\lib\\ollama C:\\Users\\frost\\AppData\\Local\\Programs\\Ollama\\lib\\ollama\\cuda_v12]" extra_envs=[] error="failed to finish discovery before timeout"
time=2025-10-18T05:03:57.737-04:00 level=WARN source=runner.go:347 msg="unable to refresh free memory, using old values"
llama_model_loader: loaded meta data with 25 key-value pairs and 291 tensors from C:\Users\frost\.ollama\models\blobs\sha256-f5074b1221da0f5a2910d33b642efa5b9eb58cfdddca1c79e16d7ad28aa2b31f (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = llama
llama_model_loader: - kv   1:                               general.name str              = Mistral-7B-Instruct-v0.3
llama_model_loader: - kv   2:                          llama.block_count u32              = 32
llama_model_loader: - kv   3:                       llama.context_length u32              = 32768
llama_model_loader: - kv   4:                     llama.embedding_length u32              = 4096
llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 14336
llama_model_loader: - kv   6:                 llama.attention.head_count u32              = 32
llama_model_loader: - kv   7:              llama.attention.head_count_kv u32              = 8
llama_model_loader: - kv   8:                       llama.rope.freq_base f32              = 1000000.000000
llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010
llama_model_loader: - kv  10:                          general.file_type u32              = 15
llama_model_loader: - kv  11:                           llama.vocab_size u32              = 32768
llama_model_loader: - kv  12:                 llama.rope.dimension_count u32              = 128
llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = llama
llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = default
llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,32768]   = ["<unk>", "<s>", "</s>", "[INST]", "[...
llama_model_loader: - kv  16:                      tokenizer.ggml.scores arr[f32,32768]   = [0.000000, 0.000000, 0.000000, 0.0000...
llama_model_loader: - kv  17:                  tokenizer.ggml.token_type arr[i32,32768]   = [2, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, ...
llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 1
llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 2
llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
llama_model_loader: - kv  21:               tokenizer.ggml.add_bos_token bool             = true
llama_model_loader: - kv  22:               tokenizer.ggml.add_eos_token bool             = false
llama_model_loader: - kv  23:                    tokenizer.chat_template str              = {{ bos_token }}{% for message in mess...
llama_model_loader: - kv  24:               general.quantization_version u32              = 2
llama_model_loader: - type  f32:   65 tensors
llama_model_loader: - type q4_K:  193 tensors
llama_model_loader: - type q6_K:   33 tensors
print_info: file format = GGUF V3 (latest)
print_info: file type   = Q4_K - Medium
print_info: file size   = 4.07 GiB (4.83 BPW) 
load: special_eos_id is not in special_eog_ids - the tokenizer config may be incorrect
load: printing all EOG tokens:
load:   - 2 ('</s>')
load: special tokens cache size = 771
load: token to piece cache size = 0.1731 MB
print_info: arch             = llama
print_info: vocab_only       = 1
print_info: model type       = ?B
print_info: model params     = 7.25 B
print_info: general.name     = Mistral-7B-Instruct-v0.3
print_info: vocab type       = SPM
print_info: n_vocab          = 32768
print_info: n_merges         = 0
print_info: BOS token        = 1 '<s>'
print_info: EOS token        = 2 '</s>'
print_info: UNK token        = 0 '<unk>'
print_info: LF token         = 781 '<0x0A>'
print_info: EOG token        = 2 '</s>'
print_info: max token length = 48
llama_model_load: vocab only - skipping tensors
time=2025-10-18T05:03:57.792-04:00 level=INFO source=cpu_windows.go:139 msg=packages count=1
time=2025-10-18T05:03:57.792-04:00 level=INFO source=cpu_windows.go:155 msg="efficiency cores detected" maxEfficiencyClass=1
time=2025-10-18T05:03:57.792-04:00 level=INFO source=cpu_windows.go:186 msg="" package=0 cores=12 efficiency=4 threads=20
time=2025-10-18T05:03:57.794-04:00 level=INFO source=server.go:400 msg="starting runner" cmd="C:\\Users\\frost\\AppData\\Local\\Programs\\Ollama\\ollama.exe runner --model C:\\Users\\frost\\.ollama\\models\\blobs\\sha256-f5074b1221da0f5a2910d33b642efa5b9eb58cfdddca1c79e16d7ad28aa2b31f --port 54901"
time=2025-10-18T05:03:57.800-04:00 level=INFO source=cpu_windows.go:139 msg=packages count=1
time=2025-10-18T05:03:57.800-04:00 level=INFO source=cpu_windows.go:155 msg="efficiency cores detected" maxEfficiencyClass=1
time=2025-10-18T05:03:57.800-04:00 level=INFO source=cpu_windows.go:186 msg="" package=0 cores=12 efficiency=4 threads=20
time=2025-10-18T05:03:57.800-04:00 level=INFO source=server.go:505 msg="system memory" total="63.8 GiB" free="42.5 GiB" free_swap="43.0 GiB"
time=2025-10-18T05:03:57.800-04:00 level=INFO source=memory.go:36 msg="new model will fit in available VRAM across minimum required GPUs, loading" model=C:\Users\frost\.ollama\models\blobs\sha256-f5074b1221da0f5a2910d33b642efa5b9eb58cfdddca1c79e16d7ad28aa2b31f library=CUDA parallel=1 required="5.4 GiB" gpus=1
time=2025-10-18T05:03:57.801-04:00 level=INFO source=server.go:545 msg=offload library=CUDA layers.requested=-1 layers.model=33 layers.offload=33 layers.split=[33] memory.available="[10.8 GiB]" memory.gpu_overhead="0 B" memory.required.full="5.4 GiB" memory.required.partial="5.4 GiB" memory.required.kv="512.0 MiB" memory.required.allocations="[5.4 GiB]" memory.weights.total="4.0 GiB" memory.weights.repeating="3.9 GiB" memory.weights.nonrepeating="105.0 MiB" memory.graph.full="296.0 MiB" memory.graph.partial="305.0 MiB"
time=2025-10-18T05:03:57.877-04:00 level=INFO source=runner.go:893 msg="starting go runner"
load_backend: loaded CPU backend from C:\Users\frost\AppData\Local\Programs\Ollama\lib\ollama\ggml-cpu-alderlake.dll
ggml_cuda_init: GGML_CUDA_FORCE_MMQ:    no
ggml_cuda_init: GGML_CUDA_FORCE_CUBLAS: no
ggml_cuda_init: found 1 CUDA devices:
  Device 0: NVIDIA GeForce RTX 5070, compute capability 12.0, VMM: yes, ID: GPU-b28645fa-aaf6-86a1-ab1b-ef8d61df29f2
load_backend: loaded CUDA backend from C:\Users\frost\AppData\Local\Programs\Ollama\lib\ollama\cuda_v12\ggml-cuda.dll
time=2025-10-18T05:03:58.034-04:00 level=INFO source=ggml.go:104 msg=system CPU.0.SSE3=1 CPU.0.SSSE3=1 CPU.0.AVX=1 CPU.0.AVX_VNNI=1 CPU.0.AVX2=1 CPU.0.F16C=1 CPU.0.FMA=1 CPU.0.BMI2=1 CPU.0.LLAMAFILE=1 CPU.1.LLAMAFILE=1 CUDA.0.ARCHS=500,520,600,610,700,750,800,860,870,890,900,1200 CUDA.0.USE_GRAPHS=1 CUDA.0.PEER_MAX_BATCH_SIZE=128 compiler=cgo(clang)
time=2025-10-18T05:03:58.035-04:00 level=INFO source=runner.go:929 msg="Server listening on 127.0.0.1:54901"
time=2025-10-18T05:03:58.043-04:00 level=INFO source=runner.go:828 msg=load request="{Operation:commit LoraPath:[] Parallel:1 BatchSize:512 FlashAttention:false KvSize:4096 KvCacheType: NumThreads:8 GPULayers:33[ID:GPU-b28645fa-aaf6-86a1-ab1b-ef8d61df29f2 Layers:33(0..32)] MultiUserCache:false ProjectorPath: MainGPU:0 UseMmap:false}"
time=2025-10-18T05:03:58.043-04:00 level=INFO source=server.go:1272 msg="waiting for llama runner to start responding"
time=2025-10-18T05:03:58.043-04:00 level=INFO source=server.go:1306 msg="waiting for server to become available" status="llm server loading model"
ggml_backend_cuda_device_get_memory utilizing NVML memory reporting free: 11714195456 total: 12820938752
llama_model_load_from_file_impl: using device CUDA0 (NVIDIA GeForce RTX 5070) (0000:01:00.0) - 11171 MiB free
llama_model_loader: loaded meta data with 25 key-value pairs and 291 tensors from C:\Users\frost\.ollama\models\blobs\sha256-f5074b1221da0f5a2910d33b642efa5b9eb58cfdddca1c79e16d7ad28aa2b31f (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = llama
llama_model_loader: - kv   1:                               general.name str              = Mistral-7B-Instruct-v0.3
llama_model_loader: - kv   2:                          llama.block_count u32              = 32
llama_model_loader: - kv   3:                       llama.context_length u32              = 32768
llama_model_loader: - kv   4:                     llama.embedding_length u32              = 4096
llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 14336
llama_model_loader: - kv   6:                 llama.attention.head_count u32              = 32
llama_model_loader: - kv   7:              llama.attention.head_count_kv u32              = 8
llama_model_loader: - kv   8:                       llama.rope.freq_base f32              = 1000000.000000
llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010
llama_model_loader: - kv  10:                          general.file_type u32              = 15
llama_model_loader: - kv  11:                           llama.vocab_size u32              = 32768
llama_model_loader: - kv  12:                 llama.rope.dimension_count u32              = 128
llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = llama
llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = default
llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,32768]   = ["<unk>", "<s>", "</s>", "[INST]", "[...
llama_model_loader: - kv  16:                      tokenizer.ggml.scores arr[f32,32768]   = [0.000000, 0.000000, 0.000000, 0.0000...
llama_model_loader: - kv  17:                  tokenizer.ggml.token_type arr[i32,32768]   = [2, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, ...
llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 1
llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 2
llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
llama_model_loader: - kv  21:               tokenizer.ggml.add_bos_token bool             = true
llama_model_loader: - kv  22:               tokenizer.ggml.add_eos_token bool             = false
llama_model_loader: - kv  23:                    tokenizer.chat_template str              = {{ bos_token }}{% for message in mess...
llama_model_loader: - kv  24:               general.quantization_version u32              = 2
llama_model_loader: - type  f32:   65 tensors
llama_model_loader: - type q4_K:  193 tensors
llama_model_loader: - type q6_K:   33 tensors
print_info: file format = GGUF V3 (latest)
print_info: file type   = Q4_K - Medium
print_info: file size   = 4.07 GiB (4.83 BPW) 
load: special_eos_id is not in special_eog_ids - the tokenizer config may be incorrect
load: printing all EOG tokens:
load:   - 2 ('</s>')
load: special tokens cache size = 771
load: token to piece cache size = 0.1731 MB
print_info: arch             = llama
print_info: vocab_only       = 0
print_info: n_ctx_train      = 32768
print_info: n_embd           = 4096
print_info: n_layer          = 32
print_info: n_head           = 32
print_info: n_head_kv        = 8
print_info: n_rot            = 128
print_info: n_swa            = 0
print_info: is_swa_any       = 0
print_info: n_embd_head_k    = 128
print_info: n_embd_head_v    = 128
print_info: n_gqa            = 4
print_info: n_embd_k_gqa     = 1024
print_info: n_embd_v_gqa     = 1024
print_info: f_norm_eps       = 0.0e+00
print_info: f_norm_rms_eps   = 1.0e-05
print_info: f_clamp_kqv      = 0.0e+00
print_info: f_max_alibi_bias = 0.0e+00
print_info: f_logit_scale    = 0.0e+00
print_info: f_attn_scale     = 0.0e+00
print_info: n_ff             = 14336
print_info: n_expert         = 0
print_info: n_expert_used    = 0
print_info: causal attn      = 1
print_info: pooling type     = 0
print_info: rope type        = 0
print_info: rope scaling     = linear
print_info: freq_base_train  = 1000000.0
print_info: freq_scale_train = 1
print_info: n_ctx_orig_yarn  = 32768
print_info: rope_finetuned   = unknown
print_info: model type       = 7B
print_info: model params     = 7.25 B
print_info: general.name     = Mistral-7B-Instruct-v0.3
print_info: vocab type       = SPM
print_info: n_vocab          = 32768
print_info: n_merges         = 0
print_info: BOS token        = 1 '<s>'
print_info: EOS token        = 2 '</s>'
print_info: UNK token        = 0 '<unk>'
print_info: LF token         = 781 '<0x0A>'
print_info: EOG token        = 2 '</s>'
print_info: max token length = 48
load_tensors: loading model tensors, this can take a while... (mmap = false)
load_tensors: offloading 32 repeating layers to GPU
load_tensors: offloading output layer to GPU
load_tensors: offloaded 33/33 layers to GPU
load_tensors:        CUDA0 model buffer size =  4097.52 MiB
load_tensors:          CPU model buffer size =    72.00 MiB
llama_init_from_model: model default pooling_type is [0], but [-1] was specified
llama_context: constructing llama_context
llama_context: n_seq_max     = 1
llama_context: n_ctx         = 4096
llama_context: n_ctx_per_seq = 4096
llama_context: n_batch       = 512
llama_context: n_ubatch      = 512
llama_context: causal_attn   = 1
llama_context: flash_attn    = disabled
llama_context: kv_unified    = false
llama_context: freq_base     = 1000000.0
llama_context: freq_scale    = 1
llama_context: n_ctx_per_seq (4096) < n_ctx_train (32768) -- the full capacity of the model will not be utilized
llama_context:  CUDA_Host  output buffer size =     0.14 MiB
llama_kv_cache:      CUDA0 KV buffer size =   512.00 MiB
llama_kv_cache: size =  512.00 MiB (  4096 cells,  32 layers,  1/1 seqs), K (f16):  256.00 MiB, V (f16):  256.00 MiB
llama_context:      CUDA0 compute buffer size =   300.01 MiB
llama_context:  CUDA_Host compute buffer size =    20.01 MiB
llama_context: graph nodes  = 1158
llama_context: graph splits = 2
time=2025-10-18T05:04:01.557-04:00 level=INFO source=server.go:1310 msg="llama runner started in 3.76 seconds"
time=2025-10-18T05:04:01.557-04:00 level=INFO source=sched.go:482 msg="loaded runners" count=1
time=2025-10-18T05:04:01.557-04:00 level=INFO source=server.go:1272 msg="waiting for llama runner to start responding"
time=2025-10-18T05:04:01.557-04:00 level=INFO source=server.go:1310 msg="llama runner started in 3.76 seconds"
[GIN] 2025/10/18 - 05:04:05 | 200 |   11.4741901s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/18 - 10:31:02 | 200 |            0s |       127.0.0.1 | GET      "/api/version"
[GIN] 2025/10/18 - 10:31:02 | 200 |            0s |       127.0.0.1 | GET      "/api/version"
[GIN] 2025/10/18 - 10:31:02 | 404 |      2.7015ms |       127.0.0.1 | POST     "/api/show"
[GIN] 2025/10/18 - 10:31:03 | 200 |     28.8787ms |       127.0.0.1 | GET      "/api/tags"
[GIN] 2025/10/18 - 10:31:05 | 404 |       793.3µs |       127.0.0.1 | POST     "/api/show"
[GIN] 2025/10/18 - 10:31:09 | 404 |       780.1µs |       127.0.0.1 | POST     "/api/show"
[GIN] 2025/10/18 - 15:50:53 | 200 |            0s |       127.0.0.1 | GET      "/api/version"
[GIN] 2025/10/18 - 15:50:53 | 200 |            0s |       127.0.0.1 | GET      "/api/version"
[GIN] 2025/10/18 - 15:50:53 | 404 |     37.3791ms |       127.0.0.1 | POST     "/api/show"
[GIN] 2025/10/18 - 15:50:53 | 200 |     69.0951ms |       127.0.0.1 | GET      "/api/tags"
[GIN] 2025/10/18 - 15:50:54 | 404 |      1.0194ms |       127.0.0.1 | POST     "/api/show"
[GIN] 2025/10/18 - 15:50:56 | 404 |      1.2835ms |       127.0.0.1 | POST     "/api/show"
[GIN] 2025/10/18 - 15:51:00 | 404 |       505.5µs |       127.0.0.1 | POST     "/api/show"
[GIN] 2025/10/18 - 15:51:00 | 404 |      1.0321ms |       127.0.0.1 | POST     "/api/show"
[GIN] 2025/10/18 - 15:51:00 | 200 |            0s |       127.0.0.1 | GET      "/api/version"
[GIN] 2025/10/18 - 15:51:00 | 200 |      1.0255ms |       127.0.0.1 | GET      "/api/tags"
time=2025-10-18T15:51:01.464-04:00 level=INFO source=download.go:177 msg="downloading aeda25e63ebd in 16 208 MB part(s)"
[GIN] 2025/10/18 - 15:51:13 | 200 |    730.2053ms |       127.0.0.1 | POST     "/api/show"
[GIN] 2025/10/18 - 15:51:20 | 200 |   20.1821116s |       127.0.0.1 | POST     "/api/pull"
[GIN] 2025/10/18 - 15:51:27 | 200 |       7.551ms |       127.0.0.1 | POST     "/api/show"
[GIN] 2025/10/18 - 15:51:27 | 200 |      7.1431ms |       127.0.0.1 | POST     "/api/show"
llama_model_loader: loaded meta data with 25 key-value pairs and 291 tensors from C:\Users\frost\.ollama\models\blobs\sha256-f5074b1221da0f5a2910d33b642efa5b9eb58cfdddca1c79e16d7ad28aa2b31f (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = llama
llama_model_loader: - kv   1:                               general.name str              = Mistral-7B-Instruct-v0.3
llama_model_loader: - kv   2:                          llama.block_count u32              = 32
llama_model_loader: - kv   3:                       llama.context_length u32              = 32768
llama_model_loader: - kv   4:                     llama.embedding_length u32              = 4096
llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 14336
llama_model_loader: - kv   6:                 llama.attention.head_count u32              = 32
llama_model_loader: - kv   7:              llama.attention.head_count_kv u32              = 8
llama_model_loader: - kv   8:                       llama.rope.freq_base f32              = 1000000.000000
llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010
llama_model_loader: - kv  10:                          general.file_type u32              = 15
llama_model_loader: - kv  11:                           llama.vocab_size u32              = 32768
llama_model_loader: - kv  12:                 llama.rope.dimension_count u32              = 128
llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = llama
llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = default
llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,32768]   = ["<unk>", "<s>", "</s>", "[INST]", "[...
llama_model_loader: - kv  16:                      tokenizer.ggml.scores arr[f32,32768]   = [0.000000, 0.000000, 0.000000, 0.0000...
llama_model_loader: - kv  17:                  tokenizer.ggml.token_type arr[i32,32768]   = [2, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, ...
llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 1
llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 2
llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
llama_model_loader: - kv  21:               tokenizer.ggml.add_bos_token bool             = true
llama_model_loader: - kv  22:               tokenizer.ggml.add_eos_token bool             = false
llama_model_loader: - kv  23:                    tokenizer.chat_template str              = {{ bos_token }}{% for message in mess...
llama_model_loader: - kv  24:               general.quantization_version u32              = 2
llama_model_loader: - type  f32:   65 tensors
llama_model_loader: - type q4_K:  193 tensors
llama_model_loader: - type q6_K:   33 tensors
print_info: file format = GGUF V3 (latest)
print_info: file type   = Q4_K - Medium
print_info: file size   = 4.07 GiB (4.83 BPW) 
load: special_eos_id is not in special_eog_ids - the tokenizer config may be incorrect
load: printing all EOG tokens:
load:   - 2 ('</s>')
load: special tokens cache size = 771
load: token to piece cache size = 0.1731 MB
print_info: arch             = llama
print_info: vocab_only       = 1
print_info: model type       = ?B
print_info: model params     = 7.25 B
print_info: general.name     = Mistral-7B-Instruct-v0.3
print_info: vocab type       = SPM
print_info: n_vocab          = 32768
print_info: n_merges         = 0
print_info: BOS token        = 1 '<s>'
print_info: EOS token        = 2 '</s>'
print_info: UNK token        = 0 '<unk>'
print_info: LF token         = 781 '<0x0A>'
print_info: EOG token        = 2 '</s>'
print_info: max token length = 48
llama_model_load: vocab only - skipping tensors
time=2025-10-18T15:51:28.954-04:00 level=INFO source=cpu_windows.go:139 msg=packages count=1
time=2025-10-18T15:51:28.954-04:00 level=INFO source=cpu_windows.go:155 msg="efficiency cores detected" maxEfficiencyClass=1
time=2025-10-18T15:51:28.954-04:00 level=INFO source=cpu_windows.go:186 msg="" package=0 cores=12 efficiency=4 threads=20
time=2025-10-18T15:51:28.955-04:00 level=INFO source=server.go:400 msg="starting runner" cmd="C:\\Users\\frost\\AppData\\Local\\Programs\\Ollama\\ollama.exe runner --model C:\\Users\\frost\\.ollama\\models\\blobs\\sha256-f5074b1221da0f5a2910d33b642efa5b9eb58cfdddca1c79e16d7ad28aa2b31f --port 61456"
time=2025-10-18T15:51:28.960-04:00 level=INFO source=cpu_windows.go:139 msg=packages count=1
time=2025-10-18T15:51:28.960-04:00 level=INFO source=cpu_windows.go:155 msg="efficiency cores detected" maxEfficiencyClass=1
time=2025-10-18T15:51:28.960-04:00 level=INFO source=cpu_windows.go:186 msg="" package=0 cores=12 efficiency=4 threads=20
time=2025-10-18T15:51:28.960-04:00 level=INFO source=server.go:505 msg="system memory" total="63.8 GiB" free="41.3 GiB" free_swap="41.6 GiB"
time=2025-10-18T15:51:28.960-04:00 level=INFO source=memory.go:36 msg="new model will fit in available VRAM across minimum required GPUs, loading" model=C:\Users\frost\.ollama\models\blobs\sha256-f5074b1221da0f5a2910d33b642efa5b9eb58cfdddca1c79e16d7ad28aa2b31f library=CUDA parallel=1 required="5.4 GiB" gpus=1
time=2025-10-18T15:51:28.960-04:00 level=INFO source=server.go:545 msg=offload library=CUDA layers.requested=-1 layers.model=33 layers.offload=33 layers.split=[33] memory.available="[10.6 GiB]" memory.gpu_overhead="0 B" memory.required.full="5.4 GiB" memory.required.partial="5.4 GiB" memory.required.kv="512.0 MiB" memory.required.allocations="[5.4 GiB]" memory.weights.total="4.0 GiB" memory.weights.repeating="3.9 GiB" memory.weights.nonrepeating="105.0 MiB" memory.graph.full="296.0 MiB" memory.graph.partial="305.0 MiB"
time=2025-10-18T15:51:28.997-04:00 level=INFO source=runner.go:893 msg="starting go runner"
load_backend: loaded CPU backend from C:\Users\frost\AppData\Local\Programs\Ollama\lib\ollama\ggml-cpu-alderlake.dll
ggml_cuda_init: GGML_CUDA_FORCE_MMQ:    no
ggml_cuda_init: GGML_CUDA_FORCE_CUBLAS: no
ggml_cuda_init: found 1 CUDA devices:
  Device 0: NVIDIA GeForce RTX 5070, compute capability 12.0, VMM: yes, ID: GPU-b28645fa-aaf6-86a1-ab1b-ef8d61df29f2
load_backend: loaded CUDA backend from C:\Users\frost\AppData\Local\Programs\Ollama\lib\ollama\cuda_v12\ggml-cuda.dll
time=2025-10-18T15:51:29.124-04:00 level=INFO source=ggml.go:104 msg=system CPU.0.SSE3=1 CPU.0.SSSE3=1 CPU.0.AVX=1 CPU.0.AVX_VNNI=1 CPU.0.AVX2=1 CPU.0.F16C=1 CPU.0.FMA=1 CPU.0.BMI2=1 CPU.0.LLAMAFILE=1 CPU.1.LLAMAFILE=1 CUDA.0.ARCHS=500,520,600,610,700,750,800,860,870,890,900,1200 CUDA.0.USE_GRAPHS=1 CUDA.0.PEER_MAX_BATCH_SIZE=128 compiler=cgo(clang)
time=2025-10-18T15:51:29.125-04:00 level=INFO source=runner.go:929 msg="Server listening on 127.0.0.1:61456"
time=2025-10-18T15:51:29.129-04:00 level=INFO source=runner.go:828 msg=load request="{Operation:commit LoraPath:[] Parallel:1 BatchSize:512 FlashAttention:false KvSize:4096 KvCacheType: NumThreads:8 GPULayers:33[ID:GPU-b28645fa-aaf6-86a1-ab1b-ef8d61df29f2 Layers:33(0..32)] MultiUserCache:false ProjectorPath: MainGPU:0 UseMmap:false}"
time=2025-10-18T15:51:29.129-04:00 level=INFO source=server.go:1272 msg="waiting for llama runner to start responding"
time=2025-10-18T15:51:29.129-04:00 level=INFO source=server.go:1306 msg="waiting for server to become available" status="llm server loading model"
ggml_backend_cuda_device_get_memory utilizing NVML memory reporting free: 11378196480 total: 12820938752
llama_model_load_from_file_impl: using device CUDA0 (NVIDIA GeForce RTX 5070) (0000:01:00.0) - 10851 MiB free
llama_model_loader: loaded meta data with 25 key-value pairs and 291 tensors from C:\Users\frost\.ollama\models\blobs\sha256-f5074b1221da0f5a2910d33b642efa5b9eb58cfdddca1c79e16d7ad28aa2b31f (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = llama
llama_model_loader: - kv   1:                               general.name str              = Mistral-7B-Instruct-v0.3
llama_model_loader: - kv   2:                          llama.block_count u32              = 32
llama_model_loader: - kv   3:                       llama.context_length u32              = 32768
llama_model_loader: - kv   4:                     llama.embedding_length u32              = 4096
llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 14336
llama_model_loader: - kv   6:                 llama.attention.head_count u32              = 32
llama_model_loader: - kv   7:              llama.attention.head_count_kv u32              = 8
llama_model_loader: - kv   8:                       llama.rope.freq_base f32              = 1000000.000000
llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010
llama_model_loader: - kv  10:                          general.file_type u32              = 15
llama_model_loader: - kv  11:                           llama.vocab_size u32              = 32768
llama_model_loader: - kv  12:                 llama.rope.dimension_count u32              = 128
llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = llama
llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = default
llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,32768]   = ["<unk>", "<s>", "</s>", "[INST]", "[...
llama_model_loader: - kv  16:                      tokenizer.ggml.scores arr[f32,32768]   = [0.000000, 0.000000, 0.000000, 0.0000...
llama_model_loader: - kv  17:                  tokenizer.ggml.token_type arr[i32,32768]   = [2, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, ...
llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 1
llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 2
llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
llama_model_loader: - kv  21:               tokenizer.ggml.add_bos_token bool             = true
llama_model_loader: - kv  22:               tokenizer.ggml.add_eos_token bool             = false
llama_model_loader: - kv  23:                    tokenizer.chat_template str              = {{ bos_token }}{% for message in mess...
llama_model_loader: - kv  24:               general.quantization_version u32              = 2
llama_model_loader: - type  f32:   65 tensors
llama_model_loader: - type q4_K:  193 tensors
llama_model_loader: - type q6_K:   33 tensors
print_info: file format = GGUF V3 (latest)
print_info: file type   = Q4_K - Medium
print_info: file size   = 4.07 GiB (4.83 BPW) 
load: special_eos_id is not in special_eog_ids - the tokenizer config may be incorrect
load: printing all EOG tokens:
load:   - 2 ('</s>')
load: special tokens cache size = 771
load: token to piece cache size = 0.1731 MB
print_info: arch             = llama
print_info: vocab_only       = 0
print_info: n_ctx_train      = 32768
print_info: n_embd           = 4096
print_info: n_layer          = 32
print_info: n_head           = 32
print_info: n_head_kv        = 8
print_info: n_rot            = 128
print_info: n_swa            = 0
print_info: is_swa_any       = 0
print_info: n_embd_head_k    = 128
print_info: n_embd_head_v    = 128
print_info: n_gqa            = 4
print_info: n_embd_k_gqa     = 1024
print_info: n_embd_v_gqa     = 1024
print_info: f_norm_eps       = 0.0e+00
print_info: f_norm_rms_eps   = 1.0e-05
print_info: f_clamp_kqv      = 0.0e+00
print_info: f_max_alibi_bias = 0.0e+00
print_info: f_logit_scale    = 0.0e+00
print_info: f_attn_scale     = 0.0e+00
print_info: n_ff             = 14336
print_info: n_expert         = 0
print_info: n_expert_used    = 0
print_info: causal attn      = 1
print_info: pooling type     = 0
print_info: rope type        = 0
print_info: rope scaling     = linear
print_info: freq_base_train  = 1000000.0
print_info: freq_scale_train = 1
print_info: n_ctx_orig_yarn  = 32768
print_info: rope_finetuned   = unknown
print_info: model type       = 7B
print_info: model params     = 7.25 B
print_info: general.name     = Mistral-7B-Instruct-v0.3
print_info: vocab type       = SPM
print_info: n_vocab          = 32768
print_info: n_merges         = 0
print_info: BOS token        = 1 '<s>'
print_info: EOS token        = 2 '</s>'
print_info: UNK token        = 0 '<unk>'
print_info: LF token         = 781 '<0x0A>'
print_info: EOG token        = 2 '</s>'
print_info: max token length = 48
load_tensors: loading model tensors, this can take a while... (mmap = false)
load_tensors: offloading 32 repeating layers to GPU
load_tensors: offloading output layer to GPU
load_tensors: offloaded 33/33 layers to GPU
load_tensors:        CUDA0 model buffer size =  4097.52 MiB
load_tensors:          CPU model buffer size =    72.00 MiB
llama_init_from_model: model default pooling_type is [0], but [-1] was specified
llama_context: constructing llama_context
llama_context: n_seq_max     = 1
llama_context: n_ctx         = 4096
llama_context: n_ctx_per_seq = 4096
llama_context: n_batch       = 512
llama_context: n_ubatch      = 512
llama_context: causal_attn   = 1
llama_context: flash_attn    = disabled
llama_context: kv_unified    = false
llama_context: freq_base     = 1000000.0
llama_context: freq_scale    = 1
llama_context: n_ctx_per_seq (4096) < n_ctx_train (32768) -- the full capacity of the model will not be utilized
llama_context:  CUDA_Host  output buffer size =     0.14 MiB
llama_kv_cache:      CUDA0 KV buffer size =   512.00 MiB
llama_kv_cache: size =  512.00 MiB (  4096 cells,  32 layers,  1/1 seqs), K (f16):  256.00 MiB, V (f16):  256.00 MiB
llama_context:      CUDA0 compute buffer size =   300.01 MiB
llama_context:  CUDA_Host compute buffer size =    20.01 MiB
llama_context: graph nodes  = 1158
llama_context: graph splits = 2
time=2025-10-18T15:51:32.633-04:00 level=INFO source=server.go:1310 msg="llama runner started in 3.68 seconds"
time=2025-10-18T15:51:32.633-04:00 level=INFO source=sched.go:482 msg="loaded runners" count=1
time=2025-10-18T15:51:32.633-04:00 level=INFO source=server.go:1272 msg="waiting for llama runner to start responding"
time=2025-10-18T15:51:32.633-04:00 level=INFO source=server.go:1310 msg="llama runner started in 3.68 seconds"
[GIN] 2025/10/18 - 15:51:32 | 200 |    5.3400001s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/18 - 15:51:47 | 200 |      7.2675ms |       127.0.0.1 | POST     "/api/show"
[GIN] 2025/10/18 - 15:51:47 | 200 |      6.6782ms |       127.0.0.1 | POST     "/api/show"
[GIN] 2025/10/18 - 15:51:49 | 200 |    1.0269089s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/18 - 16:01:38 | 200 |      8.3255ms |       127.0.0.1 | POST     "/api/show"
[GIN] 2025/10/18 - 16:01:38 | 200 |      7.8239ms |       127.0.0.1 | POST     "/api/show"
llama_model_loader: loaded meta data with 25 key-value pairs and 291 tensors from C:\Users\frost\.ollama\models\blobs\sha256-f5074b1221da0f5a2910d33b642efa5b9eb58cfdddca1c79e16d7ad28aa2b31f (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = llama
llama_model_loader: - kv   1:                               general.name str              = Mistral-7B-Instruct-v0.3
llama_model_loader: - kv   2:                          llama.block_count u32              = 32
llama_model_loader: - kv   3:                       llama.context_length u32              = 32768
llama_model_loader: - kv   4:                     llama.embedding_length u32              = 4096
llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 14336
llama_model_loader: - kv   6:                 llama.attention.head_count u32              = 32
llama_model_loader: - kv   7:              llama.attention.head_count_kv u32              = 8
llama_model_loader: - kv   8:                       llama.rope.freq_base f32              = 1000000.000000
llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010
llama_model_loader: - kv  10:                          general.file_type u32              = 15
llama_model_loader: - kv  11:                           llama.vocab_size u32              = 32768
llama_model_loader: - kv  12:                 llama.rope.dimension_count u32              = 128
llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = llama
llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = default
llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,32768]   = ["<unk>", "<s>", "</s>", "[INST]", "[...
llama_model_loader: - kv  16:                      tokenizer.ggml.scores arr[f32,32768]   = [0.000000, 0.000000, 0.000000, 0.0000...
llama_model_loader: - kv  17:                  tokenizer.ggml.token_type arr[i32,32768]   = [2, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, ...
llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 1
llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 2
llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
llama_model_loader: - kv  21:               tokenizer.ggml.add_bos_token bool             = true
llama_model_loader: - kv  22:               tokenizer.ggml.add_eos_token bool             = false
llama_model_loader: - kv  23:                    tokenizer.chat_template str              = {{ bos_token }}{% for message in mess...
llama_model_loader: - kv  24:               general.quantization_version u32              = 2
llama_model_loader: - type  f32:   65 tensors
llama_model_loader: - type q4_K:  193 tensors
llama_model_loader: - type q6_K:   33 tensors
print_info: file format = GGUF V3 (latest)
print_info: file type   = Q4_K - Medium
print_info: file size   = 4.07 GiB (4.83 BPW) 
load: special_eos_id is not in special_eog_ids - the tokenizer config may be incorrect
load: printing all EOG tokens:
load:   - 2 ('</s>')
load: special tokens cache size = 771
load: token to piece cache size = 0.1731 MB
print_info: arch             = llama
print_info: vocab_only       = 1
print_info: model type       = ?B
print_info: model params     = 7.25 B
print_info: general.name     = Mistral-7B-Instruct-v0.3
print_info: vocab type       = SPM
print_info: n_vocab          = 32768
print_info: n_merges         = 0
print_info: BOS token        = 1 '<s>'
print_info: EOS token        = 2 '</s>'
print_info: UNK token        = 0 '<unk>'
print_info: LF token         = 781 '<0x0A>'
print_info: EOG token        = 2 '</s>'
print_info: max token length = 48
llama_model_load: vocab only - skipping tensors
time=2025-10-18T16:01:38.364-04:00 level=INFO source=cpu_windows.go:139 msg=packages count=1
time=2025-10-18T16:01:38.364-04:00 level=INFO source=cpu_windows.go:155 msg="efficiency cores detected" maxEfficiencyClass=1
time=2025-10-18T16:01:38.364-04:00 level=INFO source=cpu_windows.go:186 msg="" package=0 cores=12 efficiency=4 threads=20
time=2025-10-18T16:01:38.365-04:00 level=INFO source=server.go:400 msg="starting runner" cmd="C:\\Users\\frost\\AppData\\Local\\Programs\\Ollama\\ollama.exe runner --model C:\\Users\\frost\\.ollama\\models\\blobs\\sha256-f5074b1221da0f5a2910d33b642efa5b9eb58cfdddca1c79e16d7ad28aa2b31f --port 56360"
time=2025-10-18T16:01:38.383-04:00 level=INFO source=cpu_windows.go:139 msg=packages count=1
time=2025-10-18T16:01:38.383-04:00 level=INFO source=cpu_windows.go:155 msg="efficiency cores detected" maxEfficiencyClass=1
time=2025-10-18T16:01:38.383-04:00 level=INFO source=cpu_windows.go:186 msg="" package=0 cores=12 efficiency=4 threads=20
time=2025-10-18T16:01:38.383-04:00 level=INFO source=server.go:505 msg="system memory" total="63.8 GiB" free="40.2 GiB" free_swap="40.5 GiB"
time=2025-10-18T16:01:38.383-04:00 level=INFO source=memory.go:36 msg="new model will fit in available VRAM across minimum required GPUs, loading" model=C:\Users\frost\.ollama\models\blobs\sha256-f5074b1221da0f5a2910d33b642efa5b9eb58cfdddca1c79e16d7ad28aa2b31f library=CUDA parallel=1 required="5.4 GiB" gpus=1
time=2025-10-18T16:01:38.383-04:00 level=INFO source=server.go:545 msg=offload library=CUDA layers.requested=-1 layers.model=33 layers.offload=33 layers.split=[33] memory.available="[10.4 GiB]" memory.gpu_overhead="0 B" memory.required.full="5.4 GiB" memory.required.partial="5.4 GiB" memory.required.kv="512.0 MiB" memory.required.allocations="[5.4 GiB]" memory.weights.total="4.0 GiB" memory.weights.repeating="3.9 GiB" memory.weights.nonrepeating="105.0 MiB" memory.graph.full="296.0 MiB" memory.graph.partial="305.0 MiB"
time=2025-10-18T16:01:38.422-04:00 level=INFO source=runner.go:893 msg="starting go runner"
load_backend: loaded CPU backend from C:\Users\frost\AppData\Local\Programs\Ollama\lib\ollama\ggml-cpu-alderlake.dll
ggml_cuda_init: GGML_CUDA_FORCE_MMQ:    no
ggml_cuda_init: GGML_CUDA_FORCE_CUBLAS: no
ggml_cuda_init: found 1 CUDA devices:
  Device 0: NVIDIA GeForce RTX 5070, compute capability 12.0, VMM: yes, ID: GPU-b28645fa-aaf6-86a1-ab1b-ef8d61df29f2
load_backend: loaded CUDA backend from C:\Users\frost\AppData\Local\Programs\Ollama\lib\ollama\cuda_v12\ggml-cuda.dll
time=2025-10-18T16:01:38.517-04:00 level=INFO source=ggml.go:104 msg=system CPU.0.SSE3=1 CPU.0.SSSE3=1 CPU.0.AVX=1 CPU.0.AVX_VNNI=1 CPU.0.AVX2=1 CPU.0.F16C=1 CPU.0.FMA=1 CPU.0.BMI2=1 CPU.0.LLAMAFILE=1 CPU.1.LLAMAFILE=1 CUDA.0.ARCHS=500,520,600,610,700,750,800,860,870,890,900,1200 CUDA.0.USE_GRAPHS=1 CUDA.0.PEER_MAX_BATCH_SIZE=128 compiler=cgo(clang)
time=2025-10-18T16:01:38.518-04:00 level=INFO source=runner.go:929 msg="Server listening on 127.0.0.1:56360"
time=2025-10-18T16:01:38.522-04:00 level=INFO source=runner.go:828 msg=load request="{Operation:commit LoraPath:[] Parallel:1 BatchSize:512 FlashAttention:false KvSize:4096 KvCacheType: NumThreads:8 GPULayers:33[ID:GPU-b28645fa-aaf6-86a1-ab1b-ef8d61df29f2 Layers:33(0..32)] MultiUserCache:false ProjectorPath: MainGPU:0 UseMmap:false}"
time=2025-10-18T16:01:38.522-04:00 level=INFO source=server.go:1272 msg="waiting for llama runner to start responding"
time=2025-10-18T16:01:38.522-04:00 level=INFO source=server.go:1306 msg="waiting for server to become available" status="llm server loading model"
ggml_backend_cuda_device_get_memory utilizing NVML memory reporting free: 11126898688 total: 12820938752
llama_model_load_from_file_impl: using device CUDA0 (NVIDIA GeForce RTX 5070) (0000:01:00.0) - 10611 MiB free
llama_model_loader: loaded meta data with 25 key-value pairs and 291 tensors from C:\Users\frost\.ollama\models\blobs\sha256-f5074b1221da0f5a2910d33b642efa5b9eb58cfdddca1c79e16d7ad28aa2b31f (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = llama
llama_model_loader: - kv   1:                               general.name str              = Mistral-7B-Instruct-v0.3
llama_model_loader: - kv   2:                          llama.block_count u32              = 32
llama_model_loader: - kv   3:                       llama.context_length u32              = 32768
llama_model_loader: - kv   4:                     llama.embedding_length u32              = 4096
llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 14336
llama_model_loader: - kv   6:                 llama.attention.head_count u32              = 32
llama_model_loader: - kv   7:              llama.attention.head_count_kv u32              = 8
llama_model_loader: - kv   8:                       llama.rope.freq_base f32              = 1000000.000000
llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010
llama_model_loader: - kv  10:                          general.file_type u32              = 15
llama_model_loader: - kv  11:                           llama.vocab_size u32              = 32768
llama_model_loader: - kv  12:                 llama.rope.dimension_count u32              = 128
llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = llama
llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = default
llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,32768]   = ["<unk>", "<s>", "</s>", "[INST]", "[...
llama_model_loader: - kv  16:                      tokenizer.ggml.scores arr[f32,32768]   = [0.000000, 0.000000, 0.000000, 0.0000...
llama_model_loader: - kv  17:                  tokenizer.ggml.token_type arr[i32,32768]   = [2, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, ...
llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 1
llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 2
llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
llama_model_loader: - kv  21:               tokenizer.ggml.add_bos_token bool             = true
llama_model_loader: - kv  22:               tokenizer.ggml.add_eos_token bool             = false
llama_model_loader: - kv  23:                    tokenizer.chat_template str              = {{ bos_token }}{% for message in mess...
llama_model_loader: - kv  24:               general.quantization_version u32              = 2
llama_model_loader: - type  f32:   65 tensors
llama_model_loader: - type q4_K:  193 tensors
llama_model_loader: - type q6_K:   33 tensors
print_info: file format = GGUF V3 (latest)
print_info: file type   = Q4_K - Medium
print_info: file size   = 4.07 GiB (4.83 BPW) 
load: special_eos_id is not in special_eog_ids - the tokenizer config may be incorrect
load: printing all EOG tokens:
load:   - 2 ('</s>')
load: special tokens cache size = 771
load: token to piece cache size = 0.1731 MB
print_info: arch             = llama
print_info: vocab_only       = 0
print_info: n_ctx_train      = 32768
print_info: n_embd           = 4096
print_info: n_layer          = 32
print_info: n_head           = 32
print_info: n_head_kv        = 8
print_info: n_rot            = 128
print_info: n_swa            = 0
print_info: is_swa_any       = 0
print_info: n_embd_head_k    = 128
print_info: n_embd_head_v    = 128
print_info: n_gqa            = 4
print_info: n_embd_k_gqa     = 1024
print_info: n_embd_v_gqa     = 1024
print_info: f_norm_eps       = 0.0e+00
print_info: f_norm_rms_eps   = 1.0e-05
print_info: f_clamp_kqv      = 0.0e+00
print_info: f_max_alibi_bias = 0.0e+00
print_info: f_logit_scale    = 0.0e+00
print_info: f_attn_scale     = 0.0e+00
print_info: n_ff             = 14336
print_info: n_expert         = 0
print_info: n_expert_used    = 0
print_info: causal attn      = 1
print_info: pooling type     = 0
print_info: rope type        = 0
print_info: rope scaling     = linear
print_info: freq_base_train  = 1000000.0
print_info: freq_scale_train = 1
print_info: n_ctx_orig_yarn  = 32768
print_info: rope_finetuned   = unknown
print_info: model type       = 7B
print_info: model params     = 7.25 B
print_info: general.name     = Mistral-7B-Instruct-v0.3
print_info: vocab type       = SPM
print_info: n_vocab          = 32768
print_info: n_merges         = 0
print_info: BOS token        = 1 '<s>'
print_info: EOS token        = 2 '</s>'
print_info: UNK token        = 0 '<unk>'
print_info: LF token         = 781 '<0x0A>'
print_info: EOG token        = 2 '</s>'
print_info: max token length = 48
load_tensors: loading model tensors, this can take a while... (mmap = false)
load_tensors: offloading 32 repeating layers to GPU
load_tensors: offloading output layer to GPU
load_tensors: offloaded 33/33 layers to GPU
load_tensors:        CUDA0 model buffer size =  4097.52 MiB
load_tensors:          CPU model buffer size =    72.00 MiB
llama_init_from_model: model default pooling_type is [0], but [-1] was specified
llama_context: constructing llama_context
llama_context: n_seq_max     = 1
llama_context: n_ctx         = 4096
llama_context: n_ctx_per_seq = 4096
llama_context: n_batch       = 512
llama_context: n_ubatch      = 512
llama_context: causal_attn   = 1
llama_context: flash_attn    = disabled
llama_context: kv_unified    = false
llama_context: freq_base     = 1000000.0
llama_context: freq_scale    = 1
llama_context: n_ctx_per_seq (4096) < n_ctx_train (32768) -- the full capacity of the model will not be utilized
llama_context:  CUDA_Host  output buffer size =     0.14 MiB
llama_kv_cache:      CUDA0 KV buffer size =   512.00 MiB
llama_kv_cache: size =  512.00 MiB (  4096 cells,  32 layers,  1/1 seqs), K (f16):  256.00 MiB, V (f16):  256.00 MiB
llama_context:      CUDA0 compute buffer size =   300.01 MiB
llama_context:  CUDA_Host compute buffer size =    20.01 MiB
llama_context: graph nodes  = 1158
llama_context: graph splits = 2
time=2025-10-18T16:01:40.024-04:00 level=INFO source=server.go:1310 msg="llama runner started in 1.66 seconds"
time=2025-10-18T16:01:40.024-04:00 level=INFO source=sched.go:482 msg="loaded runners" count=1
time=2025-10-18T16:01:40.024-04:00 level=INFO source=server.go:1272 msg="waiting for llama runner to start responding"
time=2025-10-18T16:01:40.024-04:00 level=INFO source=server.go:1310 msg="llama runner started in 1.66 seconds"
[GIN] 2025/10/18 - 16:01:40 | 200 |    2.4387583s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/18 - 16:01:55 | 200 |      7.3986ms |       127.0.0.1 | POST     "/api/show"
[GIN] 2025/10/18 - 16:01:55 | 200 |      6.7288ms |       127.0.0.1 | POST     "/api/show"
[GIN] 2025/10/18 - 16:01:55 | 200 |    584.0967ms |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/18 - 16:02:07 | 200 |      7.8963ms |       127.0.0.1 | POST     "/api/show"
[GIN] 2025/10/18 - 16:02:07 | 200 |      6.1463ms |       127.0.0.1 | POST     "/api/show"
[GIN] 2025/10/18 - 16:02:10 | 200 |    2.6139927s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/18 - 19:21:03 | 404 |            0s |       127.0.0.1 | POST     "/api/embeddings"
[GIN] 2025/10/18 - 19:21:49 | 200 |            0s |       127.0.0.1 | HEAD     "/"
time=2025-10-18T19:21:50.090-04:00 level=INFO source=download.go:177 msg="downloading 819c2adf5ce6 in 7 100 MB part(s)"
time=2025-10-18T19:22:21.477-04:00 level=INFO source=download.go:177 msg="downloading c71d239df917 in 1 11 KB part(s)"
time=2025-10-18T19:22:22.789-04:00 level=INFO source=download.go:177 msg="downloading b837481ff855 in 1 16 B part(s)"
time=2025-10-18T19:22:23.954-04:00 level=INFO source=download.go:177 msg="downloading 38badd946f91 in 1 408 B part(s)"
[GIN] 2025/10/18 - 19:22:25 | 200 |   36.5585722s |       127.0.0.1 | POST     "/api/pull"
llama_model_loader: loaded meta data with 23 key-value pairs and 389 tensors from C:\Users\frost\.ollama\models\blobs\sha256-819c2adf5ce6df2b6bd2ae4ca90d2a69f060afeb438d0c171db57daa02e39c3d (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = bert
llama_model_loader: - kv   1:                               general.name str              = mxbai-embed-large-v1
llama_model_loader: - kv   2:                           bert.block_count u32              = 24
llama_model_loader: - kv   3:                        bert.context_length u32              = 512
llama_model_loader: - kv   4:                      bert.embedding_length u32              = 1024
llama_model_loader: - kv   5:                   bert.feed_forward_length u32              = 4096
llama_model_loader: - kv   6:                  bert.attention.head_count u32              = 16
llama_model_loader: - kv   7:          bert.attention.layer_norm_epsilon f32              = 0.000000
llama_model_loader: - kv   8:                          general.file_type u32              = 1
llama_model_loader: - kv   9:                      bert.attention.causal bool             = false
llama_model_loader: - kv  10:                          bert.pooling_type u32              = 2
llama_model_loader: - kv  11:            tokenizer.ggml.token_type_count u32              = 2
llama_model_loader: - kv  12:                tokenizer.ggml.bos_token_id u32              = 101
llama_model_loader: - kv  13:                tokenizer.ggml.eos_token_id u32              = 102
llama_model_loader: - kv  14:                       tokenizer.ggml.model str              = bert
llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,30522]   = ["[PAD]", "[unused0]", "[unused1]", "...
llama_model_loader: - kv  16:                      tokenizer.ggml.scores arr[f32,30522]   = [-1000.000000, -1000.000000, -1000.00...
llama_model_loader: - kv  17:                  tokenizer.ggml.token_type arr[i32,30522]   = [3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  18:            tokenizer.ggml.unknown_token_id u32              = 100
llama_model_loader: - kv  19:          tokenizer.ggml.seperator_token_id u32              = 102
llama_model_loader: - kv  20:            tokenizer.ggml.padding_token_id u32              = 0
llama_model_loader: - kv  21:                tokenizer.ggml.cls_token_id u32              = 101
llama_model_loader: - kv  22:               tokenizer.ggml.mask_token_id u32              = 103
llama_model_loader: - type  f32:  243 tensors
llama_model_loader: - type  f16:  146 tensors
print_info: file format = GGUF V3 (latest)
print_info: file type   = F16
print_info: file size   = 637.85 MiB (16.02 BPW) 
load: special_eos_id is not in special_eog_ids - the tokenizer config may be incorrect
load: printing all EOG tokens:
load:   - 102 ('[SEP]')
load: special tokens cache size = 5
load: token to piece cache size = 0.2032 MB
print_info: arch             = bert
print_info: vocab_only       = 1
print_info: model type       = ?B
print_info: model params     = 334.09 M
print_info: general.name     = mxbai-embed-large-v1
print_info: vocab type       = WPM
print_info: n_vocab          = 30522
print_info: n_merges         = 0
print_info: BOS token        = 101 '[CLS]'
print_info: EOS token        = 102 '[SEP]'
print_info: UNK token        = 100 '[UNK]'
print_info: SEP token        = 102 '[SEP]'
print_info: PAD token        = 0 '[PAD]'
print_info: MASK token       = 103 '[MASK]'
print_info: LF token         = 0 '[PAD]'
print_info: EOG token        = 102 '[SEP]'
print_info: max token length = 21
llama_model_load: vocab only - skipping tensors
time=2025-10-18T19:22:37.815-04:00 level=INFO source=cpu_windows.go:139 msg=packages count=1
time=2025-10-18T19:22:37.815-04:00 level=INFO source=cpu_windows.go:155 msg="efficiency cores detected" maxEfficiencyClass=1
time=2025-10-18T19:22:37.815-04:00 level=INFO source=cpu_windows.go:186 msg="" package=0 cores=12 efficiency=4 threads=20
time=2025-10-18T19:22:37.815-04:00 level=INFO source=server.go:400 msg="starting runner" cmd="C:\\Users\\frost\\AppData\\Local\\Programs\\Ollama\\ollama.exe runner --model C:\\Users\\frost\\.ollama\\models\\blobs\\sha256-819c2adf5ce6df2b6bd2ae4ca90d2a69f060afeb438d0c171db57daa02e39c3d --port 62787"
time=2025-10-18T19:22:37.819-04:00 level=INFO source=cpu_windows.go:139 msg=packages count=1
time=2025-10-18T19:22:37.819-04:00 level=INFO source=cpu_windows.go:155 msg="efficiency cores detected" maxEfficiencyClass=1
time=2025-10-18T19:22:37.819-04:00 level=INFO source=cpu_windows.go:186 msg="" package=0 cores=12 efficiency=4 threads=20
time=2025-10-18T19:22:37.819-04:00 level=INFO source=server.go:505 msg="system memory" total="63.8 GiB" free="40.2 GiB" free_swap="40.3 GiB"
time=2025-10-18T19:22:37.819-04:00 level=INFO source=memory.go:36 msg="new model will fit in available VRAM across minimum required GPUs, loading" model=C:\Users\frost\.ollama\models\blobs\sha256-819c2adf5ce6df2b6bd2ae4ca90d2a69f060afeb438d0c171db57daa02e39c3d library=CUDA parallel=1 required="1.1 GiB" gpus=1
time=2025-10-18T19:22:37.820-04:00 level=INFO source=server.go:545 msg=offload library=CUDA layers.requested=-1 layers.model=25 layers.offload=25 layers.split=[25] memory.available="[10.4 GiB]" memory.gpu_overhead="0 B" memory.required.full="1.1 GiB" memory.required.partial="1.1 GiB" memory.required.kv="3.0 MiB" memory.required.allocations="[1.1 GiB]" memory.weights.total="636.8 MiB" memory.weights.repeating="577.2 MiB" memory.weights.nonrepeating="59.6 MiB" memory.graph.full="8.0 MiB" memory.graph.partial="8.0 MiB"
time=2025-10-18T19:22:37.852-04:00 level=INFO source=runner.go:893 msg="starting go runner"
load_backend: loaded CPU backend from C:\Users\frost\AppData\Local\Programs\Ollama\lib\ollama\ggml-cpu-alderlake.dll
ggml_cuda_init: GGML_CUDA_FORCE_MMQ:    no
ggml_cuda_init: GGML_CUDA_FORCE_CUBLAS: no
ggml_cuda_init: found 1 CUDA devices:
  Device 0: NVIDIA GeForce RTX 5070, compute capability 12.0, VMM: yes, ID: GPU-b28645fa-aaf6-86a1-ab1b-ef8d61df29f2
load_backend: loaded CUDA backend from C:\Users\frost\AppData\Local\Programs\Ollama\lib\ollama\cuda_v12\ggml-cuda.dll
time=2025-10-18T19:22:37.969-04:00 level=INFO source=ggml.go:104 msg=system CPU.0.SSE3=1 CPU.0.SSSE3=1 CPU.0.AVX=1 CPU.0.AVX_VNNI=1 CPU.0.AVX2=1 CPU.0.F16C=1 CPU.0.FMA=1 CPU.0.BMI2=1 CPU.0.LLAMAFILE=1 CPU.1.LLAMAFILE=1 CUDA.0.ARCHS=500,520,600,610,700,750,800,860,870,890,900,1200 CUDA.0.USE_GRAPHS=1 CUDA.0.PEER_MAX_BATCH_SIZE=128 compiler=cgo(clang)
time=2025-10-18T19:22:37.970-04:00 level=INFO source=runner.go:929 msg="Server listening on 127.0.0.1:62787"
time=2025-10-18T19:22:37.979-04:00 level=INFO source=runner.go:828 msg=load request="{Operation:commit LoraPath:[] Parallel:1 BatchSize:512 FlashAttention:false KvSize:512 KvCacheType: NumThreads:8 GPULayers:25[ID:GPU-b28645fa-aaf6-86a1-ab1b-ef8d61df29f2 Layers:25(0..24)] MultiUserCache:false ProjectorPath: MainGPU:0 UseMmap:false}"
time=2025-10-18T19:22:37.981-04:00 level=INFO source=server.go:1272 msg="waiting for llama runner to start responding"
time=2025-10-18T19:22:37.981-04:00 level=INFO source=server.go:1306 msg="waiting for server to become available" status="llm server loading model"
ggml_backend_cuda_device_get_memory utilizing NVML memory reporting free: 11134722048 total: 12820938752
llama_model_load_from_file_impl: using device CUDA0 (NVIDIA GeForce RTX 5070) (0000:01:00.0) - 10618 MiB free
llama_model_loader: loaded meta data with 23 key-value pairs and 389 tensors from C:\Users\frost\.ollama\models\blobs\sha256-819c2adf5ce6df2b6bd2ae4ca90d2a69f060afeb438d0c171db57daa02e39c3d (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = bert
llama_model_loader: - kv   1:                               general.name str              = mxbai-embed-large-v1
llama_model_loader: - kv   2:                           bert.block_count u32              = 24
llama_model_loader: - kv   3:                        bert.context_length u32              = 512
llama_model_loader: - kv   4:                      bert.embedding_length u32              = 1024
llama_model_loader: - kv   5:                   bert.feed_forward_length u32              = 4096
llama_model_loader: - kv   6:                  bert.attention.head_count u32              = 16
llama_model_loader: - kv   7:          bert.attention.layer_norm_epsilon f32              = 0.000000
llama_model_loader: - kv   8:                          general.file_type u32              = 1
llama_model_loader: - kv   9:                      bert.attention.causal bool             = false
llama_model_loader: - kv  10:                          bert.pooling_type u32              = 2
llama_model_loader: - kv  11:            tokenizer.ggml.token_type_count u32              = 2
llama_model_loader: - kv  12:                tokenizer.ggml.bos_token_id u32              = 101
llama_model_loader: - kv  13:                tokenizer.ggml.eos_token_id u32              = 102
llama_model_loader: - kv  14:                       tokenizer.ggml.model str              = bert
llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,30522]   = ["[PAD]", "[unused0]", "[unused1]", "...
llama_model_loader: - kv  16:                      tokenizer.ggml.scores arr[f32,30522]   = [-1000.000000, -1000.000000, -1000.00...
llama_model_loader: - kv  17:                  tokenizer.ggml.token_type arr[i32,30522]   = [3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  18:            tokenizer.ggml.unknown_token_id u32              = 100
llama_model_loader: - kv  19:          tokenizer.ggml.seperator_token_id u32              = 102
llama_model_loader: - kv  20:            tokenizer.ggml.padding_token_id u32              = 0
llama_model_loader: - kv  21:                tokenizer.ggml.cls_token_id u32              = 101
llama_model_loader: - kv  22:               tokenizer.ggml.mask_token_id u32              = 103
llama_model_loader: - type  f32:  243 tensors
llama_model_loader: - type  f16:  146 tensors
print_info: file format = GGUF V3 (latest)
print_info: file type   = F16
print_info: file size   = 637.85 MiB (16.02 BPW) 
load: special_eos_id is not in special_eog_ids - the tokenizer config may be incorrect
load: printing all EOG tokens:
load:   - 102 ('[SEP]')
load: special tokens cache size = 5
load: token to piece cache size = 0.2032 MB
print_info: arch             = bert
print_info: vocab_only       = 0
print_info: n_ctx_train      = 512
print_info: n_embd           = 1024
print_info: n_layer          = 24
print_info: n_head           = 16
print_info: n_head_kv        = 16
print_info: n_rot            = 64
print_info: n_swa            = 0
print_info: is_swa_any       = 0
print_info: n_embd_head_k    = 64
print_info: n_embd_head_v    = 64
print_info: n_gqa            = 1
print_info: n_embd_k_gqa     = 1024
print_info: n_embd_v_gqa     = 1024
print_info: f_norm_eps       = 1.0e-12
print_info: f_norm_rms_eps   = 0.0e+00
print_info: f_clamp_kqv      = 0.0e+00
print_info: f_max_alibi_bias = 0.0e+00
print_info: f_logit_scale    = 0.0e+00
print_info: f_attn_scale     = 0.0e+00
print_info: n_ff             = 4096
print_info: n_expert         = 0
print_info: n_expert_used    = 0
print_info: causal attn      = 0
print_info: pooling type     = 2
print_info: rope type        = 2
print_info: rope scaling     = linear
print_info: freq_base_train  = 10000.0
print_info: freq_scale_train = 1
print_info: n_ctx_orig_yarn  = 512
print_info: rope_finetuned   = unknown
print_info: model type       = 335M
print_info: model params     = 334.09 M
print_info: general.name     = mxbai-embed-large-v1
print_info: vocab type       = WPM
print_info: n_vocab          = 30522
print_info: n_merges         = 0
print_info: BOS token        = 101 '[CLS]'
print_info: EOS token        = 102 '[SEP]'
print_info: UNK token        = 100 '[UNK]'
print_info: SEP token        = 102 '[SEP]'
print_info: PAD token        = 0 '[PAD]'
print_info: MASK token       = 103 '[MASK]'
print_info: LF token         = 0 '[PAD]'
print_info: EOG token        = 102 '[SEP]'
print_info: max token length = 21
load_tensors: loading model tensors, this can take a while... (mmap = false)
load_tensors: offloading 24 repeating layers to GPU
load_tensors: offloading output layer to GPU
load_tensors: offloaded 25/25 layers to GPU
load_tensors:    CUDA_Host model buffer size =    60.63 MiB
load_tensors:        CUDA0 model buffer size =   577.22 MiB
llama_init_from_model: model default pooling_type is [2], but [-1] was specified
llama_context: constructing llama_context
llama_context: n_seq_max     = 1
llama_context: n_ctx         = 512
llama_context: n_ctx_per_seq = 512
llama_context: n_batch       = 512
llama_context: n_ubatch      = 512
llama_context: causal_attn   = 0
llama_context: flash_attn    = disabled
llama_context: kv_unified    = false
llama_context: freq_base     = 10000.0
llama_context: freq_scale    = 1
llama_context:  CUDA_Host  output buffer size =     0.12 MiB
llama_context:      CUDA0 compute buffer size =    27.01 MiB
llama_context:  CUDA_Host compute buffer size =     5.01 MiB
llama_context: graph nodes  = 851
llama_context: graph splits = 4 (with bs=512), 2 (with bs=1)
time=2025-10-18T19:22:38.481-04:00 level=INFO source=server.go:1310 msg="llama runner started in 0.67 seconds"
time=2025-10-18T19:22:38.481-04:00 level=INFO source=sched.go:482 msg="loaded runners" count=1
time=2025-10-18T19:22:38.481-04:00 level=INFO source=server.go:1272 msg="waiting for llama runner to start responding"
time=2025-10-18T19:22:38.481-04:00 level=INFO source=server.go:1310 msg="llama runner started in 0.67 seconds"
init: embeddings required but some input tokens were not marked as outputs -> overriding
output_reserve: reallocating output buffer from size 0.12 MiB to 1.20 MiB
[GIN] 2025/10/18 - 19:22:38 | 200 |    892.3497ms |       127.0.0.1 | POST     "/api/embeddings"
init: embeddings required but some input tokens were not marked as outputs -> overriding
[GIN] 2025/10/18 - 19:24:49 | 200 |     63.1158ms |       127.0.0.1 | POST     "/api/embeddings"
time=2025-10-18T19:29:55.086-04:00 level=INFO source=runner.go:545 msg="failure during GPU discovery" OLLAMA_LIBRARY_PATH="[C:\\Users\\frost\\AppData\\Local\\Programs\\Ollama\\lib\\ollama C:\\Users\\frost\\AppData\\Local\\Programs\\Ollama\\lib\\ollama\\cuda_v12]" extra_envs=[] error="failed to finish discovery before timeout"
time=2025-10-18T19:29:55.086-04:00 level=WARN source=runner.go:347 msg="unable to refresh free memory, using old values"
llama_model_loader: loaded meta data with 23 key-value pairs and 389 tensors from C:\Users\frost\.ollama\models\blobs\sha256-819c2adf5ce6df2b6bd2ae4ca90d2a69f060afeb438d0c171db57daa02e39c3d (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = bert
llama_model_loader: - kv   1:                               general.name str              = mxbai-embed-large-v1
llama_model_loader: - kv   2:                           bert.block_count u32              = 24
llama_model_loader: - kv   3:                        bert.context_length u32              = 512
llama_model_loader: - kv   4:                      bert.embedding_length u32              = 1024
llama_model_loader: - kv   5:                   bert.feed_forward_length u32              = 4096
llama_model_loader: - kv   6:                  bert.attention.head_count u32              = 16
llama_model_loader: - kv   7:          bert.attention.layer_norm_epsilon f32              = 0.000000
llama_model_loader: - kv   8:                          general.file_type u32              = 1
llama_model_loader: - kv   9:                      bert.attention.causal bool             = false
llama_model_loader: - kv  10:                          bert.pooling_type u32              = 2
llama_model_loader: - kv  11:            tokenizer.ggml.token_type_count u32              = 2
llama_model_loader: - kv  12:                tokenizer.ggml.bos_token_id u32              = 101
llama_model_loader: - kv  13:                tokenizer.ggml.eos_token_id u32              = 102
llama_model_loader: - kv  14:                       tokenizer.ggml.model str              = bert
llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,30522]   = ["[PAD]", "[unused0]", "[unused1]", "...
llama_model_loader: - kv  16:                      tokenizer.ggml.scores arr[f32,30522]   = [-1000.000000, -1000.000000, -1000.00...
llama_model_loader: - kv  17:                  tokenizer.ggml.token_type arr[i32,30522]   = [3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  18:            tokenizer.ggml.unknown_token_id u32              = 100
llama_model_loader: - kv  19:          tokenizer.ggml.seperator_token_id u32              = 102
llama_model_loader: - kv  20:            tokenizer.ggml.padding_token_id u32              = 0
llama_model_loader: - kv  21:                tokenizer.ggml.cls_token_id u32              = 101
llama_model_loader: - kv  22:               tokenizer.ggml.mask_token_id u32              = 103
llama_model_loader: - type  f32:  243 tensors
llama_model_loader: - type  f16:  146 tensors
print_info: file format = GGUF V3 (latest)
print_info: file type   = F16
print_info: file size   = 637.85 MiB (16.02 BPW) 
load: special_eos_id is not in special_eog_ids - the tokenizer config may be incorrect
load: printing all EOG tokens:
load:   - 102 ('[SEP]')
load: special tokens cache size = 5
load: token to piece cache size = 0.2032 MB
print_info: arch             = bert
print_info: vocab_only       = 1
print_info: model type       = ?B
print_info: model params     = 334.09 M
print_info: general.name     = mxbai-embed-large-v1
print_info: vocab type       = WPM
print_info: n_vocab          = 30522
print_info: n_merges         = 0
print_info: BOS token        = 101 '[CLS]'
print_info: EOS token        = 102 '[SEP]'
print_info: UNK token        = 100 '[UNK]'
print_info: SEP token        = 102 '[SEP]'
print_info: PAD token        = 0 '[PAD]'
print_info: MASK token       = 103 '[MASK]'
print_info: LF token         = 0 '[PAD]'
print_info: EOG token        = 102 '[SEP]'
print_info: max token length = 21
llama_model_load: vocab only - skipping tensors
time=2025-10-18T19:32:45.688-04:00 level=INFO source=cpu_windows.go:139 msg=packages count=1
time=2025-10-18T19:32:45.688-04:00 level=INFO source=cpu_windows.go:155 msg="efficiency cores detected" maxEfficiencyClass=1
time=2025-10-18T19:32:45.688-04:00 level=INFO source=cpu_windows.go:186 msg="" package=0 cores=12 efficiency=4 threads=20
time=2025-10-18T19:32:45.689-04:00 level=INFO source=server.go:400 msg="starting runner" cmd="C:\\Users\\frost\\AppData\\Local\\Programs\\Ollama\\ollama.exe runner --model C:\\Users\\frost\\.ollama\\models\\blobs\\sha256-819c2adf5ce6df2b6bd2ae4ca90d2a69f060afeb438d0c171db57daa02e39c3d --port 49363"
time=2025-10-18T19:32:45.693-04:00 level=INFO source=cpu_windows.go:139 msg=packages count=1
time=2025-10-18T19:32:45.693-04:00 level=INFO source=cpu_windows.go:155 msg="efficiency cores detected" maxEfficiencyClass=1
time=2025-10-18T19:32:45.693-04:00 level=INFO source=cpu_windows.go:186 msg="" package=0 cores=12 efficiency=4 threads=20
time=2025-10-18T19:32:45.693-04:00 level=INFO source=server.go:505 msg="system memory" total="63.8 GiB" free="40.1 GiB" free_swap="40.0 GiB"
time=2025-10-18T19:32:45.693-04:00 level=INFO source=memory.go:36 msg="new model will fit in available VRAM across minimum required GPUs, loading" model=C:\Users\frost\.ollama\models\blobs\sha256-819c2adf5ce6df2b6bd2ae4ca90d2a69f060afeb438d0c171db57daa02e39c3d library=CUDA parallel=1 required="1.1 GiB" gpus=1
time=2025-10-18T19:32:45.693-04:00 level=INFO source=server.go:545 msg=offload library=CUDA layers.requested=-1 layers.model=25 layers.offload=25 layers.split=[25] memory.available="[10.5 GiB]" memory.gpu_overhead="0 B" memory.required.full="1.1 GiB" memory.required.partial="1.1 GiB" memory.required.kv="3.0 MiB" memory.required.allocations="[1.1 GiB]" memory.weights.total="636.8 MiB" memory.weights.repeating="577.2 MiB" memory.weights.nonrepeating="59.6 MiB" memory.graph.full="8.0 MiB" memory.graph.partial="8.0 MiB"
time=2025-10-18T19:32:45.726-04:00 level=INFO source=runner.go:893 msg="starting go runner"
load_backend: loaded CPU backend from C:\Users\frost\AppData\Local\Programs\Ollama\lib\ollama\ggml-cpu-alderlake.dll
ggml_cuda_init: GGML_CUDA_FORCE_MMQ:    no
ggml_cuda_init: GGML_CUDA_FORCE_CUBLAS: no
ggml_cuda_init: found 1 CUDA devices:
  Device 0: NVIDIA GeForce RTX 5070, compute capability 12.0, VMM: yes, ID: GPU-b28645fa-aaf6-86a1-ab1b-ef8d61df29f2
load_backend: loaded CUDA backend from C:\Users\frost\AppData\Local\Programs\Ollama\lib\ollama\cuda_v12\ggml-cuda.dll
time=2025-10-18T19:32:45.822-04:00 level=INFO source=ggml.go:104 msg=system CPU.0.SSE3=1 CPU.0.SSSE3=1 CPU.0.AVX=1 CPU.0.AVX_VNNI=1 CPU.0.AVX2=1 CPU.0.F16C=1 CPU.0.FMA=1 CPU.0.BMI2=1 CPU.0.LLAMAFILE=1 CPU.1.LLAMAFILE=1 CUDA.0.ARCHS=500,520,600,610,700,750,800,860,870,890,900,1200 CUDA.0.USE_GRAPHS=1 CUDA.0.PEER_MAX_BATCH_SIZE=128 compiler=cgo(clang)
time=2025-10-18T19:32:45.823-04:00 level=INFO source=runner.go:929 msg="Server listening on 127.0.0.1:49363"
time=2025-10-18T19:32:45.833-04:00 level=INFO source=server.go:1272 msg="waiting for llama runner to start responding"
time=2025-10-18T19:32:45.832-04:00 level=INFO source=runner.go:828 msg=load request="{Operation:commit LoraPath:[] Parallel:1 BatchSize:512 FlashAttention:false KvSize:512 KvCacheType: NumThreads:8 GPULayers:25[ID:GPU-b28645fa-aaf6-86a1-ab1b-ef8d61df29f2 Layers:25(0..24)] MultiUserCache:false ProjectorPath: MainGPU:0 UseMmap:false}"
time=2025-10-18T19:32:45.833-04:00 level=INFO source=server.go:1306 msg="waiting for server to become available" status="llm server loading model"
ggml_backend_cuda_device_get_memory utilizing NVML memory reporting free: 11303477248 total: 12820938752
llama_model_load_from_file_impl: using device CUDA0 (NVIDIA GeForce RTX 5070) (0000:01:00.0) - 10779 MiB free
llama_model_loader: loaded meta data with 23 key-value pairs and 389 tensors from C:\Users\frost\.ollama\models\blobs\sha256-819c2adf5ce6df2b6bd2ae4ca90d2a69f060afeb438d0c171db57daa02e39c3d (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = bert
llama_model_loader: - kv   1:                               general.name str              = mxbai-embed-large-v1
llama_model_loader: - kv   2:                           bert.block_count u32              = 24
llama_model_loader: - kv   3:                        bert.context_length u32              = 512
llama_model_loader: - kv   4:                      bert.embedding_length u32              = 1024
llama_model_loader: - kv   5:                   bert.feed_forward_length u32              = 4096
llama_model_loader: - kv   6:                  bert.attention.head_count u32              = 16
llama_model_loader: - kv   7:          bert.attention.layer_norm_epsilon f32              = 0.000000
llama_model_loader: - kv   8:                          general.file_type u32              = 1
llama_model_loader: - kv   9:                      bert.attention.causal bool             = false
llama_model_loader: - kv  10:                          bert.pooling_type u32              = 2
llama_model_loader: - kv  11:            tokenizer.ggml.token_type_count u32              = 2
llama_model_loader: - kv  12:                tokenizer.ggml.bos_token_id u32              = 101
llama_model_loader: - kv  13:                tokenizer.ggml.eos_token_id u32              = 102
llama_model_loader: - kv  14:                       tokenizer.ggml.model str              = bert
llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,30522]   = ["[PAD]", "[unused0]", "[unused1]", "...
llama_model_loader: - kv  16:                      tokenizer.ggml.scores arr[f32,30522]   = [-1000.000000, -1000.000000, -1000.00...
llama_model_loader: - kv  17:                  tokenizer.ggml.token_type arr[i32,30522]   = [3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  18:            tokenizer.ggml.unknown_token_id u32              = 100
llama_model_loader: - kv  19:          tokenizer.ggml.seperator_token_id u32              = 102
llama_model_loader: - kv  20:            tokenizer.ggml.padding_token_id u32              = 0
llama_model_loader: - kv  21:                tokenizer.ggml.cls_token_id u32              = 101
llama_model_loader: - kv  22:               tokenizer.ggml.mask_token_id u32              = 103
llama_model_loader: - type  f32:  243 tensors
llama_model_loader: - type  f16:  146 tensors
print_info: file format = GGUF V3 (latest)
print_info: file type   = F16
print_info: file size   = 637.85 MiB (16.02 BPW) 
load: special_eos_id is not in special_eog_ids - the tokenizer config may be incorrect
load: printing all EOG tokens:
load:   - 102 ('[SEP]')
load: special tokens cache size = 5
load: token to piece cache size = 0.2032 MB
print_info: arch             = bert
print_info: vocab_only       = 0
print_info: n_ctx_train      = 512
print_info: n_embd           = 1024
print_info: n_layer          = 24
print_info: n_head           = 16
print_info: n_head_kv        = 16
print_info: n_rot            = 64
print_info: n_swa            = 0
print_info: is_swa_any       = 0
print_info: n_embd_head_k    = 64
print_info: n_embd_head_v    = 64
print_info: n_gqa            = 1
print_info: n_embd_k_gqa     = 1024
print_info: n_embd_v_gqa     = 1024
print_info: f_norm_eps       = 1.0e-12
print_info: f_norm_rms_eps   = 0.0e+00
print_info: f_clamp_kqv      = 0.0e+00
print_info: f_max_alibi_bias = 0.0e+00
print_info: f_logit_scale    = 0.0e+00
print_info: f_attn_scale     = 0.0e+00
print_info: n_ff             = 4096
print_info: n_expert         = 0
print_info: n_expert_used    = 0
print_info: causal attn      = 0
print_info: pooling type     = 2
print_info: rope type        = 2
print_info: rope scaling     = linear
print_info: freq_base_train  = 10000.0
print_info: freq_scale_train = 1
print_info: n_ctx_orig_yarn  = 512
print_info: rope_finetuned   = unknown
print_info: model type       = 335M
print_info: model params     = 334.09 M
print_info: general.name     = mxbai-embed-large-v1
print_info: vocab type       = WPM
print_info: n_vocab          = 30522
print_info: n_merges         = 0
print_info: BOS token        = 101 '[CLS]'
print_info: EOS token        = 102 '[SEP]'
print_info: UNK token        = 100 '[UNK]'
print_info: SEP token        = 102 '[SEP]'
print_info: PAD token        = 0 '[PAD]'
print_info: MASK token       = 103 '[MASK]'
print_info: LF token         = 0 '[PAD]'
print_info: EOG token        = 102 '[SEP]'
print_info: max token length = 21
load_tensors: loading model tensors, this can take a while... (mmap = false)
load_tensors: offloading 24 repeating layers to GPU
load_tensors: offloading output layer to GPU
load_tensors: offloaded 25/25 layers to GPU
load_tensors:    CUDA_Host model buffer size =    60.63 MiB
load_tensors:        CUDA0 model buffer size =   577.22 MiB
llama_init_from_model: model default pooling_type is [2], but [-1] was specified
llama_context: constructing llama_context
llama_context: n_seq_max     = 1
llama_context: n_ctx         = 512
llama_context: n_ctx_per_seq = 512
llama_context: n_batch       = 512
llama_context: n_ubatch      = 512
llama_context: causal_attn   = 0
llama_context: flash_attn    = disabled
llama_context: kv_unified    = false
llama_context: freq_base     = 10000.0
llama_context: freq_scale    = 1
llama_context:  CUDA_Host  output buffer size =     0.12 MiB
llama_context:      CUDA0 compute buffer size =    27.01 MiB
llama_context:  CUDA_Host compute buffer size =     5.01 MiB
llama_context: graph nodes  = 851
llama_context: graph splits = 4 (with bs=512), 2 (with bs=1)
time=2025-10-18T19:32:46.334-04:00 level=INFO source=server.go:1310 msg="llama runner started in 0.64 seconds"
time=2025-10-18T19:32:46.334-04:00 level=INFO source=sched.go:482 msg="loaded runners" count=1
time=2025-10-18T19:32:46.334-04:00 level=INFO source=server.go:1272 msg="waiting for llama runner to start responding"
time=2025-10-18T19:32:46.335-04:00 level=INFO source=server.go:1310 msg="llama runner started in 0.65 seconds"
init: embeddings required but some input tokens were not marked as outputs -> overriding
output_reserve: reallocating output buffer from size 0.12 MiB to 1.20 MiB
[GIN] 2025/10/18 - 19:32:46 | 200 |     848.949ms |       127.0.0.1 | POST     "/api/embeddings"
init: embeddings required but some input tokens were not marked as outputs -> overriding
[GIN] 2025/10/18 - 19:32:46 | 200 |     34.0428ms |       127.0.0.1 | POST     "/api/embeddings"
time=2025-10-18T19:37:52.037-04:00 level=INFO source=runner.go:545 msg="failure during GPU discovery" OLLAMA_LIBRARY_PATH="[C:\\Users\\frost\\AppData\\Local\\Programs\\Ollama\\lib\\ollama C:\\Users\\frost\\AppData\\Local\\Programs\\Ollama\\lib\\ollama\\cuda_v12]" extra_envs=[] error="failed to finish discovery before timeout"
time=2025-10-18T19:37:52.037-04:00 level=WARN source=runner.go:347 msg="unable to refresh free memory, using old values"
[GIN] 2025/10/18 - 19:58:44 | 200 |      7.8076ms |       127.0.0.1 | POST     "/api/show"
[GIN] 2025/10/18 - 19:58:44 | 200 |      7.6662ms |       127.0.0.1 | POST     "/api/show"
llama_model_loader: loaded meta data with 25 key-value pairs and 291 tensors from C:\Users\frost\.ollama\models\blobs\sha256-f5074b1221da0f5a2910d33b642efa5b9eb58cfdddca1c79e16d7ad28aa2b31f (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = llama
llama_model_loader: - kv   1:                               general.name str              = Mistral-7B-Instruct-v0.3
llama_model_loader: - kv   2:                          llama.block_count u32              = 32
llama_model_loader: - kv   3:                       llama.context_length u32              = 32768
llama_model_loader: - kv   4:                     llama.embedding_length u32              = 4096
llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 14336
llama_model_loader: - kv   6:                 llama.attention.head_count u32              = 32
llama_model_loader: - kv   7:              llama.attention.head_count_kv u32              = 8
llama_model_loader: - kv   8:                       llama.rope.freq_base f32              = 1000000.000000
llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010
llama_model_loader: - kv  10:                          general.file_type u32              = 15
llama_model_loader: - kv  11:                           llama.vocab_size u32              = 32768
llama_model_loader: - kv  12:                 llama.rope.dimension_count u32              = 128
llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = llama
llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = default
llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,32768]   = ["<unk>", "<s>", "</s>", "[INST]", "[...
llama_model_loader: - kv  16:                      tokenizer.ggml.scores arr[f32,32768]   = [0.000000, 0.000000, 0.000000, 0.0000...
llama_model_loader: - kv  17:                  tokenizer.ggml.token_type arr[i32,32768]   = [2, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, ...
llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 1
llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 2
llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
llama_model_loader: - kv  21:               tokenizer.ggml.add_bos_token bool             = true
llama_model_loader: - kv  22:               tokenizer.ggml.add_eos_token bool             = false
llama_model_loader: - kv  23:                    tokenizer.chat_template str              = {{ bos_token }}{% for message in mess...
llama_model_loader: - kv  24:               general.quantization_version u32              = 2
llama_model_loader: - type  f32:   65 tensors
llama_model_loader: - type q4_K:  193 tensors
llama_model_loader: - type q6_K:   33 tensors
print_info: file format = GGUF V3 (latest)
print_info: file type   = Q4_K - Medium
print_info: file size   = 4.07 GiB (4.83 BPW) 
load: special_eos_id is not in special_eog_ids - the tokenizer config may be incorrect
load: printing all EOG tokens:
load:   - 2 ('</s>')
load: special tokens cache size = 771
load: token to piece cache size = 0.1731 MB
print_info: arch             = llama
print_info: vocab_only       = 1
print_info: model type       = ?B
print_info: model params     = 7.25 B
print_info: general.name     = Mistral-7B-Instruct-v0.3
print_info: vocab type       = SPM
print_info: n_vocab          = 32768
print_info: n_merges         = 0
print_info: BOS token        = 1 '<s>'
print_info: EOS token        = 2 '</s>'
print_info: UNK token        = 0 '<unk>'
print_info: LF token         = 781 '<0x0A>'
print_info: EOG token        = 2 '</s>'
print_info: max token length = 48
llama_model_load: vocab only - skipping tensors
time=2025-10-18T19:58:44.812-04:00 level=INFO source=cpu_windows.go:139 msg=packages count=1
time=2025-10-18T19:58:44.812-04:00 level=INFO source=cpu_windows.go:155 msg="efficiency cores detected" maxEfficiencyClass=1
time=2025-10-18T19:58:44.812-04:00 level=INFO source=cpu_windows.go:186 msg="" package=0 cores=12 efficiency=4 threads=20
time=2025-10-18T19:58:44.813-04:00 level=INFO source=server.go:400 msg="starting runner" cmd="C:\\Users\\frost\\AppData\\Local\\Programs\\Ollama\\ollama.exe runner --model C:\\Users\\frost\\.ollama\\models\\blobs\\sha256-f5074b1221da0f5a2910d33b642efa5b9eb58cfdddca1c79e16d7ad28aa2b31f --port 58114"
time=2025-10-18T19:58:44.817-04:00 level=INFO source=cpu_windows.go:139 msg=packages count=1
time=2025-10-18T19:58:44.817-04:00 level=INFO source=cpu_windows.go:155 msg="efficiency cores detected" maxEfficiencyClass=1
time=2025-10-18T19:58:44.817-04:00 level=INFO source=cpu_windows.go:186 msg="" package=0 cores=12 efficiency=4 threads=20
time=2025-10-18T19:58:44.817-04:00 level=INFO source=server.go:505 msg="system memory" total="63.8 GiB" free="39.5 GiB" free_swap="39.5 GiB"
time=2025-10-18T19:58:44.817-04:00 level=INFO source=memory.go:36 msg="new model will fit in available VRAM across minimum required GPUs, loading" model=C:\Users\frost\.ollama\models\blobs\sha256-f5074b1221da0f5a2910d33b642efa5b9eb58cfdddca1c79e16d7ad28aa2b31f library=CUDA parallel=1 required="5.4 GiB" gpus=1
time=2025-10-18T19:58:44.817-04:00 level=INFO source=server.go:545 msg=offload library=CUDA layers.requested=-1 layers.model=33 layers.offload=33 layers.split=[33] memory.available="[10.4 GiB]" memory.gpu_overhead="0 B" memory.required.full="5.4 GiB" memory.required.partial="5.4 GiB" memory.required.kv="512.0 MiB" memory.required.allocations="[5.4 GiB]" memory.weights.total="4.0 GiB" memory.weights.repeating="3.9 GiB" memory.weights.nonrepeating="105.0 MiB" memory.graph.full="296.0 MiB" memory.graph.partial="305.0 MiB"
time=2025-10-18T19:58:44.853-04:00 level=INFO source=runner.go:893 msg="starting go runner"
load_backend: loaded CPU backend from C:\Users\frost\AppData\Local\Programs\Ollama\lib\ollama\ggml-cpu-alderlake.dll
ggml_cuda_init: GGML_CUDA_FORCE_MMQ:    no
ggml_cuda_init: GGML_CUDA_FORCE_CUBLAS: no
ggml_cuda_init: found 1 CUDA devices:
  Device 0: NVIDIA GeForce RTX 5070, compute capability 12.0, VMM: yes, ID: GPU-b28645fa-aaf6-86a1-ab1b-ef8d61df29f2
load_backend: loaded CUDA backend from C:\Users\frost\AppData\Local\Programs\Ollama\lib\ollama\cuda_v12\ggml-cuda.dll
time=2025-10-18T19:58:44.974-04:00 level=INFO source=ggml.go:104 msg=system CPU.0.SSE3=1 CPU.0.SSSE3=1 CPU.0.AVX=1 CPU.0.AVX_VNNI=1 CPU.0.AVX2=1 CPU.0.F16C=1 CPU.0.FMA=1 CPU.0.BMI2=1 CPU.0.LLAMAFILE=1 CPU.1.LLAMAFILE=1 CUDA.0.ARCHS=500,520,600,610,700,750,800,860,870,890,900,1200 CUDA.0.USE_GRAPHS=1 CUDA.0.PEER_MAX_BATCH_SIZE=128 compiler=cgo(clang)
time=2025-10-18T19:58:44.976-04:00 level=INFO source=runner.go:929 msg="Server listening on 127.0.0.1:58114"
time=2025-10-18T19:58:44.977-04:00 level=INFO source=runner.go:828 msg=load request="{Operation:commit LoraPath:[] Parallel:1 BatchSize:512 FlashAttention:false KvSize:4096 KvCacheType: NumThreads:8 GPULayers:33[ID:GPU-b28645fa-aaf6-86a1-ab1b-ef8d61df29f2 Layers:33(0..32)] MultiUserCache:false ProjectorPath: MainGPU:0 UseMmap:false}"
time=2025-10-18T19:58:44.977-04:00 level=INFO source=server.go:1272 msg="waiting for llama runner to start responding"
time=2025-10-18T19:58:44.977-04:00 level=INFO source=server.go:1306 msg="waiting for server to become available" status="llm server loading model"
ggml_backend_cuda_device_get_memory utilizing NVML memory reporting free: 11212357632 total: 12820938752
llama_model_load_from_file_impl: using device CUDA0 (NVIDIA GeForce RTX 5070) (0000:01:00.0) - 10692 MiB free
llama_model_loader: loaded meta data with 25 key-value pairs and 291 tensors from C:\Users\frost\.ollama\models\blobs\sha256-f5074b1221da0f5a2910d33b642efa5b9eb58cfdddca1c79e16d7ad28aa2b31f (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = llama
llama_model_loader: - kv   1:                               general.name str              = Mistral-7B-Instruct-v0.3
llama_model_loader: - kv   2:                          llama.block_count u32              = 32
llama_model_loader: - kv   3:                       llama.context_length u32              = 32768
llama_model_loader: - kv   4:                     llama.embedding_length u32              = 4096
llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 14336
llama_model_loader: - kv   6:                 llama.attention.head_count u32              = 32
llama_model_loader: - kv   7:              llama.attention.head_count_kv u32              = 8
llama_model_loader: - kv   8:                       llama.rope.freq_base f32              = 1000000.000000
llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010
llama_model_loader: - kv  10:                          general.file_type u32              = 15
llama_model_loader: - kv  11:                           llama.vocab_size u32              = 32768
llama_model_loader: - kv  12:                 llama.rope.dimension_count u32              = 128
llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = llama
llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = default
llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,32768]   = ["<unk>", "<s>", "</s>", "[INST]", "[...
llama_model_loader: - kv  16:                      tokenizer.ggml.scores arr[f32,32768]   = [0.000000, 0.000000, 0.000000, 0.0000...
llama_model_loader: - kv  17:                  tokenizer.ggml.token_type arr[i32,32768]   = [2, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, ...
llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 1
llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 2
llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
llama_model_loader: - kv  21:               tokenizer.ggml.add_bos_token bool             = true
llama_model_loader: - kv  22:               tokenizer.ggml.add_eos_token bool             = false
llama_model_loader: - kv  23:                    tokenizer.chat_template str              = {{ bos_token }}{% for message in mess...
llama_model_loader: - kv  24:               general.quantization_version u32              = 2
llama_model_loader: - type  f32:   65 tensors
llama_model_loader: - type q4_K:  193 tensors
llama_model_loader: - type q6_K:   33 tensors
print_info: file format = GGUF V3 (latest)
print_info: file type   = Q4_K - Medium
print_info: file size   = 4.07 GiB (4.83 BPW) 
load: special_eos_id is not in special_eog_ids - the tokenizer config may be incorrect
load: printing all EOG tokens:
load:   - 2 ('</s>')
load: special tokens cache size = 771
load: token to piece cache size = 0.1731 MB
print_info: arch             = llama
print_info: vocab_only       = 0
print_info: n_ctx_train      = 32768
print_info: n_embd           = 4096
print_info: n_layer          = 32
print_info: n_head           = 32
print_info: n_head_kv        = 8
print_info: n_rot            = 128
print_info: n_swa            = 0
print_info: is_swa_any       = 0
print_info: n_embd_head_k    = 128
print_info: n_embd_head_v    = 128
print_info: n_gqa            = 4
print_info: n_embd_k_gqa     = 1024
print_info: n_embd_v_gqa     = 1024
print_info: f_norm_eps       = 0.0e+00
print_info: f_norm_rms_eps   = 1.0e-05
print_info: f_clamp_kqv      = 0.0e+00
print_info: f_max_alibi_bias = 0.0e+00
print_info: f_logit_scale    = 0.0e+00
print_info: f_attn_scale     = 0.0e+00
print_info: n_ff             = 14336
print_info: n_expert         = 0
print_info: n_expert_used    = 0
print_info: causal attn      = 1
print_info: pooling type     = 0
print_info: rope type        = 0
print_info: rope scaling     = linear
print_info: freq_base_train  = 1000000.0
print_info: freq_scale_train = 1
print_info: n_ctx_orig_yarn  = 32768
print_info: rope_finetuned   = unknown
print_info: model type       = 7B
print_info: model params     = 7.25 B
print_info: general.name     = Mistral-7B-Instruct-v0.3
print_info: vocab type       = SPM
print_info: n_vocab          = 32768
print_info: n_merges         = 0
print_info: BOS token        = 1 '<s>'
print_info: EOS token        = 2 '</s>'
print_info: UNK token        = 0 '<unk>'
print_info: LF token         = 781 '<0x0A>'
print_info: EOG token        = 2 '</s>'
print_info: max token length = 48
load_tensors: loading model tensors, this can take a while... (mmap = false)
load_tensors: offloading 32 repeating layers to GPU
load_tensors: offloading output layer to GPU
load_tensors: offloaded 33/33 layers to GPU
load_tensors:        CUDA0 model buffer size =  4097.52 MiB
load_tensors:          CPU model buffer size =    72.00 MiB
llama_init_from_model: model default pooling_type is [0], but [-1] was specified
llama_context: constructing llama_context
llama_context: n_seq_max     = 1
llama_context: n_ctx         = 4096
llama_context: n_ctx_per_seq = 4096
llama_context: n_batch       = 512
llama_context: n_ubatch      = 512
llama_context: causal_attn   = 1
llama_context: flash_attn    = disabled
llama_context: kv_unified    = false
llama_context: freq_base     = 1000000.0
llama_context: freq_scale    = 1
llama_context: n_ctx_per_seq (4096) < n_ctx_train (32768) -- the full capacity of the model will not be utilized
llama_context:  CUDA_Host  output buffer size =     0.14 MiB
llama_kv_cache:      CUDA0 KV buffer size =   512.00 MiB
llama_kv_cache: size =  512.00 MiB (  4096 cells,  32 layers,  1/1 seqs), K (f16):  256.00 MiB, V (f16):  256.00 MiB
llama_context:      CUDA0 compute buffer size =   300.01 MiB
llama_context:  CUDA_Host compute buffer size =    20.01 MiB
llama_context: graph nodes  = 1158
llama_context: graph splits = 2
time=2025-10-18T19:58:46.480-04:00 level=INFO source=server.go:1310 msg="llama runner started in 1.67 seconds"
time=2025-10-18T19:58:46.480-04:00 level=INFO source=sched.go:482 msg="loaded runners" count=1
time=2025-10-18T19:58:46.480-04:00 level=INFO source=server.go:1272 msg="waiting for llama runner to start responding"
time=2025-10-18T19:58:46.480-04:00 level=INFO source=server.go:1310 msg="llama runner started in 1.67 seconds"
[GIN] 2025/10/18 - 19:58:47 | 200 |     2.941893s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/18 - 20:29:53 | 200 |      8.2118ms |       127.0.0.1 | POST     "/api/show"
[GIN] 2025/10/18 - 20:29:53 | 200 |      7.2446ms |       127.0.0.1 | POST     "/api/show"
llama_model_loader: loaded meta data with 25 key-value pairs and 291 tensors from C:\Users\frost\.ollama\models\blobs\sha256-f5074b1221da0f5a2910d33b642efa5b9eb58cfdddca1c79e16d7ad28aa2b31f (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = llama
llama_model_loader: - kv   1:                               general.name str              = Mistral-7B-Instruct-v0.3
llama_model_loader: - kv   2:                          llama.block_count u32              = 32
llama_model_loader: - kv   3:                       llama.context_length u32              = 32768
llama_model_loader: - kv   4:                     llama.embedding_length u32              = 4096
llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 14336
llama_model_loader: - kv   6:                 llama.attention.head_count u32              = 32
llama_model_loader: - kv   7:              llama.attention.head_count_kv u32              = 8
llama_model_loader: - kv   8:                       llama.rope.freq_base f32              = 1000000.000000
llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010
llama_model_loader: - kv  10:                          general.file_type u32              = 15
llama_model_loader: - kv  11:                           llama.vocab_size u32              = 32768
llama_model_loader: - kv  12:                 llama.rope.dimension_count u32              = 128
llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = llama
llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = default
llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,32768]   = ["<unk>", "<s>", "</s>", "[INST]", "[...
llama_model_loader: - kv  16:                      tokenizer.ggml.scores arr[f32,32768]   = [0.000000, 0.000000, 0.000000, 0.0000...
llama_model_loader: - kv  17:                  tokenizer.ggml.token_type arr[i32,32768]   = [2, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, ...
llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 1
llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 2
llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
llama_model_loader: - kv  21:               tokenizer.ggml.add_bos_token bool             = true
llama_model_loader: - kv  22:               tokenizer.ggml.add_eos_token bool             = false
llama_model_loader: - kv  23:                    tokenizer.chat_template str              = {{ bos_token }}{% for message in mess...
llama_model_loader: - kv  24:               general.quantization_version u32              = 2
llama_model_loader: - type  f32:   65 tensors
llama_model_loader: - type q4_K:  193 tensors
llama_model_loader: - type q6_K:   33 tensors
print_info: file format = GGUF V3 (latest)
print_info: file type   = Q4_K - Medium
print_info: file size   = 4.07 GiB (4.83 BPW) 
load: special_eos_id is not in special_eog_ids - the tokenizer config may be incorrect
load: printing all EOG tokens:
load:   - 2 ('</s>')
load: special tokens cache size = 771
load: token to piece cache size = 0.1731 MB
print_info: arch             = llama
print_info: vocab_only       = 1
print_info: model type       = ?B
print_info: model params     = 7.25 B
print_info: general.name     = Mistral-7B-Instruct-v0.3
print_info: vocab type       = SPM
print_info: n_vocab          = 32768
print_info: n_merges         = 0
print_info: BOS token        = 1 '<s>'
print_info: EOS token        = 2 '</s>'
print_info: UNK token        = 0 '<unk>'
print_info: LF token         = 781 '<0x0A>'
print_info: EOG token        = 2 '</s>'
print_info: max token length = 48
llama_model_load: vocab only - skipping tensors
time=2025-10-18T20:29:53.607-04:00 level=INFO source=cpu_windows.go:139 msg=packages count=1
time=2025-10-18T20:29:53.607-04:00 level=INFO source=cpu_windows.go:155 msg="efficiency cores detected" maxEfficiencyClass=1
time=2025-10-18T20:29:53.607-04:00 level=INFO source=cpu_windows.go:186 msg="" package=0 cores=12 efficiency=4 threads=20
time=2025-10-18T20:29:53.607-04:00 level=INFO source=server.go:400 msg="starting runner" cmd="C:\\Users\\frost\\AppData\\Local\\Programs\\Ollama\\ollama.exe runner --model C:\\Users\\frost\\.ollama\\models\\blobs\\sha256-f5074b1221da0f5a2910d33b642efa5b9eb58cfdddca1c79e16d7ad28aa2b31f --port 52127"
time=2025-10-18T20:29:53.611-04:00 level=INFO source=cpu_windows.go:139 msg=packages count=1
time=2025-10-18T20:29:53.611-04:00 level=INFO source=cpu_windows.go:155 msg="efficiency cores detected" maxEfficiencyClass=1
time=2025-10-18T20:29:53.611-04:00 level=INFO source=cpu_windows.go:186 msg="" package=0 cores=12 efficiency=4 threads=20
time=2025-10-18T20:29:53.611-04:00 level=INFO source=server.go:505 msg="system memory" total="63.8 GiB" free="37.7 GiB" free_swap="37.3 GiB"
time=2025-10-18T20:29:53.612-04:00 level=INFO source=memory.go:36 msg="new model will fit in available VRAM across minimum required GPUs, loading" model=C:\Users\frost\.ollama\models\blobs\sha256-f5074b1221da0f5a2910d33b642efa5b9eb58cfdddca1c79e16d7ad28aa2b31f library=CUDA parallel=1 required="5.4 GiB" gpus=1
time=2025-10-18T20:29:53.612-04:00 level=INFO source=server.go:545 msg=offload library=CUDA layers.requested=-1 layers.model=33 layers.offload=33 layers.split=[33] memory.available="[10.4 GiB]" memory.gpu_overhead="0 B" memory.required.full="5.4 GiB" memory.required.partial="5.4 GiB" memory.required.kv="512.0 MiB" memory.required.allocations="[5.4 GiB]" memory.weights.total="4.0 GiB" memory.weights.repeating="3.9 GiB" memory.weights.nonrepeating="105.0 MiB" memory.graph.full="296.0 MiB" memory.graph.partial="305.0 MiB"
time=2025-10-18T20:29:53.657-04:00 level=INFO source=runner.go:893 msg="starting go runner"
load_backend: loaded CPU backend from C:\Users\frost\AppData\Local\Programs\Ollama\lib\ollama\ggml-cpu-alderlake.dll
ggml_cuda_init: GGML_CUDA_FORCE_MMQ:    no
ggml_cuda_init: GGML_CUDA_FORCE_CUBLAS: no
ggml_cuda_init: found 1 CUDA devices:
  Device 0: NVIDIA GeForce RTX 5070, compute capability 12.0, VMM: yes, ID: GPU-b28645fa-aaf6-86a1-ab1b-ef8d61df29f2
load_backend: loaded CUDA backend from C:\Users\frost\AppData\Local\Programs\Ollama\lib\ollama\cuda_v12\ggml-cuda.dll
time=2025-10-18T20:29:53.769-04:00 level=INFO source=ggml.go:104 msg=system CPU.0.SSE3=1 CPU.0.SSSE3=1 CPU.0.AVX=1 CPU.0.AVX_VNNI=1 CPU.0.AVX2=1 CPU.0.F16C=1 CPU.0.FMA=1 CPU.0.BMI2=1 CPU.0.LLAMAFILE=1 CPU.1.LLAMAFILE=1 CUDA.0.ARCHS=500,520,600,610,700,750,800,860,870,890,900,1200 CUDA.0.USE_GRAPHS=1 CUDA.0.PEER_MAX_BATCH_SIZE=128 compiler=cgo(clang)
time=2025-10-18T20:29:53.769-04:00 level=INFO source=runner.go:929 msg="Server listening on 127.0.0.1:52127"
time=2025-10-18T20:29:53.772-04:00 level=INFO source=runner.go:828 msg=load request="{Operation:commit LoraPath:[] Parallel:1 BatchSize:512 FlashAttention:false KvSize:4096 KvCacheType: NumThreads:8 GPULayers:33[ID:GPU-b28645fa-aaf6-86a1-ab1b-ef8d61df29f2 Layers:33(0..32)] MultiUserCache:false ProjectorPath: MainGPU:0 UseMmap:false}"
time=2025-10-18T20:29:53.772-04:00 level=INFO source=server.go:1272 msg="waiting for llama runner to start responding"
time=2025-10-18T20:29:53.772-04:00 level=INFO source=server.go:1306 msg="waiting for server to become available" status="llm server loading model"
ggml_backend_cuda_device_get_memory utilizing NVML memory reporting free: 11133173760 total: 12820938752
llama_model_load_from_file_impl: using device CUDA0 (NVIDIA GeForce RTX 5070) (0000:01:00.0) - 10617 MiB free
llama_model_loader: loaded meta data with 25 key-value pairs and 291 tensors from C:\Users\frost\.ollama\models\blobs\sha256-f5074b1221da0f5a2910d33b642efa5b9eb58cfdddca1c79e16d7ad28aa2b31f (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = llama
llama_model_loader: - kv   1:                               general.name str              = Mistral-7B-Instruct-v0.3
llama_model_loader: - kv   2:                          llama.block_count u32              = 32
llama_model_loader: - kv   3:                       llama.context_length u32              = 32768
llama_model_loader: - kv   4:                     llama.embedding_length u32              = 4096
llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 14336
llama_model_loader: - kv   6:                 llama.attention.head_count u32              = 32
llama_model_loader: - kv   7:              llama.attention.head_count_kv u32              = 8
llama_model_loader: - kv   8:                       llama.rope.freq_base f32              = 1000000.000000
llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010
llama_model_loader: - kv  10:                          general.file_type u32              = 15
llama_model_loader: - kv  11:                           llama.vocab_size u32              = 32768
llama_model_loader: - kv  12:                 llama.rope.dimension_count u32              = 128
llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = llama
llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = default
llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,32768]   = ["<unk>", "<s>", "</s>", "[INST]", "[...
llama_model_loader: - kv  16:                      tokenizer.ggml.scores arr[f32,32768]   = [0.000000, 0.000000, 0.000000, 0.0000...
llama_model_loader: - kv  17:                  tokenizer.ggml.token_type arr[i32,32768]   = [2, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, ...
llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 1
llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 2
llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
llama_model_loader: - kv  21:               tokenizer.ggml.add_bos_token bool             = true
llama_model_loader: - kv  22:               tokenizer.ggml.add_eos_token bool             = false
llama_model_loader: - kv  23:                    tokenizer.chat_template str              = {{ bos_token }}{% for message in mess...
llama_model_loader: - kv  24:               general.quantization_version u32              = 2
llama_model_loader: - type  f32:   65 tensors
llama_model_loader: - type q4_K:  193 tensors
llama_model_loader: - type q6_K:   33 tensors
print_info: file format = GGUF V3 (latest)
print_info: file type   = Q4_K - Medium
print_info: file size   = 4.07 GiB (4.83 BPW) 
load: special_eos_id is not in special_eog_ids - the tokenizer config may be incorrect
load: printing all EOG tokens:
load:   - 2 ('</s>')
load: special tokens cache size = 771
load: token to piece cache size = 0.1731 MB
print_info: arch             = llama
print_info: vocab_only       = 0
print_info: n_ctx_train      = 32768
print_info: n_embd           = 4096
print_info: n_layer          = 32
print_info: n_head           = 32
print_info: n_head_kv        = 8
print_info: n_rot            = 128
print_info: n_swa            = 0
print_info: is_swa_any       = 0
print_info: n_embd_head_k    = 128
print_info: n_embd_head_v    = 128
print_info: n_gqa            = 4
print_info: n_embd_k_gqa     = 1024
print_info: n_embd_v_gqa     = 1024
print_info: f_norm_eps       = 0.0e+00
print_info: f_norm_rms_eps   = 1.0e-05
print_info: f_clamp_kqv      = 0.0e+00
print_info: f_max_alibi_bias = 0.0e+00
print_info: f_logit_scale    = 0.0e+00
print_info: f_attn_scale     = 0.0e+00
print_info: n_ff             = 14336
print_info: n_expert         = 0
print_info: n_expert_used    = 0
print_info: causal attn      = 1
print_info: pooling type     = 0
print_info: rope type        = 0
print_info: rope scaling     = linear
print_info: freq_base_train  = 1000000.0
print_info: freq_scale_train = 1
print_info: n_ctx_orig_yarn  = 32768
print_info: rope_finetuned   = unknown
print_info: model type       = 7B
print_info: model params     = 7.25 B
print_info: general.name     = Mistral-7B-Instruct-v0.3
print_info: vocab type       = SPM
print_info: n_vocab          = 32768
print_info: n_merges         = 0
print_info: BOS token        = 1 '<s>'
print_info: EOS token        = 2 '</s>'
print_info: UNK token        = 0 '<unk>'
print_info: LF token         = 781 '<0x0A>'
print_info: EOG token        = 2 '</s>'
print_info: max token length = 48
load_tensors: loading model tensors, this can take a while... (mmap = false)
load_tensors: offloading 32 repeating layers to GPU
load_tensors: offloading output layer to GPU
load_tensors: offloaded 33/33 layers to GPU
load_tensors:        CUDA0 model buffer size =  4097.52 MiB
load_tensors:          CPU model buffer size =    72.00 MiB
llama_init_from_model: model default pooling_type is [0], but [-1] was specified
llama_context: constructing llama_context
llama_context: n_seq_max     = 1
llama_context: n_ctx         = 4096
llama_context: n_ctx_per_seq = 4096
llama_context: n_batch       = 512
llama_context: n_ubatch      = 512
llama_context: causal_attn   = 1
llama_context: flash_attn    = disabled
llama_context: kv_unified    = false
llama_context: freq_base     = 1000000.0
llama_context: freq_scale    = 1
llama_context: n_ctx_per_seq (4096) < n_ctx_train (32768) -- the full capacity of the model will not be utilized
llama_context:  CUDA_Host  output buffer size =     0.14 MiB
llama_kv_cache:      CUDA0 KV buffer size =   512.00 MiB
llama_kv_cache: size =  512.00 MiB (  4096 cells,  32 layers,  1/1 seqs), K (f16):  256.00 MiB, V (f16):  256.00 MiB
llama_context:      CUDA0 compute buffer size =   300.01 MiB
llama_context:  CUDA_Host compute buffer size =    20.01 MiB
llama_context: graph nodes  = 1158
llama_context: graph splits = 2
time=2025-10-18T20:29:55.274-04:00 level=INFO source=server.go:1310 msg="llama runner started in 1.67 seconds"
time=2025-10-18T20:29:55.274-04:00 level=INFO source=sched.go:482 msg="loaded runners" count=1
time=2025-10-18T20:29:55.274-04:00 level=INFO source=server.go:1272 msg="waiting for llama runner to start responding"
time=2025-10-18T20:29:55.274-04:00 level=INFO source=server.go:1310 msg="llama runner started in 1.67 seconds"
[GIN] 2025/10/18 - 20:29:56 | 200 |    2.9050094s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/18 - 20:30:05 | 200 |      9.0927ms |       127.0.0.1 | POST     "/api/show"
[GIN] 2025/10/18 - 20:30:05 | 200 |       7.194ms |       127.0.0.1 | POST     "/api/show"
[GIN] 2025/10/18 - 20:30:06 | 200 |    898.0659ms |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/18 - 20:31:57 | 200 |      7.2836ms |       127.0.0.1 | POST     "/api/show"
[GIN] 2025/10/18 - 20:31:57 | 200 |       7.739ms |       127.0.0.1 | POST     "/api/show"
[GIN] 2025/10/18 - 20:32:00 | 200 |    2.4173687s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/18 - 20:32:23 | 200 |      7.8663ms |       127.0.0.1 | POST     "/api/show"
[GIN] 2025/10/18 - 20:32:23 | 200 |      7.1828ms |       127.0.0.1 | POST     "/api/show"
[GIN] 2025/10/18 - 20:32:25 | 200 |    1.8577994s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/18 - 20:32:40 | 200 |      7.7983ms |       127.0.0.1 | POST     "/api/show"
[GIN] 2025/10/18 - 20:32:40 | 200 |      8.2594ms |       127.0.0.1 | POST     "/api/show"
[GIN] 2025/10/18 - 20:32:42 | 200 |    2.2487213s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/18 - 20:32:57 | 200 |      7.3566ms |       127.0.0.1 | POST     "/api/show"
[GIN] 2025/10/18 - 20:32:57 | 200 |      6.7115ms |       127.0.0.1 | POST     "/api/show"
[GIN] 2025/10/18 - 20:32:59 | 200 |    2.5517727s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/19 - 13:22:11 | 200 |     54.6309ms |       127.0.0.1 | GET      "/v1/models"
[GIN] 2025/10/19 - 13:24:40 | 200 |            0s |       127.0.0.1 | HEAD     "/"
[GIN] 2025/10/19 - 13:24:41 | 200 |     796.128ms |       127.0.0.1 | POST     "/api/pull"
[GIN] 2025/10/19 - 13:26:05 | 200 |            0s |       127.0.0.1 | HEAD     "/"
[GIN] 2025/10/19 - 13:26:05 | 200 |    200.9897ms |       127.0.0.1 | POST     "/api/pull"
[GIN] 2025/10/19 - 13:26:29 | 200 |            0s |       127.0.0.1 | HEAD     "/"
[GIN] 2025/10/19 - 13:26:29 | 200 |       2.012ms |       127.0.0.1 | GET      "/api/tags"
[GIN] 2025/10/19 - 13:26:51 | 200 |            0s |       127.0.0.1 | HEAD     "/"
time=2025-10-19T13:26:52.062-04:00 level=INFO source=download.go:177 msg="downloading 6a0746a1ec1a in 16 291 MB part(s)"
time=2025-10-19T13:31:10.020-04:00 level=INFO source=download.go:177 msg="downloading 4fa551d4f938 in 1 12 KB part(s)"
time=2025-10-19T13:31:11.268-04:00 level=INFO source=download.go:177 msg="downloading 8ab4849b038c in 1 254 B part(s)"
time=2025-10-19T13:31:12.431-04:00 level=INFO source=download.go:177 msg="downloading 577073ffcc6c in 1 110 B part(s)"
time=2025-10-19T13:31:13.678-04:00 level=INFO source=download.go:177 msg="downloading 3f8eb4da87fa in 1 485 B part(s)"
[GIN] 2025/10/19 - 13:31:21 | 200 |         4m30s |       127.0.0.1 | POST     "/api/pull"
[GIN] 2025/10/19 - 13:32:04 | 200 |            0s |       127.0.0.1 | HEAD     "/"
[GIN] 2025/10/19 - 13:32:04 | 200 |      2.7747ms |       127.0.0.1 | GET      "/api/tags"
[GIN] 2025/10/19 - 13:39:26 | 200 |            0s |       127.0.0.1 | HEAD     "/"
[GIN] 2025/10/19 - 13:39:26 | 404 |       525.8µs |       127.0.0.1 | POST     "/api/show"
time=2025-10-19T13:39:27.741-04:00 level=INFO source=download.go:177 msg="downloading 6a0746a1ec1a in 16 291 MB part(s)"
[GIN] 2025/10/19 - 13:41:02 | 204 |            0s |       127.0.0.1 | OPTIONS  "/v1/models"
[GIN] 2025/10/19 - 13:41:02 | 200 |            0s |       127.0.0.1 | GET      "/v1/models"
time=2025-10-19T13:43:51.119-04:00 level=INFO source=download.go:177 msg="downloading 4fa551d4f938 in 1 12 KB part(s)"
time=2025-10-19T13:43:52.345-04:00 level=INFO source=download.go:177 msg="downloading 8ab4849b038c in 1 254 B part(s)"
time=2025-10-19T13:43:53.490-04:00 level=INFO source=download.go:177 msg="downloading 577073ffcc6c in 1 110 B part(s)"
time=2025-10-19T13:43:54.662-04:00 level=INFO source=download.go:177 msg="downloading 3f8eb4da87fa in 1 485 B part(s)"
[GIN] 2025/10/19 - 13:44:02 | 200 |         4m35s |       127.0.0.1 | POST     "/api/pull"
[GIN] 2025/10/19 - 13:44:02 | 200 |     37.9734ms |       127.0.0.1 | POST     "/api/show"
llama_model_loader: loaded meta data with 22 key-value pairs and 291 tensors from C:\Users\frost\.ollama\models\blobs\sha256-6a0746a1ec1aef3e7ec53868f220ff6e389f6f8ef87a01d77c96807de94ca2aa (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = llama
llama_model_loader: - kv   1:                               general.name str              = Meta-Llama-3-8B-Instruct
llama_model_loader: - kv   2:                          llama.block_count u32              = 32
llama_model_loader: - kv   3:                       llama.context_length u32              = 8192
llama_model_loader: - kv   4:                     llama.embedding_length u32              = 4096
llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 14336
llama_model_loader: - kv   6:                 llama.attention.head_count u32              = 32
llama_model_loader: - kv   7:              llama.attention.head_count_kv u32              = 8
llama_model_loader: - kv   8:                       llama.rope.freq_base f32              = 500000.000000
llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010
llama_model_loader: - kv  10:                          general.file_type u32              = 2
llama_model_loader: - kv  11:                           llama.vocab_size u32              = 128256
llama_model_loader: - kv  12:                 llama.rope.dimension_count u32              = 128
llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = llama-bpe
llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,128256]  = ["!", "\"", "#", "$", "%", "&", "'", ...
llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,128256]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,280147]  = ["Ġ Ġ", "Ġ ĠĠĠ", "ĠĠ ĠĠ", "...
llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 128000
llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 128009
llama_model_loader: - kv  20:                    tokenizer.chat_template str              = {% set loop_messages = messages %}{% ...
llama_model_loader: - kv  21:               general.quantization_version u32              = 2
llama_model_loader: - type  f32:   65 tensors
llama_model_loader: - type q4_0:  225 tensors
llama_model_loader: - type q6_K:    1 tensors
print_info: file format = GGUF V3 (latest)
print_info: file type   = Q4_0
print_info: file size   = 4.33 GiB (4.64 BPW) 
load: printing all EOG tokens:
load:   - 128001 ('<|end_of_text|>')
load:   - 128009 ('<|eot_id|>')
load: special tokens cache size = 256
load: token to piece cache size = 0.8000 MB
print_info: arch             = llama
print_info: vocab_only       = 1
print_info: model type       = ?B
print_info: model params     = 8.03 B
print_info: general.name     = Meta-Llama-3-8B-Instruct
print_info: vocab type       = BPE
print_info: n_vocab          = 128256
print_info: n_merges         = 280147
print_info: BOS token        = 128000 '<|begin_of_text|>'
print_info: EOS token        = 128009 '<|eot_id|>'
print_info: EOT token        = 128001 '<|end_of_text|>'
print_info: LF token         = 198 'Ċ'
print_info: EOG token        = 128001 '<|end_of_text|>'
print_info: EOG token        = 128009 '<|eot_id|>'
print_info: max token length = 256
llama_model_load: vocab only - skipping tensors
time=2025-10-19T13:44:04.159-04:00 level=INFO source=cpu_windows.go:139 msg=packages count=1
time=2025-10-19T13:44:04.159-04:00 level=INFO source=cpu_windows.go:155 msg="efficiency cores detected" maxEfficiencyClass=1
time=2025-10-19T13:44:04.159-04:00 level=INFO source=cpu_windows.go:186 msg="" package=0 cores=12 efficiency=4 threads=20
time=2025-10-19T13:44:04.160-04:00 level=INFO source=server.go:400 msg="starting runner" cmd="C:\\Users\\frost\\AppData\\Local\\Programs\\Ollama\\ollama.exe runner --model C:\\Users\\frost\\.ollama\\models\\blobs\\sha256-6a0746a1ec1aef3e7ec53868f220ff6e389f6f8ef87a01d77c96807de94ca2aa --port 51062"
time=2025-10-19T13:44:04.165-04:00 level=INFO source=cpu_windows.go:139 msg=packages count=1
time=2025-10-19T13:44:04.165-04:00 level=INFO source=cpu_windows.go:155 msg="efficiency cores detected" maxEfficiencyClass=1
time=2025-10-19T13:44:04.165-04:00 level=INFO source=cpu_windows.go:186 msg="" package=0 cores=12 efficiency=4 threads=20
time=2025-10-19T13:44:04.165-04:00 level=INFO source=server.go:505 msg="system memory" total="63.8 GiB" free="45.1 GiB" free_swap="39.8 GiB"
time=2025-10-19T13:44:04.165-04:00 level=INFO source=memory.go:36 msg="new model will fit in available VRAM across minimum required GPUs, loading" model=C:\Users\frost\.ollama\models\blobs\sha256-6a0746a1ec1aef3e7ec53868f220ff6e389f6f8ef87a01d77c96807de94ca2aa library=CUDA parallel=1 required="5.4 GiB" gpus=1
time=2025-10-19T13:44:04.165-04:00 level=INFO source=server.go:545 msg=offload library=CUDA layers.requested=-1 layers.model=33 layers.offload=33 layers.split=[33] memory.available="[10.5 GiB]" memory.gpu_overhead="0 B" memory.required.full="5.4 GiB" memory.required.partial="5.4 GiB" memory.required.kv="512.0 MiB" memory.required.allocations="[5.4 GiB]" memory.weights.total="4.1 GiB" memory.weights.repeating="3.7 GiB" memory.weights.nonrepeating="411.0 MiB" memory.graph.full="296.0 MiB" memory.graph.partial="677.5 MiB"
time=2025-10-19T13:44:04.204-04:00 level=INFO source=runner.go:893 msg="starting go runner"
load_backend: loaded CPU backend from C:\Users\frost\AppData\Local\Programs\Ollama\lib\ollama\ggml-cpu-alderlake.dll
ggml_cuda_init: GGML_CUDA_FORCE_MMQ:    no
ggml_cuda_init: GGML_CUDA_FORCE_CUBLAS: no
ggml_cuda_init: found 1 CUDA devices:
  Device 0: NVIDIA GeForce RTX 5070, compute capability 12.0, VMM: yes, ID: GPU-b28645fa-aaf6-86a1-ab1b-ef8d61df29f2
load_backend: loaded CUDA backend from C:\Users\frost\AppData\Local\Programs\Ollama\lib\ollama\cuda_v12\ggml-cuda.dll
time=2025-10-19T13:44:04.309-04:00 level=INFO source=ggml.go:104 msg=system CPU.0.SSE3=1 CPU.0.SSSE3=1 CPU.0.AVX=1 CPU.0.AVX_VNNI=1 CPU.0.AVX2=1 CPU.0.F16C=1 CPU.0.FMA=1 CPU.0.BMI2=1 CPU.0.LLAMAFILE=1 CPU.1.LLAMAFILE=1 CUDA.0.ARCHS=500,520,600,610,700,750,800,860,870,890,900,1200 CUDA.0.USE_GRAPHS=1 CUDA.0.PEER_MAX_BATCH_SIZE=128 compiler=cgo(clang)
time=2025-10-19T13:44:04.310-04:00 level=INFO source=runner.go:929 msg="Server listening on 127.0.0.1:51062"
time=2025-10-19T13:44:04.313-04:00 level=INFO source=runner.go:828 msg=load request="{Operation:commit LoraPath:[] Parallel:1 BatchSize:512 FlashAttention:false KvSize:4096 KvCacheType: NumThreads:8 GPULayers:33[ID:GPU-b28645fa-aaf6-86a1-ab1b-ef8d61df29f2 Layers:33(0..32)] MultiUserCache:false ProjectorPath: MainGPU:0 UseMmap:false}"
time=2025-10-19T13:44:04.313-04:00 level=INFO source=server.go:1272 msg="waiting for llama runner to start responding"
time=2025-10-19T13:44:04.313-04:00 level=INFO source=server.go:1306 msg="waiting for server to become available" status="llm server loading model"
ggml_backend_cuda_device_get_memory utilizing NVML memory reporting free: 11225448448 total: 12820938752
llama_model_load_from_file_impl: using device CUDA0 (NVIDIA GeForce RTX 5070) (0000:01:00.0) - 10705 MiB free
llama_model_loader: loaded meta data with 22 key-value pairs and 291 tensors from C:\Users\frost\.ollama\models\blobs\sha256-6a0746a1ec1aef3e7ec53868f220ff6e389f6f8ef87a01d77c96807de94ca2aa (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = llama
llama_model_loader: - kv   1:                               general.name str              = Meta-Llama-3-8B-Instruct
llama_model_loader: - kv   2:                          llama.block_count u32              = 32
llama_model_loader: - kv   3:                       llama.context_length u32              = 8192
llama_model_loader: - kv   4:                     llama.embedding_length u32              = 4096
llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 14336
llama_model_loader: - kv   6:                 llama.attention.head_count u32              = 32
llama_model_loader: - kv   7:              llama.attention.head_count_kv u32              = 8
llama_model_loader: - kv   8:                       llama.rope.freq_base f32              = 500000.000000
llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010
llama_model_loader: - kv  10:                          general.file_type u32              = 2
llama_model_loader: - kv  11:                           llama.vocab_size u32              = 128256
llama_model_loader: - kv  12:                 llama.rope.dimension_count u32              = 128
llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = llama-bpe
llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,128256]  = ["!", "\"", "#", "$", "%", "&", "'", ...
llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,128256]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,280147]  = ["Ġ Ġ", "Ġ ĠĠĠ", "ĠĠ ĠĠ", "...
llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 128000
llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 128009
llama_model_loader: - kv  20:                    tokenizer.chat_template str              = {% set loop_messages = messages %}{% ...
llama_model_loader: - kv  21:               general.quantization_version u32              = 2
llama_model_loader: - type  f32:   65 tensors
llama_model_loader: - type q4_0:  225 tensors
llama_model_loader: - type q6_K:    1 tensors
print_info: file format = GGUF V3 (latest)
print_info: file type   = Q4_0
print_info: file size   = 4.33 GiB (4.64 BPW) 
load: printing all EOG tokens:
load:   - 128001 ('<|end_of_text|>')
load:   - 128009 ('<|eot_id|>')
load: special tokens cache size = 256
load: token to piece cache size = 0.8000 MB
print_info: arch             = llama
print_info: vocab_only       = 0
print_info: n_ctx_train      = 8192
print_info: n_embd           = 4096
print_info: n_layer          = 32
print_info: n_head           = 32
print_info: n_head_kv        = 8
print_info: n_rot            = 128
print_info: n_swa            = 0
print_info: is_swa_any       = 0
print_info: n_embd_head_k    = 128
print_info: n_embd_head_v    = 128
print_info: n_gqa            = 4
print_info: n_embd_k_gqa     = 1024
print_info: n_embd_v_gqa     = 1024
print_info: f_norm_eps       = 0.0e+00
print_info: f_norm_rms_eps   = 1.0e-05
print_info: f_clamp_kqv      = 0.0e+00
print_info: f_max_alibi_bias = 0.0e+00
print_info: f_logit_scale    = 0.0e+00
print_info: f_attn_scale     = 0.0e+00
print_info: n_ff             = 14336
print_info: n_expert         = 0
print_info: n_expert_used    = 0
print_info: causal attn      = 1
print_info: pooling type     = 0
print_info: rope type        = 0
print_info: rope scaling     = linear
print_info: freq_base_train  = 500000.0
print_info: freq_scale_train = 1
print_info: n_ctx_orig_yarn  = 8192
print_info: rope_finetuned   = unknown
print_info: model type       = 8B
print_info: model params     = 8.03 B
print_info: general.name     = Meta-Llama-3-8B-Instruct
print_info: vocab type       = BPE
print_info: n_vocab          = 128256
print_info: n_merges         = 280147
print_info: BOS token        = 128000 '<|begin_of_text|>'
print_info: EOS token        = 128009 '<|eot_id|>'
print_info: EOT token        = 128001 '<|end_of_text|>'
print_info: LF token         = 198 'Ċ'
print_info: EOG token        = 128001 '<|end_of_text|>'
print_info: EOG token        = 128009 '<|eot_id|>'
print_info: max token length = 256
load_tensors: loading model tensors, this can take a while... (mmap = false)
load_tensors: offloading 32 repeating layers to GPU
load_tensors: offloading output layer to GPU
load_tensors: offloaded 33/33 layers to GPU
load_tensors:    CUDA_Host model buffer size =   281.81 MiB
load_tensors:        CUDA0 model buffer size =  4155.99 MiB
llama_init_from_model: model default pooling_type is [0], but [-1] was specified
llama_context: constructing llama_context
llama_context: n_seq_max     = 1
llama_context: n_ctx         = 4096
llama_context: n_ctx_per_seq = 4096
llama_context: n_batch       = 512
llama_context: n_ubatch      = 512
llama_context: causal_attn   = 1
llama_context: flash_attn    = disabled
llama_context: kv_unified    = false
llama_context: freq_base     = 500000.0
llama_context: freq_scale    = 1
llama_context: n_ctx_per_seq (4096) < n_ctx_train (8192) -- the full capacity of the model will not be utilized
llama_context:  CUDA_Host  output buffer size =     0.50 MiB
llama_kv_cache:      CUDA0 KV buffer size =   512.00 MiB
llama_kv_cache: size =  512.00 MiB (  4096 cells,  32 layers,  1/1 seqs), K (f16):  256.00 MiB, V (f16):  256.00 MiB
llama_context:      CUDA0 compute buffer size =   300.01 MiB
llama_context:  CUDA_Host compute buffer size =    20.01 MiB
llama_context: graph nodes  = 1158
llama_context: graph splits = 2
time=2025-10-19T13:44:06.065-04:00 level=INFO source=server.go:1310 msg="llama runner started in 1.90 seconds"
time=2025-10-19T13:44:06.065-04:00 level=INFO source=sched.go:482 msg="loaded runners" count=1
time=2025-10-19T13:44:06.065-04:00 level=INFO source=server.go:1272 msg="waiting for llama runner to start responding"
time=2025-10-19T13:44:06.065-04:00 level=INFO source=server.go:1310 msg="llama runner started in 1.90 seconds"
[GIN] 2025/10/19 - 13:44:06 | 200 |    3.6175689s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/10/19 - 13:44:44 | 200 |    2.8422678s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/19 - 13:44:51 | 200 |    2.4873209s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/19 - 13:45:00 | 200 |    827.5423ms |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/19 - 13:45:08 | 200 |     1.512078s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/19 - 13:53:27 | 200 |            0s |       127.0.0.1 | HEAD     "/"
[GIN] 2025/10/19 - 13:53:27 | 200 |     37.8856ms |       127.0.0.1 | POST     "/api/show"
llama_model_loader: loaded meta data with 22 key-value pairs and 291 tensors from C:\Users\frost\.ollama\models\blobs\sha256-6a0746a1ec1aef3e7ec53868f220ff6e389f6f8ef87a01d77c96807de94ca2aa (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = llama
llama_model_loader: - kv   1:                               general.name str              = Meta-Llama-3-8B-Instruct
llama_model_loader: - kv   2:                          llama.block_count u32              = 32
llama_model_loader: - kv   3:                       llama.context_length u32              = 8192
llama_model_loader: - kv   4:                     llama.embedding_length u32              = 4096
llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 14336
llama_model_loader: - kv   6:                 llama.attention.head_count u32              = 32
llama_model_loader: - kv   7:              llama.attention.head_count_kv u32              = 8
llama_model_loader: - kv   8:                       llama.rope.freq_base f32              = 500000.000000
llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010
llama_model_loader: - kv  10:                          general.file_type u32              = 2
llama_model_loader: - kv  11:                           llama.vocab_size u32              = 128256
llama_model_loader: - kv  12:                 llama.rope.dimension_count u32              = 128
llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = llama-bpe
llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,128256]  = ["!", "\"", "#", "$", "%", "&", "'", ...
llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,128256]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,280147]  = ["Ġ Ġ", "Ġ ĠĠĠ", "ĠĠ ĠĠ", "...
llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 128000
llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 128009
llama_model_loader: - kv  20:                    tokenizer.chat_template str              = {% set loop_messages = messages %}{% ...
llama_model_loader: - kv  21:               general.quantization_version u32              = 2
llama_model_loader: - type  f32:   65 tensors
llama_model_loader: - type q4_0:  225 tensors
llama_model_loader: - type q6_K:    1 tensors
print_info: file format = GGUF V3 (latest)
print_info: file type   = Q4_0
print_info: file size   = 4.33 GiB (4.64 BPW) 
load: printing all EOG tokens:
load:   - 128001 ('<|end_of_text|>')
load:   - 128009 ('<|eot_id|>')
load: special tokens cache size = 256
load: token to piece cache size = 0.8000 MB
print_info: arch             = llama
print_info: vocab_only       = 1
print_info: model type       = ?B
print_info: model params     = 8.03 B
print_info: general.name     = Meta-Llama-3-8B-Instruct
print_info: vocab type       = BPE
print_info: n_vocab          = 128256
print_info: n_merges         = 280147
print_info: BOS token        = 128000 '<|begin_of_text|>'
print_info: EOS token        = 128009 '<|eot_id|>'
print_info: EOT token        = 128001 '<|end_of_text|>'
print_info: LF token         = 198 'Ċ'
print_info: EOG token        = 128001 '<|end_of_text|>'
print_info: EOG token        = 128009 '<|eot_id|>'
print_info: max token length = 256
llama_model_load: vocab only - skipping tensors
time=2025-10-19T13:53:27.994-04:00 level=INFO source=cpu_windows.go:139 msg=packages count=1
time=2025-10-19T13:53:27.994-04:00 level=INFO source=cpu_windows.go:155 msg="efficiency cores detected" maxEfficiencyClass=1
time=2025-10-19T13:53:27.994-04:00 level=INFO source=cpu_windows.go:186 msg="" package=0 cores=12 efficiency=4 threads=20
time=2025-10-19T13:53:27.994-04:00 level=INFO source=server.go:400 msg="starting runner" cmd="C:\\Users\\frost\\AppData\\Local\\Programs\\Ollama\\ollama.exe runner --model C:\\Users\\frost\\.ollama\\models\\blobs\\sha256-6a0746a1ec1aef3e7ec53868f220ff6e389f6f8ef87a01d77c96807de94ca2aa --port 65336"
time=2025-10-19T13:53:27.998-04:00 level=INFO source=cpu_windows.go:139 msg=packages count=1
time=2025-10-19T13:53:27.998-04:00 level=INFO source=cpu_windows.go:155 msg="efficiency cores detected" maxEfficiencyClass=1
time=2025-10-19T13:53:27.998-04:00 level=INFO source=cpu_windows.go:186 msg="" package=0 cores=12 efficiency=4 threads=20
time=2025-10-19T13:53:27.998-04:00 level=INFO source=server.go:505 msg="system memory" total="63.8 GiB" free="45.1 GiB" free_swap="39.6 GiB"
time=2025-10-19T13:53:27.998-04:00 level=INFO source=memory.go:36 msg="new model will fit in available VRAM across minimum required GPUs, loading" model=C:\Users\frost\.ollama\models\blobs\sha256-6a0746a1ec1aef3e7ec53868f220ff6e389f6f8ef87a01d77c96807de94ca2aa library=CUDA parallel=1 required="5.4 GiB" gpus=1
time=2025-10-19T13:53:27.999-04:00 level=INFO source=server.go:545 msg=offload library=CUDA layers.requested=-1 layers.model=33 layers.offload=33 layers.split=[33] memory.available="[10.4 GiB]" memory.gpu_overhead="0 B" memory.required.full="5.4 GiB" memory.required.partial="5.4 GiB" memory.required.kv="512.0 MiB" memory.required.allocations="[5.4 GiB]" memory.weights.total="4.1 GiB" memory.weights.repeating="3.7 GiB" memory.weights.nonrepeating="411.0 MiB" memory.graph.full="296.0 MiB" memory.graph.partial="677.5 MiB"
time=2025-10-19T13:53:28.034-04:00 level=INFO source=runner.go:893 msg="starting go runner"
load_backend: loaded CPU backend from C:\Users\frost\AppData\Local\Programs\Ollama\lib\ollama\ggml-cpu-alderlake.dll
ggml_cuda_init: GGML_CUDA_FORCE_MMQ:    no
ggml_cuda_init: GGML_CUDA_FORCE_CUBLAS: no
ggml_cuda_init: found 1 CUDA devices:
  Device 0: NVIDIA GeForce RTX 5070, compute capability 12.0, VMM: yes, ID: GPU-b28645fa-aaf6-86a1-ab1b-ef8d61df29f2
load_backend: loaded CUDA backend from C:\Users\frost\AppData\Local\Programs\Ollama\lib\ollama\cuda_v12\ggml-cuda.dll
time=2025-10-19T13:53:28.146-04:00 level=INFO source=ggml.go:104 msg=system CPU.0.SSE3=1 CPU.0.SSSE3=1 CPU.0.AVX=1 CPU.0.AVX_VNNI=1 CPU.0.AVX2=1 CPU.0.F16C=1 CPU.0.FMA=1 CPU.0.BMI2=1 CPU.0.LLAMAFILE=1 CPU.1.LLAMAFILE=1 CUDA.0.ARCHS=500,520,600,610,700,750,800,860,870,890,900,1200 CUDA.0.USE_GRAPHS=1 CUDA.0.PEER_MAX_BATCH_SIZE=128 compiler=cgo(clang)
time=2025-10-19T13:53:28.146-04:00 level=INFO source=runner.go:929 msg="Server listening on 127.0.0.1:65336"
time=2025-10-19T13:53:28.157-04:00 level=INFO source=runner.go:828 msg=load request="{Operation:commit LoraPath:[] Parallel:1 BatchSize:512 FlashAttention:false KvSize:4096 KvCacheType: NumThreads:8 GPULayers:33[ID:GPU-b28645fa-aaf6-86a1-ab1b-ef8d61df29f2 Layers:33(0..32)] MultiUserCache:false ProjectorPath: MainGPU:0 UseMmap:false}"
time=2025-10-19T13:53:28.157-04:00 level=INFO source=server.go:1272 msg="waiting for llama runner to start responding"
time=2025-10-19T13:53:28.157-04:00 level=INFO source=server.go:1306 msg="waiting for server to become available" status="llm server loading model"
ggml_backend_cuda_device_get_memory utilizing NVML memory reporting free: 11174899712 total: 12820938752
llama_model_load_from_file_impl: using device CUDA0 (NVIDIA GeForce RTX 5070) (0000:01:00.0) - 10657 MiB free
llama_model_loader: loaded meta data with 22 key-value pairs and 291 tensors from C:\Users\frost\.ollama\models\blobs\sha256-6a0746a1ec1aef3e7ec53868f220ff6e389f6f8ef87a01d77c96807de94ca2aa (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = llama
llama_model_loader: - kv   1:                               general.name str              = Meta-Llama-3-8B-Instruct
llama_model_loader: - kv   2:                          llama.block_count u32              = 32
llama_model_loader: - kv   3:                       llama.context_length u32              = 8192
llama_model_loader: - kv   4:                     llama.embedding_length u32              = 4096
llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 14336
llama_model_loader: - kv   6:                 llama.attention.head_count u32              = 32
llama_model_loader: - kv   7:              llama.attention.head_count_kv u32              = 8
llama_model_loader: - kv   8:                       llama.rope.freq_base f32              = 500000.000000
llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010
llama_model_loader: - kv  10:                          general.file_type u32              = 2
llama_model_loader: - kv  11:                           llama.vocab_size u32              = 128256
llama_model_loader: - kv  12:                 llama.rope.dimension_count u32              = 128
llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = llama-bpe
llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,128256]  = ["!", "\"", "#", "$", "%", "&", "'", ...
llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,128256]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,280147]  = ["Ġ Ġ", "Ġ ĠĠĠ", "ĠĠ ĠĠ", "...
llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 128000
llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 128009
llama_model_loader: - kv  20:                    tokenizer.chat_template str              = {% set loop_messages = messages %}{% ...
llama_model_loader: - kv  21:               general.quantization_version u32              = 2
llama_model_loader: - type  f32:   65 tensors
llama_model_loader: - type q4_0:  225 tensors
llama_model_loader: - type q6_K:    1 tensors
print_info: file format = GGUF V3 (latest)
print_info: file type   = Q4_0
print_info: file size   = 4.33 GiB (4.64 BPW) 
load: printing all EOG tokens:
load:   - 128001 ('<|end_of_text|>')
load:   - 128009 ('<|eot_id|>')
load: special tokens cache size = 256
load: token to piece cache size = 0.8000 MB
print_info: arch             = llama
print_info: vocab_only       = 0
print_info: n_ctx_train      = 8192
print_info: n_embd           = 4096
print_info: n_layer          = 32
print_info: n_head           = 32
print_info: n_head_kv        = 8
print_info: n_rot            = 128
print_info: n_swa            = 0
print_info: is_swa_any       = 0
print_info: n_embd_head_k    = 128
print_info: n_embd_head_v    = 128
print_info: n_gqa            = 4
print_info: n_embd_k_gqa     = 1024
print_info: n_embd_v_gqa     = 1024
print_info: f_norm_eps       = 0.0e+00
print_info: f_norm_rms_eps   = 1.0e-05
print_info: f_clamp_kqv      = 0.0e+00
print_info: f_max_alibi_bias = 0.0e+00
print_info: f_logit_scale    = 0.0e+00
print_info: f_attn_scale     = 0.0e+00
print_info: n_ff             = 14336
print_info: n_expert         = 0
print_info: n_expert_used    = 0
print_info: causal attn      = 1
print_info: pooling type     = 0
print_info: rope type        = 0
print_info: rope scaling     = linear
print_info: freq_base_train  = 500000.0
print_info: freq_scale_train = 1
print_info: n_ctx_orig_yarn  = 8192
print_info: rope_finetuned   = unknown
print_info: model type       = 8B
print_info: model params     = 8.03 B
print_info: general.name     = Meta-Llama-3-8B-Instruct
print_info: vocab type       = BPE
print_info: n_vocab          = 128256
print_info: n_merges         = 280147
print_info: BOS token        = 128000 '<|begin_of_text|>'
print_info: EOS token        = 128009 '<|eot_id|>'
print_info: EOT token        = 128001 '<|end_of_text|>'
print_info: LF token         = 198 'Ċ'
print_info: EOG token        = 128001 '<|end_of_text|>'
print_info: EOG token        = 128009 '<|eot_id|>'
print_info: max token length = 256
load_tensors: loading model tensors, this can take a while... (mmap = false)
load_tensors: offloading 32 repeating layers to GPU
load_tensors: offloading output layer to GPU
load_tensors: offloaded 33/33 layers to GPU
load_tensors:    CUDA_Host model buffer size =   281.81 MiB
load_tensors:        CUDA0 model buffer size =  4155.99 MiB
llama_init_from_model: model default pooling_type is [0], but [-1] was specified
llama_context: constructing llama_context
llama_context: n_seq_max     = 1
llama_context: n_ctx         = 4096
llama_context: n_ctx_per_seq = 4096
llama_context: n_batch       = 512
llama_context: n_ubatch      = 512
llama_context: causal_attn   = 1
llama_context: flash_attn    = disabled
llama_context: kv_unified    = false
llama_context: freq_base     = 500000.0
llama_context: freq_scale    = 1
llama_context: n_ctx_per_seq (4096) < n_ctx_train (8192) -- the full capacity of the model will not be utilized
llama_context:  CUDA_Host  output buffer size =     0.50 MiB
llama_kv_cache:      CUDA0 KV buffer size =   512.00 MiB
llama_kv_cache: size =  512.00 MiB (  4096 cells,  32 layers,  1/1 seqs), K (f16):  256.00 MiB, V (f16):  256.00 MiB
llama_context:      CUDA0 compute buffer size =   300.01 MiB
llama_context:  CUDA_Host compute buffer size =    20.01 MiB
llama_context: graph nodes  = 1158
llama_context: graph splits = 2
time=2025-10-19T13:53:29.660-04:00 level=INFO source=server.go:1310 msg="llama runner started in 1.67 seconds"
time=2025-10-19T13:53:29.660-04:00 level=INFO source=sched.go:482 msg="loaded runners" count=1
time=2025-10-19T13:53:29.660-04:00 level=INFO source=server.go:1272 msg="waiting for llama runner to start responding"
time=2025-10-19T13:53:29.660-04:00 level=INFO source=server.go:1310 msg="llama runner started in 1.67 seconds"
[GIN] 2025/10/19 - 13:53:29 | 200 |    2.1089688s |       127.0.0.1 | POST     "/api/generate"
time=2025-10-19T14:08:12.626-04:00 level=INFO source=routes.go:1511 msg="server config" env="map[CUDA_VISIBLE_DEVICES: GGML_VK_VISIBLE_DEVICES: GPU_DEVICE_ORDINAL: HIP_VISIBLE_DEVICES: HSA_OVERRIDE_GFX_VERSION: HTTPS_PROXY: HTTP_PROXY: NO_PROXY: OLLAMA_CONTEXT_LENGTH:4096 OLLAMA_DEBUG:INFO OLLAMA_FLASH_ATTENTION:false OLLAMA_GPU_OVERHEAD:0 OLLAMA_HOST:http://127.0.0.1:11434 OLLAMA_INTEL_GPU:false OLLAMA_KEEP_ALIVE:5m0s OLLAMA_KV_CACHE_TYPE: OLLAMA_LLM_LIBRARY: OLLAMA_LOAD_TIMEOUT:5m0s OLLAMA_MAX_LOADED_MODELS:0 OLLAMA_MAX_QUEUE:512 OLLAMA_MODELS:C:\\Users\\frost\\.ollama\\models OLLAMA_MULTIUSER_CACHE:false OLLAMA_NEW_ENGINE:false OLLAMA_NOHISTORY:false OLLAMA_NOPRUNE:false OLLAMA_NUM_PARALLEL:1 OLLAMA_ORIGINS:[http://localhost https://localhost http://localhost:* https://localhost:* http://127.0.0.1 https://127.0.0.1 http://127.0.0.1:* https://127.0.0.1:* http://0.0.0.0 https://0.0.0.0 http://0.0.0.0:* https://0.0.0.0:* app://* file://* tauri://* vscode-webview://* vscode-file://*] OLLAMA_REMOTES:[ollama.com] OLLAMA_SCHED_SPREAD:false ROCR_VISIBLE_DEVICES:]"
time=2025-10-19T14:08:12.628-04:00 level=INFO source=images.go:522 msg="total blobs: 5"
time=2025-10-19T14:08:12.628-04:00 level=INFO source=images.go:529 msg="total unused blobs removed: 0"
time=2025-10-19T14:08:12.628-04:00 level=INFO source=routes.go:1564 msg="Listening on 127.0.0.1:11434 (version 0.12.6)"
time=2025-10-19T14:08:12.629-04:00 level=INFO source=runner.go:80 msg="discovering available GPUs..."
time=2025-10-19T14:08:14.027-04:00 level=INFO source=types.go:112 msg="inference compute" id=GPU-b28645fa-aaf6-86a1-ab1b-ef8d61df29f2 library=CUDA compute=12.0 name=CUDA0 description="NVIDIA GeForce RTX 5070" libdirs=ollama,cuda_v12 driver=12.9 pci_id=01:00.0 type=discrete total="11.9 GiB" available="10.4 GiB"
time=2025-10-19T14:08:14.027-04:00 level=INFO source=routes.go:1605 msg="entering low vram mode" "total vram"="11.9 GiB" threshold="20.0 GiB"
time=2025-10-19T14:08:27.414-04:00 level=INFO source=routes.go:1511 msg="server config" env="map[CUDA_VISIBLE_DEVICES: GGML_VK_VISIBLE_DEVICES: GPU_DEVICE_ORDINAL: HIP_VISIBLE_DEVICES: HSA_OVERRIDE_GFX_VERSION: HTTPS_PROXY: HTTP_PROXY: NO_PROXY: OLLAMA_CONTEXT_LENGTH:4096 OLLAMA_DEBUG:INFO OLLAMA_FLASH_ATTENTION:false OLLAMA_GPU_OVERHEAD:0 OLLAMA_HOST:http://127.0.0.1:11434 OLLAMA_INTEL_GPU:false OLLAMA_KEEP_ALIVE:5m0s OLLAMA_KV_CACHE_TYPE: OLLAMA_LLM_LIBRARY: OLLAMA_LOAD_TIMEOUT:5m0s OLLAMA_MAX_LOADED_MODELS:0 OLLAMA_MAX_QUEUE:512 OLLAMA_MODELS:C:\\Users\\frost\\.ollama\\models OLLAMA_MULTIUSER_CACHE:false OLLAMA_NEW_ENGINE:false OLLAMA_NOHISTORY:false OLLAMA_NOPRUNE:false OLLAMA_NUM_PARALLEL:1 OLLAMA_ORIGINS:[http://localhost https://localhost http://localhost:* https://localhost:* http://127.0.0.1 https://127.0.0.1 http://127.0.0.1:* https://127.0.0.1:* http://0.0.0.0 https://0.0.0.0 http://0.0.0.0:* https://0.0.0.0:* app://* file://* tauri://* vscode-webview://* vscode-file://*] OLLAMA_REMOTES:[ollama.com] OLLAMA_SCHED_SPREAD:false ROCR_VISIBLE_DEVICES:]"
time=2025-10-19T14:08:27.415-04:00 level=INFO source=images.go:522 msg="total blobs: 5"
time=2025-10-19T14:08:27.415-04:00 level=INFO source=images.go:529 msg="total unused blobs removed: 0"
time=2025-10-19T14:08:27.416-04:00 level=INFO source=routes.go:1564 msg="Listening on 127.0.0.1:11434 (version 0.12.6)"
time=2025-10-19T14:08:27.416-04:00 level=INFO source=runner.go:80 msg="discovering available GPUs..."
time=2025-10-19T14:08:28.028-04:00 level=INFO source=types.go:112 msg="inference compute" id=GPU-b28645fa-aaf6-86a1-ab1b-ef8d61df29f2 library=CUDA compute=12.0 name=CUDA0 description="NVIDIA GeForce RTX 5070" libdirs=ollama,cuda_v12 driver=12.9 pci_id=01:00.0 type=discrete total="11.9 GiB" available="10.4 GiB"
time=2025-10-19T14:08:28.028-04:00 level=INFO source=routes.go:1605 msg="entering low vram mode" "total vram"="11.9 GiB" threshold="20.0 GiB"
time=2025-10-19T14:09:00.882-04:00 level=INFO source=routes.go:1511 msg="server config" env="map[CUDA_VISIBLE_DEVICES: GGML_VK_VISIBLE_DEVICES: GPU_DEVICE_ORDINAL: HIP_VISIBLE_DEVICES: HSA_OVERRIDE_GFX_VERSION: HTTPS_PROXY: HTTP_PROXY: NO_PROXY: OLLAMA_CONTEXT_LENGTH:4096 OLLAMA_DEBUG:INFO OLLAMA_FLASH_ATTENTION:false OLLAMA_GPU_OVERHEAD:0 OLLAMA_HOST:http://127.0.0.1:11434 OLLAMA_INTEL_GPU:false OLLAMA_KEEP_ALIVE:5m0s OLLAMA_KV_CACHE_TYPE: OLLAMA_LLM_LIBRARY: OLLAMA_LOAD_TIMEOUT:5m0s OLLAMA_MAX_LOADED_MODELS:0 OLLAMA_MAX_QUEUE:512 OLLAMA_MODELS:C:\\Users\\frost\\.ollama\\models OLLAMA_MULTIUSER_CACHE:false OLLAMA_NEW_ENGINE:false OLLAMA_NOHISTORY:false OLLAMA_NOPRUNE:false OLLAMA_NUM_PARALLEL:1 OLLAMA_ORIGINS:[http://localhost https://localhost http://localhost:* https://localhost:* http://127.0.0.1 https://127.0.0.1 http://127.0.0.1:* https://127.0.0.1:* http://0.0.0.0 https://0.0.0.0 http://0.0.0.0:* https://0.0.0.0:* app://* file://* tauri://* vscode-webview://* vscode-file://*] OLLAMA_REMOTES:[ollama.com] OLLAMA_SCHED_SPREAD:false ROCR_VISIBLE_DEVICES:]"
time=2025-10-19T14:09:00.883-04:00 level=INFO source=images.go:522 msg="total blobs: 5"
time=2025-10-19T14:09:00.883-04:00 level=INFO source=images.go:529 msg="total unused blobs removed: 0"
time=2025-10-19T14:09:00.884-04:00 level=INFO source=routes.go:1564 msg="Listening on 127.0.0.1:11434 (version 0.12.6)"
time=2025-10-19T14:09:00.884-04:00 level=INFO source=runner.go:80 msg="discovering available GPUs..."
time=2025-10-19T14:09:01.533-04:00 level=INFO source=types.go:112 msg="inference compute" id=GPU-b28645fa-aaf6-86a1-ab1b-ef8d61df29f2 library=CUDA compute=12.0 name=CUDA0 description="NVIDIA GeForce RTX 5070" libdirs=ollama,cuda_v12 driver=12.9 pci_id=01:00.0 type=discrete total="11.9 GiB" available="10.4 GiB"
time=2025-10-19T14:09:01.533-04:00 level=INFO source=routes.go:1605 msg="entering low vram mode" "total vram"="11.9 GiB" threshold="20.0 GiB"
time=2025-10-19T14:09:09.696-04:00 level=INFO source=routes.go:1511 msg="server config" env="map[CUDA_VISIBLE_DEVICES: GGML_VK_VISIBLE_DEVICES: GPU_DEVICE_ORDINAL: HIP_VISIBLE_DEVICES: HSA_OVERRIDE_GFX_VERSION: HTTPS_PROXY: HTTP_PROXY: NO_PROXY: OLLAMA_CONTEXT_LENGTH:4096 OLLAMA_DEBUG:INFO OLLAMA_FLASH_ATTENTION:false OLLAMA_GPU_OVERHEAD:0 OLLAMA_HOST:http://127.0.0.1:11434 OLLAMA_INTEL_GPU:false OLLAMA_KEEP_ALIVE:5m0s OLLAMA_KV_CACHE_TYPE: OLLAMA_LLM_LIBRARY: OLLAMA_LOAD_TIMEOUT:5m0s OLLAMA_MAX_LOADED_MODELS:0 OLLAMA_MAX_QUEUE:512 OLLAMA_MODELS:C:\\Users\\frost\\.ollama\\models OLLAMA_MULTIUSER_CACHE:false OLLAMA_NEW_ENGINE:false OLLAMA_NOHISTORY:false OLLAMA_NOPRUNE:false OLLAMA_NUM_PARALLEL:1 OLLAMA_ORIGINS:[http://localhost https://localhost http://localhost:* https://localhost:* http://127.0.0.1 https://127.0.0.1 http://127.0.0.1:* https://127.0.0.1:* http://0.0.0.0 https://0.0.0.0 http://0.0.0.0:* https://0.0.0.0:* app://* file://* tauri://* vscode-webview://* vscode-file://*] OLLAMA_REMOTES:[ollama.com] OLLAMA_SCHED_SPREAD:false ROCR_VISIBLE_DEVICES:]"
time=2025-10-19T14:09:09.697-04:00 level=INFO source=images.go:522 msg="total blobs: 5"
time=2025-10-19T14:09:09.697-04:00 level=INFO source=images.go:529 msg="total unused blobs removed: 0"
time=2025-10-19T14:09:09.698-04:00 level=INFO source=routes.go:1564 msg="Listening on 127.0.0.1:11434 (version 0.12.6)"
time=2025-10-19T14:09:09.699-04:00 level=INFO source=runner.go:80 msg="discovering available GPUs..."
time=2025-10-19T14:09:10.311-04:00 level=INFO source=types.go:112 msg="inference compute" id=GPU-b28645fa-aaf6-86a1-ab1b-ef8d61df29f2 library=CUDA compute=12.0 name=CUDA0 description="NVIDIA GeForce RTX 5070" libdirs=ollama,cuda_v12 driver=12.9 pci_id=01:00.0 type=discrete total="11.9 GiB" available="10.4 GiB"
time=2025-10-19T14:09:10.311-04:00 level=INFO source=routes.go:1605 msg="entering low vram mode" "total vram"="11.9 GiB" threshold="20.0 GiB"
time=2025-10-19T14:09:12.593-04:00 level=INFO source=routes.go:1511 msg="server config" env="map[CUDA_VISIBLE_DEVICES: GGML_VK_VISIBLE_DEVICES: GPU_DEVICE_ORDINAL: HIP_VISIBLE_DEVICES: HSA_OVERRIDE_GFX_VERSION: HTTPS_PROXY: HTTP_PROXY: NO_PROXY: OLLAMA_CONTEXT_LENGTH:4096 OLLAMA_DEBUG:INFO OLLAMA_FLASH_ATTENTION:false OLLAMA_GPU_OVERHEAD:0 OLLAMA_HOST:http://127.0.0.1:11434 OLLAMA_INTEL_GPU:false OLLAMA_KEEP_ALIVE:5m0s OLLAMA_KV_CACHE_TYPE: OLLAMA_LLM_LIBRARY: OLLAMA_LOAD_TIMEOUT:5m0s OLLAMA_MAX_LOADED_MODELS:0 OLLAMA_MAX_QUEUE:512 OLLAMA_MODELS:C:\\Users\\frost\\.ollama\\models OLLAMA_MULTIUSER_CACHE:false OLLAMA_NEW_ENGINE:false OLLAMA_NOHISTORY:false OLLAMA_NOPRUNE:false OLLAMA_NUM_PARALLEL:1 OLLAMA_ORIGINS:[http://localhost https://localhost http://localhost:* https://localhost:* http://127.0.0.1 https://127.0.0.1 http://127.0.0.1:* https://127.0.0.1:* http://0.0.0.0 https://0.0.0.0 http://0.0.0.0:* https://0.0.0.0:* app://* file://* tauri://* vscode-webview://* vscode-file://*] OLLAMA_REMOTES:[ollama.com] OLLAMA_SCHED_SPREAD:false ROCR_VISIBLE_DEVICES:]"
time=2025-10-19T14:09:12.594-04:00 level=INFO source=images.go:522 msg="total blobs: 5"
time=2025-10-19T14:09:12.594-04:00 level=INFO source=images.go:529 msg="total unused blobs removed: 0"
time=2025-10-19T14:09:12.595-04:00 level=INFO source=routes.go:1564 msg="Listening on 127.0.0.1:11434 (version 0.12.6)"
time=2025-10-19T14:09:12.596-04:00 level=INFO source=runner.go:80 msg="discovering available GPUs..."
time=2025-10-19T14:09:13.165-04:00 level=INFO source=types.go:112 msg="inference compute" id=GPU-b28645fa-aaf6-86a1-ab1b-ef8d61df29f2 library=CUDA compute=12.0 name=CUDA0 description="NVIDIA GeForce RTX 5070" libdirs=ollama,cuda_v12 driver=12.9 pci_id=01:00.0 type=discrete total="11.9 GiB" available="10.4 GiB"
time=2025-10-19T14:09:13.165-04:00 level=INFO source=routes.go:1605 msg="entering low vram mode" "total vram"="11.9 GiB" threshold="20.0 GiB"
[GIN] 2025/10/19 - 14:10:07 | 200 |       742.2µs |       127.0.0.1 | GET      "/v1/models"
[GIN] 2025/10/19 - 14:11:02 | 200 |       505.4µs |       127.0.0.1 | GET      "/v1/models"
[GIN] 2025/10/19 - 14:11:36 | 200 |            0s |       127.0.0.1 | HEAD     "/"
[GIN] 2025/10/19 - 14:11:36 | 200 |         648µs |       127.0.0.1 | GET      "/api/tags"
[GIN] 2025/10/19 - 14:15:54 | 204 |            0s |       127.0.0.1 | OPTIONS  "/v1/models"
[GIN] 2025/10/19 - 14:15:54 | 200 |       520.8µs |       127.0.0.1 | GET      "/v1/models"
[GIN] 2025/10/19 - 14:45:54 | 200 |     11.8838ms |       127.0.0.1 | GET      "/v1/models"
[GIN] 2025/10/19 - 14:47:04 | 204 |            0s |       127.0.0.1 | OPTIONS  "/v1/models"
[GIN] 2025/10/19 - 14:47:04 | 200 |       505.8µs |       127.0.0.1 | GET      "/v1/models"
[GIN] 2025/10/19 - 14:50:51 | 204 |            0s |       127.0.0.1 | OPTIONS  "/v1/models"
[GIN] 2025/10/19 - 14:50:51 | 200 |       505.2µs |       127.0.0.1 | GET      "/v1/models"
[GIN] 2025/10/19 - 15:20:51 | 200 |      2.5071ms |       127.0.0.1 | GET      "/v1/models"
[GIN] 2025/10/19 - 15:50:51 | 200 |       506.1µs |       127.0.0.1 | GET      "/v1/models"
[GIN] 2025/10/19 - 16:20:50 | 200 |      1.0847ms |       127.0.0.1 | GET      "/v1/models"
[GIN] 2025/10/19 - 16:50:50 | 200 |      1.0546ms |       127.0.0.1 | GET      "/v1/models"
[GIN] 2025/10/19 - 17:20:50 | 204 |            0s |       127.0.0.1 | OPTIONS  "/v1/models"
[GIN] 2025/10/19 - 17:20:50 | 200 |         505µs |       127.0.0.1 | GET      "/v1/models"
time=2025-10-19T17:51:48.134-04:00 level=INFO source=routes.go:1511 msg="server config" env="map[CUDA_VISIBLE_DEVICES: GGML_VK_VISIBLE_DEVICES: GPU_DEVICE_ORDINAL: HIP_VISIBLE_DEVICES: HSA_OVERRIDE_GFX_VERSION: HTTPS_PROXY: HTTP_PROXY: NO_PROXY: OLLAMA_CONTEXT_LENGTH:4096 OLLAMA_DEBUG:INFO OLLAMA_FLASH_ATTENTION:false OLLAMA_GPU_OVERHEAD:0 OLLAMA_HOST:http://127.0.0.1:11434 OLLAMA_INTEL_GPU:false OLLAMA_KEEP_ALIVE:5m0s OLLAMA_KV_CACHE_TYPE: OLLAMA_LLM_LIBRARY: OLLAMA_LOAD_TIMEOUT:5m0s OLLAMA_MAX_LOADED_MODELS:0 OLLAMA_MAX_QUEUE:512 OLLAMA_MODELS:C:\\Users\\frost\\.ollama\\models OLLAMA_MULTIUSER_CACHE:false OLLAMA_NEW_ENGINE:false OLLAMA_NOHISTORY:false OLLAMA_NOPRUNE:false OLLAMA_NUM_PARALLEL:1 OLLAMA_ORIGINS:[http://localhost https://localhost http://localhost:* https://localhost:* http://127.0.0.1 https://127.0.0.1 http://127.0.0.1:* https://127.0.0.1:* http://0.0.0.0 https://0.0.0.0 http://0.0.0.0:* https://0.0.0.0:* app://* file://* tauri://* vscode-webview://* vscode-file://*] OLLAMA_REMOTES:[ollama.com] OLLAMA_SCHED_SPREAD:false ROCR_VISIBLE_DEVICES:]"
time=2025-10-19T17:51:48.135-04:00 level=INFO source=images.go:522 msg="total blobs: 5"
time=2025-10-19T17:51:48.136-04:00 level=INFO source=images.go:529 msg="total unused blobs removed: 0"
time=2025-10-19T17:51:48.136-04:00 level=INFO source=routes.go:1564 msg="Listening on 127.0.0.1:11434 (version 0.12.6)"
time=2025-10-19T17:51:48.137-04:00 level=INFO source=runner.go:80 msg="discovering available GPUs..."
time=2025-10-19T17:51:50.659-04:00 level=INFO source=types.go:112 msg="inference compute" id=GPU-b28645fa-aaf6-86a1-ab1b-ef8d61df29f2 library=CUDA compute=12.0 name=CUDA0 description="NVIDIA GeForce RTX 5070" libdirs=ollama,cuda_v12 driver=12.9 pci_id=01:00.0 type=discrete total="11.9 GiB" available="9.9 GiB"
time=2025-10-19T17:51:50.660-04:00 level=INFO source=routes.go:1605 msg="entering low vram mode" "total vram"="11.9 GiB" threshold="20.0 GiB"
[GIN] 2025/10/19 - 18:18:58 | 204 |            0s |       127.0.0.1 | OPTIONS  "/v1/models"
[GIN] 2025/10/19 - 18:18:58 | 200 |      1.6167ms |       127.0.0.1 | GET      "/v1/models"
[GIN] 2025/10/19 - 18:29:03 | 200 |            0s |       127.0.0.1 | GET      "/api/version"
[GIN] 2025/10/19 - 18:29:03 | 200 |            0s |       127.0.0.1 | GET      "/api/version"
[GIN] 2025/10/19 - 18:29:03 | 200 |      1.0297ms |       127.0.0.1 | GET      "/api/tags"
[GIN] 2025/10/19 - 18:29:03 | 404 |      1.0841ms |       127.0.0.1 | POST     "/api/show"
[GIN] 2025/10/19 - 18:29:04 | 404 |       504.8µs |       127.0.0.1 | POST     "/api/show"
[GIN] 2025/10/19 - 18:29:06 | 404 |       509.7µs |       127.0.0.1 | POST     "/api/show"
[GIN] 2025/10/19 - 18:29:10 | 404 |       505.1µs |       127.0.0.1 | POST     "/api/show"
[GIN] 2025/10/19 - 18:29:11 | 404 |       505.3µs |       127.0.0.1 | POST     "/api/show"
[GIN] 2025/10/19 - 18:29:11 | 200 |            0s |       127.0.0.1 | GET      "/api/version"
[GIN] 2025/10/19 - 18:29:11 | 200 |       511.9µs |       127.0.0.1 | GET      "/api/tags"
time=2025-10-19T18:29:12.568-04:00 level=INFO source=download.go:177 msg="downloading f5074b1221da in 16 273 MB part(s)"
time=2025-10-19T18:32:36.965-04:00 level=INFO source=download.go:177 msg="downloading 43070e2d4e53 in 1 11 KB part(s)"
time=2025-10-19T18:32:38.242-04:00 level=INFO source=download.go:177 msg="downloading 1ff5b64b61b9 in 1 799 B part(s)"
time=2025-10-19T18:32:39.422-04:00 level=INFO source=download.go:177 msg="downloading ed11eda7790d in 1 30 B part(s)"
time=2025-10-19T18:32:40.635-04:00 level=INFO source=download.go:177 msg="downloading 1064e17101bd in 1 487 B part(s)"
[GIN] 2025/10/19 - 18:32:47 | 200 |         3m35s |       127.0.0.1 | POST     "/api/pull"
[GIN] 2025/10/19 - 18:32:47 | 200 |            0s |       127.0.0.1 | GET      "/api/version"
[GIN] 2025/10/19 - 18:32:47 | 200 |      6.1562ms |       127.0.0.1 | GET      "/api/tags"
[GIN] 2025/10/19 - 18:32:47 | 200 |     16.2807ms |       127.0.0.1 | POST     "/api/show"
llama_model_loader: loaded meta data with 25 key-value pairs and 291 tensors from C:\Users\frost\.ollama\models\blobs\sha256-f5074b1221da0f5a2910d33b642efa5b9eb58cfdddca1c79e16d7ad28aa2b31f (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = llama
llama_model_loader: - kv   1:                               general.name str              = Mistral-7B-Instruct-v0.3
llama_model_loader: - kv   2:                          llama.block_count u32              = 32
llama_model_loader: - kv   3:                       llama.context_length u32              = 32768
llama_model_loader: - kv   4:                     llama.embedding_length u32              = 4096
llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 14336
llama_model_loader: - kv   6:                 llama.attention.head_count u32              = 32
llama_model_loader: - kv   7:              llama.attention.head_count_kv u32              = 8
llama_model_loader: - kv   8:                       llama.rope.freq_base f32              = 1000000.000000
llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010
llama_model_loader: - kv  10:                          general.file_type u32              = 15
llama_model_loader: - kv  11:                           llama.vocab_size u32              = 32768
llama_model_loader: - kv  12:                 llama.rope.dimension_count u32              = 128
llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = llama
llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = default
llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,32768]   = ["<unk>", "<s>", "</s>", "[INST]", "[...
llama_model_loader: - kv  16:                      tokenizer.ggml.scores arr[f32,32768]   = [0.000000, 0.000000, 0.000000, 0.0000...
llama_model_loader: - kv  17:                  tokenizer.ggml.token_type arr[i32,32768]   = [2, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, ...
llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 1
llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 2
llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
llama_model_loader: - kv  21:               tokenizer.ggml.add_bos_token bool             = true
llama_model_loader: - kv  22:               tokenizer.ggml.add_eos_token bool             = false
llama_model_loader: - kv  23:                    tokenizer.chat_template str              = {{ bos_token }}{% for message in mess...
llama_model_loader: - kv  24:               general.quantization_version u32              = 2
llama_model_loader: - type  f32:   65 tensors
llama_model_loader: - type q4_K:  193 tensors
llama_model_loader: - type q6_K:   33 tensors
print_info: file format = GGUF V3 (latest)
print_info: file type   = Q4_K - Medium
print_info: file size   = 4.07 GiB (4.83 BPW) 
load: special_eos_id is not in special_eog_ids - the tokenizer config may be incorrect
load: printing all EOG tokens:
load:   - 2 ('</s>')
load: special tokens cache size = 771
load: token to piece cache size = 0.1731 MB
print_info: arch             = llama
print_info: vocab_only       = 1
print_info: model type       = ?B
print_info: model params     = 7.25 B
print_info: general.name     = Mistral-7B-Instruct-v0.3
print_info: vocab type       = SPM
print_info: n_vocab          = 32768
print_info: n_merges         = 0
print_info: BOS token        = 1 '<s>'
print_info: EOS token        = 2 '</s>'
print_info: UNK token        = 0 '<unk>'
print_info: LF token         = 781 '<0x0A>'
print_info: EOG token        = 2 '</s>'
print_info: max token length = 48
llama_model_load: vocab only - skipping tensors
time=2025-10-19T18:32:47.586-04:00 level=INFO source=cpu_windows.go:139 msg=packages count=1
time=2025-10-19T18:32:47.586-04:00 level=INFO source=cpu_windows.go:155 msg="efficiency cores detected" maxEfficiencyClass=1
time=2025-10-19T18:32:47.586-04:00 level=INFO source=cpu_windows.go:186 msg="" package=0 cores=12 efficiency=4 threads=20
time=2025-10-19T18:32:47.587-04:00 level=INFO source=server.go:400 msg="starting runner" cmd="C:\\Users\\frost\\AppData\\Local\\Programs\\Ollama\\ollama.exe runner --model C:\\Users\\frost\\.ollama\\models\\blobs\\sha256-f5074b1221da0f5a2910d33b642efa5b9eb58cfdddca1c79e16d7ad28aa2b31f --port 65163"
time=2025-10-19T18:32:47.594-04:00 level=INFO source=cpu_windows.go:139 msg=packages count=1
time=2025-10-19T18:32:47.594-04:00 level=INFO source=cpu_windows.go:155 msg="efficiency cores detected" maxEfficiencyClass=1
time=2025-10-19T18:32:47.594-04:00 level=INFO source=cpu_windows.go:186 msg="" package=0 cores=12 efficiency=4 threads=20
time=2025-10-19T18:32:47.594-04:00 level=INFO source=server.go:505 msg="system memory" total="63.8 GiB" free="43.4 GiB" free_swap="37.5 GiB"
time=2025-10-19T18:32:47.595-04:00 level=INFO source=memory.go:36 msg="new model will fit in available VRAM across minimum required GPUs, loading" model=C:\Users\frost\.ollama\models\blobs\sha256-f5074b1221da0f5a2910d33b642efa5b9eb58cfdddca1c79e16d7ad28aa2b31f library=CUDA parallel=1 required="5.4 GiB" gpus=1
time=2025-10-19T18:32:47.595-04:00 level=INFO source=server.go:545 msg=offload library=CUDA layers.requested=-1 layers.model=33 layers.offload=33 layers.split=[33] memory.available="[9.8 GiB]" memory.gpu_overhead="0 B" memory.required.full="5.4 GiB" memory.required.partial="5.4 GiB" memory.required.kv="512.0 MiB" memory.required.allocations="[5.4 GiB]" memory.weights.total="4.0 GiB" memory.weights.repeating="3.9 GiB" memory.weights.nonrepeating="105.0 MiB" memory.graph.full="296.0 MiB" memory.graph.partial="305.0 MiB"
time=2025-10-19T18:32:47.637-04:00 level=INFO source=runner.go:893 msg="starting go runner"
load_backend: loaded CPU backend from C:\Users\frost\AppData\Local\Programs\Ollama\lib\ollama\ggml-cpu-alderlake.dll
ggml_cuda_init: GGML_CUDA_FORCE_MMQ:    no
ggml_cuda_init: GGML_CUDA_FORCE_CUBLAS: no
ggml_cuda_init: found 1 CUDA devices:
  Device 0: NVIDIA GeForce RTX 5070, compute capability 12.0, VMM: yes, ID: GPU-b28645fa-aaf6-86a1-ab1b-ef8d61df29f2
load_backend: loaded CUDA backend from C:\Users\frost\AppData\Local\Programs\Ollama\lib\ollama\cuda_v12\ggml-cuda.dll
time=2025-10-19T18:32:47.733-04:00 level=INFO source=ggml.go:104 msg=system CPU.0.SSE3=1 CPU.0.SSSE3=1 CPU.0.AVX=1 CPU.0.AVX_VNNI=1 CPU.0.AVX2=1 CPU.0.F16C=1 CPU.0.FMA=1 CPU.0.BMI2=1 CPU.0.LLAMAFILE=1 CPU.1.LLAMAFILE=1 CUDA.0.ARCHS=500,520,600,610,700,750,800,860,870,890,900,1200 CUDA.0.USE_GRAPHS=1 CUDA.0.PEER_MAX_BATCH_SIZE=128 compiler=cgo(clang)
time=2025-10-19T18:32:47.733-04:00 level=INFO source=runner.go:929 msg="Server listening on 127.0.0.1:65163"
time=2025-10-19T18:32:47.744-04:00 level=INFO source=runner.go:828 msg=load request="{Operation:commit LoraPath:[] Parallel:1 BatchSize:512 FlashAttention:false KvSize:4096 KvCacheType: NumThreads:8 GPULayers:33[ID:GPU-b28645fa-aaf6-86a1-ab1b-ef8d61df29f2 Layers:33(0..32)] MultiUserCache:false ProjectorPath: MainGPU:0 UseMmap:false}"
time=2025-10-19T18:32:47.744-04:00 level=INFO source=server.go:1272 msg="waiting for llama runner to start responding"
time=2025-10-19T18:32:47.745-04:00 level=INFO source=server.go:1306 msg="waiting for server to become available" status="llm server loading model"
ggml_backend_cuda_device_get_memory utilizing NVML memory reporting free: 10572054528 total: 12820938752
llama_model_load_from_file_impl: using device CUDA0 (NVIDIA GeForce RTX 5070) (0000:01:00.0) - 10082 MiB free
llama_model_loader: loaded meta data with 25 key-value pairs and 291 tensors from C:\Users\frost\.ollama\models\blobs\sha256-f5074b1221da0f5a2910d33b642efa5b9eb58cfdddca1c79e16d7ad28aa2b31f (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = llama
llama_model_loader: - kv   1:                               general.name str              = Mistral-7B-Instruct-v0.3
llama_model_loader: - kv   2:                          llama.block_count u32              = 32
llama_model_loader: - kv   3:                       llama.context_length u32              = 32768
llama_model_loader: - kv   4:                     llama.embedding_length u32              = 4096
llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 14336
llama_model_loader: - kv   6:                 llama.attention.head_count u32              = 32
llama_model_loader: - kv   7:              llama.attention.head_count_kv u32              = 8
llama_model_loader: - kv   8:                       llama.rope.freq_base f32              = 1000000.000000
llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010
llama_model_loader: - kv  10:                          general.file_type u32              = 15
llama_model_loader: - kv  11:                           llama.vocab_size u32              = 32768
llama_model_loader: - kv  12:                 llama.rope.dimension_count u32              = 128
llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = llama
llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = default
llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,32768]   = ["<unk>", "<s>", "</s>", "[INST]", "[...
llama_model_loader: - kv  16:                      tokenizer.ggml.scores arr[f32,32768]   = [0.000000, 0.000000, 0.000000, 0.0000...
llama_model_loader: - kv  17:                  tokenizer.ggml.token_type arr[i32,32768]   = [2, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, ...
llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 1
llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 2
llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
llama_model_loader: - kv  21:               tokenizer.ggml.add_bos_token bool             = true
llama_model_loader: - kv  22:               tokenizer.ggml.add_eos_token bool             = false
llama_model_loader: - kv  23:                    tokenizer.chat_template str              = {{ bos_token }}{% for message in mess...
llama_model_loader: - kv  24:               general.quantization_version u32              = 2
llama_model_loader: - type  f32:   65 tensors
llama_model_loader: - type q4_K:  193 tensors
llama_model_loader: - type q6_K:   33 tensors
print_info: file format = GGUF V3 (latest)
print_info: file type   = Q4_K - Medium
print_info: file size   = 4.07 GiB (4.83 BPW) 
load: special_eos_id is not in special_eog_ids - the tokenizer config may be incorrect
load: printing all EOG tokens:
load:   - 2 ('</s>')
load: special tokens cache size = 771
load: token to piece cache size = 0.1731 MB
print_info: arch             = llama
print_info: vocab_only       = 0
print_info: n_ctx_train      = 32768
print_info: n_embd           = 4096
print_info: n_layer          = 32
print_info: n_head           = 32
print_info: n_head_kv        = 8
print_info: n_rot            = 128
print_info: n_swa            = 0
print_info: is_swa_any       = 0
print_info: n_embd_head_k    = 128
print_info: n_embd_head_v    = 128
print_info: n_gqa            = 4
print_info: n_embd_k_gqa     = 1024
print_info: n_embd_v_gqa     = 1024
print_info: f_norm_eps       = 0.0e+00
print_info: f_norm_rms_eps   = 1.0e-05
print_info: f_clamp_kqv      = 0.0e+00
print_info: f_max_alibi_bias = 0.0e+00
print_info: f_logit_scale    = 0.0e+00
print_info: f_attn_scale     = 0.0e+00
print_info: n_ff             = 14336
print_info: n_expert         = 0
print_info: n_expert_used    = 0
print_info: causal attn      = 1
print_info: pooling type     = 0
print_info: rope type        = 0
print_info: rope scaling     = linear
print_info: freq_base_train  = 1000000.0
print_info: freq_scale_train = 1
print_info: n_ctx_orig_yarn  = 32768
print_info: rope_finetuned   = unknown
print_info: model type       = 7B
print_info: model params     = 7.25 B
print_info: general.name     = Mistral-7B-Instruct-v0.3
print_info: vocab type       = SPM
print_info: n_vocab          = 32768
print_info: n_merges         = 0
print_info: BOS token        = 1 '<s>'
print_info: EOS token        = 2 '</s>'
print_info: UNK token        = 0 '<unk>'
print_info: LF token         = 781 '<0x0A>'
print_info: EOG token        = 2 '</s>'
print_info: max token length = 48
load_tensors: loading model tensors, this can take a while... (mmap = false)
load_tensors: offloading 32 repeating layers to GPU
load_tensors: offloading output layer to GPU
load_tensors: offloaded 33/33 layers to GPU
load_tensors:        CUDA0 model buffer size =  4097.52 MiB
load_tensors:          CPU model buffer size =    72.00 MiB
llama_init_from_model: model default pooling_type is [0], but [-1] was specified
llama_context: constructing llama_context
llama_context: n_seq_max     = 1
llama_context: n_ctx         = 4096
llama_context: n_ctx_per_seq = 4096
llama_context: n_batch       = 512
llama_context: n_ubatch      = 512
llama_context: causal_attn   = 1
llama_context: flash_attn    = disabled
llama_context: kv_unified    = false
llama_context: freq_base     = 1000000.0
llama_context: freq_scale    = 1
llama_context: n_ctx_per_seq (4096) < n_ctx_train (32768) -- the full capacity of the model will not be utilized
llama_context:  CUDA_Host  output buffer size =     0.14 MiB
llama_kv_cache:      CUDA0 KV buffer size =   512.00 MiB
llama_kv_cache: size =  512.00 MiB (  4096 cells,  32 layers,  1/1 seqs), K (f16):  256.00 MiB, V (f16):  256.00 MiB
llama_context:      CUDA0 compute buffer size =   300.01 MiB
llama_context:  CUDA_Host compute buffer size =    20.01 MiB
llama_context: graph nodes  = 1158
llama_context: graph splits = 2
time=2025-10-19T18:32:48.997-04:00 level=INFO source=server.go:1310 msg="llama runner started in 1.41 seconds"
time=2025-10-19T18:32:48.997-04:00 level=INFO source=sched.go:482 msg="loaded runners" count=1
time=2025-10-19T18:32:48.997-04:00 level=INFO source=server.go:1272 msg="waiting for llama runner to start responding"
time=2025-10-19T18:32:48.997-04:00 level=INFO source=server.go:1310 msg="llama runner started in 1.41 seconds"
[GIN] 2025/10/19 - 18:32:52 | 200 |    5.1158709s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/19 - 18:33:30 | 200 |      7.4391ms |       127.0.0.1 | POST     "/api/show"
[GIN] 2025/10/19 - 18:33:30 | 200 |      6.0973ms |       127.0.0.1 | POST     "/api/show"
[GIN] 2025/10/19 - 18:33:33 | 200 |    2.6698638s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/19 - 18:33:43 | 200 |      6.6914ms |       127.0.0.1 | POST     "/api/show"
[GIN] 2025/10/19 - 18:33:43 | 200 |      6.6939ms |       127.0.0.1 | POST     "/api/show"
[GIN] 2025/10/19 - 18:33:47 | 200 |    3.7740183s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/19 - 18:34:09 | 200 |      7.2719ms |       127.0.0.1 | POST     "/api/show"
[GIN] 2025/10/19 - 18:34:09 | 200 |      7.2514ms |       127.0.0.1 | POST     "/api/show"
[GIN] 2025/10/19 - 18:34:16 | 200 |    6.8311383s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/19 - 18:34:39 | 200 |      6.6794ms |       127.0.0.1 | POST     "/api/show"
[GIN] 2025/10/19 - 18:34:39 | 200 |       7.593ms |       127.0.0.1 | POST     "/api/show"
[GIN] 2025/10/19 - 18:34:41 | 200 |    2.2627115s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/20 - 07:26:17 | 200 |       506.1µs |       127.0.0.1 | GET      "/api/version"
[GIN] 2025/10/20 - 07:26:17 | 200 |     90.7998ms |       127.0.0.1 | GET      "/api/tags"
[GIN] 2025/10/20 - 07:26:18 | 200 |    814.0603ms |       127.0.0.1 | POST     "/api/show"
[GIN] 2025/10/20 - 07:26:19 | 200 |    1.2298923s |       127.0.0.1 | POST     "/api/show"
[GIN] 2025/10/20 - 07:27:27 | 200 |            0s |       127.0.0.1 | GET      "/api/version"
[GIN] 2025/10/20 - 07:27:27 | 200 |      3.4242ms |       127.0.0.1 | GET      "/api/tags"
[GIN] 2025/10/20 - 07:27:27 | 200 |     16.6445ms |       127.0.0.1 | POST     "/api/show"
[GIN] 2025/10/20 - 07:27:27 | 200 |     85.3523ms |       127.0.0.1 | POST     "/api/show"
[GIN] 2025/10/20 - 07:36:32 | 200 |       1.016ms |       127.0.0.1 | GET      "/v1/models"
[GIN] 2025/10/20 - 07:38:05 | 200 |      1.0822ms |       127.0.0.1 | GET      "/api/tags"
[GIN] 2025/10/20 - 07:38:05 | 200 |      1.0802ms |       127.0.0.1 | GET      "/api/tags"
[GIN] 2025/10/20 - 07:38:05 | 200 |      9.9586ms |       127.0.0.1 | POST     "/api/show"
[GIN] 2025/10/20 - 07:38:05 | 200 |     10.5266ms |       127.0.0.1 | POST     "/api/show"
[GIN] 2025/10/20 - 07:38:05 | 200 |     57.1406ms |       127.0.0.1 | POST     "/api/show"
[GIN] 2025/10/20 - 07:38:05 | 200 |     57.1268ms |       127.0.0.1 | POST     "/api/show"
[GIN] 2025/10/20 - 07:38:05 | 200 |      1.0265ms |       127.0.0.1 | GET      "/api/tags"
[GIN] 2025/10/20 - 07:38:05 | 200 |      9.4634ms |       127.0.0.1 | POST     "/api/show"
[GIN] 2025/10/20 - 07:38:05 | 200 |      44.883ms |       127.0.0.1 | POST     "/api/show"
llama_model_loader: loaded meta data with 25 key-value pairs and 291 tensors from C:\Users\frost\.ollama\models\blobs\sha256-f5074b1221da0f5a2910d33b642efa5b9eb58cfdddca1c79e16d7ad28aa2b31f (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = llama
llama_model_loader: - kv   1:                               general.name str              = Mistral-7B-Instruct-v0.3
llama_model_loader: - kv   2:                          llama.block_count u32              = 32
llama_model_loader: - kv   3:                       llama.context_length u32              = 32768
llama_model_loader: - kv   4:                     llama.embedding_length u32              = 4096
llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 14336
llama_model_loader: - kv   6:                 llama.attention.head_count u32              = 32
llama_model_loader: - kv   7:              llama.attention.head_count_kv u32              = 8
llama_model_loader: - kv   8:                       llama.rope.freq_base f32              = 1000000.000000
llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010
llama_model_loader: - kv  10:                          general.file_type u32              = 15
llama_model_loader: - kv  11:                           llama.vocab_size u32              = 32768
llama_model_loader: - kv  12:                 llama.rope.dimension_count u32              = 128
llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = llama
llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = default
llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,32768]   = ["<unk>", "<s>", "</s>", "[INST]", "[...
llama_model_loader: - kv  16:                      tokenizer.ggml.scores arr[f32,32768]   = [0.000000, 0.000000, 0.000000, 0.0000...
llama_model_loader: - kv  17:                  tokenizer.ggml.token_type arr[i32,32768]   = [2, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, ...
llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 1
llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 2
llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
llama_model_loader: - kv  21:               tokenizer.ggml.add_bos_token bool             = true
llama_model_loader: - kv  22:               tokenizer.ggml.add_eos_token bool             = false
llama_model_loader: - kv  23:                    tokenizer.chat_template str              = {{ bos_token }}{% for message in mess...
llama_model_loader: - kv  24:               general.quantization_version u32              = 2
llama_model_loader: - type  f32:   65 tensors
llama_model_loader: - type q4_K:  193 tensors
llama_model_loader: - type q6_K:   33 tensors
print_info: file format = GGUF V3 (latest)
print_info: file type   = Q4_K - Medium
print_info: file size   = 4.07 GiB (4.83 BPW) 
load: special_eos_id is not in special_eog_ids - the tokenizer config may be incorrect
load: printing all EOG tokens:
load:   - 2 ('</s>')
load: special tokens cache size = 771
load: token to piece cache size = 0.1731 MB
print_info: arch             = llama
print_info: vocab_only       = 1
print_info: model type       = ?B
print_info: model params     = 7.25 B
print_info: general.name     = Mistral-7B-Instruct-v0.3
print_info: vocab type       = SPM
print_info: n_vocab          = 32768
print_info: n_merges         = 0
print_info: BOS token        = 1 '<s>'
print_info: EOS token        = 2 '</s>'
print_info: UNK token        = 0 '<unk>'
print_info: LF token         = 781 '<0x0A>'
print_info: EOG token        = 2 '</s>'
print_info: max token length = 48
llama_model_load: vocab only - skipping tensors
time=2025-10-20T07:39:32.106-04:00 level=INFO source=cpu_windows.go:139 msg=packages count=1
time=2025-10-20T07:39:32.106-04:00 level=INFO source=cpu_windows.go:155 msg="efficiency cores detected" maxEfficiencyClass=1
time=2025-10-20T07:39:32.106-04:00 level=INFO source=cpu_windows.go:186 msg="" package=0 cores=12 efficiency=4 threads=20
time=2025-10-20T07:39:32.107-04:00 level=INFO source=server.go:400 msg="starting runner" cmd="C:\\Users\\frost\\AppData\\Local\\Programs\\Ollama\\ollama.exe runner --model C:\\Users\\frost\\.ollama\\models\\blobs\\sha256-f5074b1221da0f5a2910d33b642efa5b9eb58cfdddca1c79e16d7ad28aa2b31f --port 52110"
time=2025-10-20T07:39:32.111-04:00 level=INFO source=cpu_windows.go:139 msg=packages count=1
time=2025-10-20T07:39:32.111-04:00 level=INFO source=cpu_windows.go:155 msg="efficiency cores detected" maxEfficiencyClass=1
time=2025-10-20T07:39:32.111-04:00 level=INFO source=cpu_windows.go:186 msg="" package=0 cores=12 efficiency=4 threads=20
time=2025-10-20T07:39:32.111-04:00 level=INFO source=server.go:505 msg="system memory" total="63.8 GiB" free="42.6 GiB" free_swap="38.0 GiB"
time=2025-10-20T07:39:32.111-04:00 level=INFO source=memory.go:36 msg="new model will fit in available VRAM across minimum required GPUs, loading" model=C:\Users\frost\.ollama\models\blobs\sha256-f5074b1221da0f5a2910d33b642efa5b9eb58cfdddca1c79e16d7ad28aa2b31f library=CUDA parallel=1 required="6.2 GiB" gpus=1
time=2025-10-20T07:39:32.111-04:00 level=INFO source=server.go:545 msg=offload library=CUDA layers.requested=-1 layers.model=33 layers.offload=33 layers.split=[33] memory.available="[10.5 GiB]" memory.gpu_overhead="0 B" memory.required.full="6.2 GiB" memory.required.partial="6.2 GiB" memory.required.kv="1.0 GiB" memory.required.allocations="[6.2 GiB]" memory.weights.total="4.0 GiB" memory.weights.repeating="3.9 GiB" memory.weights.nonrepeating="105.0 MiB" memory.graph.full="560.0 MiB" memory.graph.partial="585.0 MiB"
time=2025-10-20T07:39:32.149-04:00 level=INFO source=runner.go:893 msg="starting go runner"
load_backend: loaded CPU backend from C:\Users\frost\AppData\Local\Programs\Ollama\lib\ollama\ggml-cpu-alderlake.dll
ggml_cuda_init: GGML_CUDA_FORCE_MMQ:    no
ggml_cuda_init: GGML_CUDA_FORCE_CUBLAS: no
ggml_cuda_init: found 1 CUDA devices:
  Device 0: NVIDIA GeForce RTX 5070, compute capability 12.0, VMM: yes, ID: GPU-b28645fa-aaf6-86a1-ab1b-ef8d61df29f2
load_backend: loaded CUDA backend from C:\Users\frost\AppData\Local\Programs\Ollama\lib\ollama\cuda_v12\ggml-cuda.dll
time=2025-10-20T07:39:32.252-04:00 level=INFO source=ggml.go:104 msg=system CPU.0.SSE3=1 CPU.0.SSSE3=1 CPU.0.AVX=1 CPU.0.AVX_VNNI=1 CPU.0.AVX2=1 CPU.0.F16C=1 CPU.0.FMA=1 CPU.0.BMI2=1 CPU.0.LLAMAFILE=1 CPU.1.LLAMAFILE=1 CUDA.0.ARCHS=500,520,600,610,700,750,800,860,870,890,900,1200 CUDA.0.USE_GRAPHS=1 CUDA.0.PEER_MAX_BATCH_SIZE=128 compiler=cgo(clang)
time=2025-10-20T07:39:32.253-04:00 level=INFO source=runner.go:929 msg="Server listening on 127.0.0.1:52110"
time=2025-10-20T07:39:32.260-04:00 level=INFO source=runner.go:828 msg=load request="{Operation:commit LoraPath:[] Parallel:1 BatchSize:512 FlashAttention:false KvSize:8192 KvCacheType: NumThreads:8 GPULayers:33[ID:GPU-b28645fa-aaf6-86a1-ab1b-ef8d61df29f2 Layers:33(0..32)] MultiUserCache:false ProjectorPath: MainGPU:0 UseMmap:false}"
time=2025-10-20T07:39:32.260-04:00 level=INFO source=server.go:1272 msg="waiting for llama runner to start responding"
time=2025-10-20T07:39:32.260-04:00 level=INFO source=server.go:1306 msg="waiting for server to become available" status="llm server loading model"
ggml_backend_cuda_device_get_memory utilizing NVML memory reporting free: 11294441472 total: 12820938752
llama_model_load_from_file_impl: using device CUDA0 (NVIDIA GeForce RTX 5070) (0000:01:00.0) - 10771 MiB free
llama_model_loader: loaded meta data with 25 key-value pairs and 291 tensors from C:\Users\frost\.ollama\models\blobs\sha256-f5074b1221da0f5a2910d33b642efa5b9eb58cfdddca1c79e16d7ad28aa2b31f (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = llama
llama_model_loader: - kv   1:                               general.name str              = Mistral-7B-Instruct-v0.3
llama_model_loader: - kv   2:                          llama.block_count u32              = 32
llama_model_loader: - kv   3:                       llama.context_length u32              = 32768
llama_model_loader: - kv   4:                     llama.embedding_length u32              = 4096
llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 14336
llama_model_loader: - kv   6:                 llama.attention.head_count u32              = 32
llama_model_loader: - kv   7:              llama.attention.head_count_kv u32              = 8
llama_model_loader: - kv   8:                       llama.rope.freq_base f32              = 1000000.000000
llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010
llama_model_loader: - kv  10:                          general.file_type u32              = 15
llama_model_loader: - kv  11:                           llama.vocab_size u32              = 32768
llama_model_loader: - kv  12:                 llama.rope.dimension_count u32              = 128
llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = llama
llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = default
llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,32768]   = ["<unk>", "<s>", "</s>", "[INST]", "[...
llama_model_loader: - kv  16:                      tokenizer.ggml.scores arr[f32,32768]   = [0.000000, 0.000000, 0.000000, 0.0000...
llama_model_loader: - kv  17:                  tokenizer.ggml.token_type arr[i32,32768]   = [2, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, ...
llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 1
llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 2
llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
llama_model_loader: - kv  21:               tokenizer.ggml.add_bos_token bool             = true
llama_model_loader: - kv  22:               tokenizer.ggml.add_eos_token bool             = false
llama_model_loader: - kv  23:                    tokenizer.chat_template str              = {{ bos_token }}{% for message in mess...
llama_model_loader: - kv  24:               general.quantization_version u32              = 2
llama_model_loader: - type  f32:   65 tensors
llama_model_loader: - type q4_K:  193 tensors
llama_model_loader: - type q6_K:   33 tensors
print_info: file format = GGUF V3 (latest)
print_info: file type   = Q4_K - Medium
print_info: file size   = 4.07 GiB (4.83 BPW) 
load: special_eos_id is not in special_eog_ids - the tokenizer config may be incorrect
load: printing all EOG tokens:
load:   - 2 ('</s>')
load: special tokens cache size = 771
load: token to piece cache size = 0.1731 MB
print_info: arch             = llama
print_info: vocab_only       = 0
print_info: n_ctx_train      = 32768
print_info: n_embd           = 4096
print_info: n_layer          = 32
print_info: n_head           = 32
print_info: n_head_kv        = 8
print_info: n_rot            = 128
print_info: n_swa            = 0
print_info: is_swa_any       = 0
print_info: n_embd_head_k    = 128
print_info: n_embd_head_v    = 128
print_info: n_gqa            = 4
print_info: n_embd_k_gqa     = 1024
print_info: n_embd_v_gqa     = 1024
print_info: f_norm_eps       = 0.0e+00
print_info: f_norm_rms_eps   = 1.0e-05
print_info: f_clamp_kqv      = 0.0e+00
print_info: f_max_alibi_bias = 0.0e+00
print_info: f_logit_scale    = 0.0e+00
print_info: f_attn_scale     = 0.0e+00
print_info: n_ff             = 14336
print_info: n_expert         = 0
print_info: n_expert_used    = 0
print_info: causal attn      = 1
print_info: pooling type     = 0
print_info: rope type        = 0
print_info: rope scaling     = linear
print_info: freq_base_train  = 1000000.0
print_info: freq_scale_train = 1
print_info: n_ctx_orig_yarn  = 32768
print_info: rope_finetuned   = unknown
print_info: model type       = 7B
print_info: model params     = 7.25 B
print_info: general.name     = Mistral-7B-Instruct-v0.3
print_info: vocab type       = SPM
print_info: n_vocab          = 32768
print_info: n_merges         = 0
print_info: BOS token        = 1 '<s>'
print_info: EOS token        = 2 '</s>'
print_info: UNK token        = 0 '<unk>'
print_info: LF token         = 781 '<0x0A>'
print_info: EOG token        = 2 '</s>'
print_info: max token length = 48
load_tensors: loading model tensors, this can take a while... (mmap = false)
load_tensors: offloading 32 repeating layers to GPU
load_tensors: offloading output layer to GPU
load_tensors: offloaded 33/33 layers to GPU
load_tensors:        CUDA0 model buffer size =  4097.52 MiB
load_tensors:          CPU model buffer size =    72.00 MiB
llama_init_from_model: model default pooling_type is [0], but [-1] was specified
llama_context: constructing llama_context
llama_context: n_seq_max     = 1
llama_context: n_ctx         = 8192
llama_context: n_ctx_per_seq = 8192
llama_context: n_batch       = 512
llama_context: n_ubatch      = 512
llama_context: causal_attn   = 1
llama_context: flash_attn    = disabled
llama_context: kv_unified    = false
llama_context: freq_base     = 1000000.0
llama_context: freq_scale    = 1
llama_context: n_ctx_per_seq (8192) < n_ctx_train (32768) -- the full capacity of the model will not be utilized
llama_context:  CUDA_Host  output buffer size =     0.14 MiB
llama_kv_cache:      CUDA0 KV buffer size =  1024.00 MiB
llama_kv_cache: size = 1024.00 MiB (  8192 cells,  32 layers,  1/1 seqs), K (f16):  512.00 MiB, V (f16):  512.00 MiB
llama_context:      CUDA0 compute buffer size =   564.01 MiB
llama_context:  CUDA_Host compute buffer size =    28.01 MiB
llama_context: graph nodes  = 1158
llama_context: graph splits = 2
time=2025-10-20T07:39:35.264-04:00 level=INFO source=server.go:1310 msg="llama runner started in 3.16 seconds"
time=2025-10-20T07:39:35.264-04:00 level=INFO source=sched.go:482 msg="loaded runners" count=1
time=2025-10-20T07:39:35.264-04:00 level=INFO source=server.go:1272 msg="waiting for llama runner to start responding"
time=2025-10-20T07:39:35.264-04:00 level=INFO source=server.go:1310 msg="llama runner started in 3.16 seconds"
[GIN] 2025/10/20 - 07:39:37 | 200 |    6.3712379s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/20 - 07:39:37 | 200 |    101.3737ms |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/20 - 07:40:25 | 200 |    2.2594852s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/20 - 07:40:45 | 200 |    244.4142ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/10/20 - 07:42:42 | 200 |    6.8457352s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/20 - 07:54:27 | 200 |      1.0208ms |       127.0.0.1 | GET      "/api/tags"
[GIN] 2025/10/20 - 07:54:27 | 200 |      8.8514ms |       127.0.0.1 | POST     "/api/show"
[GIN] 2025/10/20 - 07:54:27 | 200 |     40.7111ms |       127.0.0.1 | POST     "/api/show"
[GIN] 2025/10/20 - 07:54:28 | 200 |            0s |       127.0.0.1 | GET      "/api/version"
[GIN] 2025/10/20 - 07:54:28 | 200 |       505.6µs |       127.0.0.1 | GET      "/api/tags"
[GIN] 2025/10/20 - 07:54:28 | 200 |      6.7682ms |       127.0.0.1 | POST     "/api/show"
[GIN] 2025/10/20 - 07:54:28 | 200 |     35.1106ms |       127.0.0.1 | POST     "/api/show"
[GIN] 2025/10/20 - 07:55:19 | 200 |            0s |       127.0.0.1 | GET      "/api/version"
[GIN] 2025/10/20 - 07:55:19 | 200 |      1.2056ms |       127.0.0.1 | GET      "/api/tags"
[GIN] 2025/10/20 - 07:55:19 | 200 |      7.2497ms |       127.0.0.1 | POST     "/api/show"
[GIN] 2025/10/20 - 07:55:19 | 200 |      41.931ms |       127.0.0.1 | POST     "/api/show"
[GIN] 2025/10/20 - 07:55:21 | 200 |      1.0507ms |       127.0.0.1 | GET      "/api/tags"
[GIN] 2025/10/20 - 07:55:21 | 200 |     11.5635ms |       127.0.0.1 | POST     "/api/show"
[GIN] 2025/10/20 - 07:55:21 | 200 |     51.7865ms |       127.0.0.1 | POST     "/api/show"
[GIN] 2025/10/20 - 07:55:21 | 200 |       505.4µs |       127.0.0.1 | GET      "/api/tags"
[GIN] 2025/10/20 - 07:55:21 | 200 |      6.7746ms |       127.0.0.1 | POST     "/api/show"
[GIN] 2025/10/20 - 07:55:21 | 200 |     37.1657ms |       127.0.0.1 | POST     "/api/show"
[GIN] 2025/10/20 - 07:55:24 | 200 |            0s |       127.0.0.1 | GET      "/api/version"
[GIN] 2025/10/20 - 07:55:24 | 200 |      1.0238ms |       127.0.0.1 | GET      "/api/tags"
[GIN] 2025/10/20 - 07:55:24 | 200 |      6.8843ms |       127.0.0.1 | POST     "/api/show"
[GIN] 2025/10/20 - 07:55:25 | 200 |     54.0409ms |       127.0.0.1 | POST     "/api/show"
[GIN] 2025/10/20 - 07:55:26 | 200 |       506.9µs |       127.0.0.1 | GET      "/api/tags"
[GIN] 2025/10/20 - 07:55:26 | 200 |      7.7684ms |       127.0.0.1 | POST     "/api/show"
[GIN] 2025/10/20 - 07:55:26 | 200 |     45.4052ms |       127.0.0.1 | POST     "/api/show"
[GIN] 2025/10/20 - 07:55:26 | 200 |       506.4µs |       127.0.0.1 | GET      "/api/tags"
[GIN] 2025/10/20 - 07:55:26 | 200 |      7.2859ms |       127.0.0.1 | POST     "/api/show"
[GIN] 2025/10/20 - 07:55:26 | 200 |     33.6025ms |       127.0.0.1 | POST     "/api/show"
[GIN] 2025/10/20 - 08:10:16 | 200 |        1.02ms |       127.0.0.1 | GET      "/api/tags"
[GIN] 2025/10/20 - 08:10:16 | 200 |      7.1869ms |       127.0.0.1 | POST     "/api/show"
[GIN] 2025/10/20 - 08:10:16 | 200 |     38.7323ms |       127.0.0.1 | POST     "/api/show"
[GIN] 2025/10/20 - 08:17:24 | 200 |      2.0419ms |       127.0.0.1 | GET      "/api/tags"
[GIN] 2025/10/20 - 08:17:24 | 200 |     44.6243ms |       127.0.0.1 | POST     "/api/show"
[GIN] 2025/10/20 - 08:17:24 | 200 |      7.5588ms |       127.0.0.1 | POST     "/api/show"
[GIN] 2025/10/20 - 08:31:20 | 200 |       996.6µs |       127.0.0.1 | GET      "/api/tags"
[GIN] 2025/10/20 - 08:31:20 | 200 |      7.3078ms |       127.0.0.1 | POST     "/api/show"
[GIN] 2025/10/20 - 08:31:20 | 200 |     37.8894ms |       127.0.0.1 | POST     "/api/show"
[GIN] 2025/10/20 - 08:33:18 | 200 |      1.4714ms |       127.0.0.1 | GET      "/api/tags"
[GIN] 2025/10/20 - 08:33:18 | 200 |      6.5167ms |       127.0.0.1 | POST     "/api/show"
[GIN] 2025/10/20 - 08:33:18 | 200 |     35.3442ms |       127.0.0.1 | POST     "/api/show"
[GIN] 2025/10/20 - 08:34:34 | 200 |       505.6µs |       127.0.0.1 | GET      "/api/tags"
[GIN] 2025/10/20 - 08:34:34 | 200 |      7.1612ms |       127.0.0.1 | POST     "/api/show"
[GIN] 2025/10/20 - 08:34:34 | 200 |     42.0066ms |       127.0.0.1 | POST     "/api/show"
[GIN] 2025/10/20 - 08:40:46 | 200 |       3.011ms |       127.0.0.1 | GET      "/api/tags"
[GIN] 2025/10/20 - 08:40:46 | 200 |    120.9145ms |       127.0.0.1 | POST     "/api/show"
[GIN] 2025/10/20 - 08:40:46 | 200 |     28.2072ms |       127.0.0.1 | POST     "/api/show"
llama_model_loader: loaded meta data with 22 key-value pairs and 291 tensors from C:\Users\frost\.ollama\models\blobs\sha256-6a0746a1ec1aef3e7ec53868f220ff6e389f6f8ef87a01d77c96807de94ca2aa (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = llama
llama_model_loader: - kv   1:                               general.name str              = Meta-Llama-3-8B-Instruct
llama_model_loader: - kv   2:                          llama.block_count u32              = 32
llama_model_loader: - kv   3:                       llama.context_length u32              = 8192
llama_model_loader: - kv   4:                     llama.embedding_length u32              = 4096
llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 14336
llama_model_loader: - kv   6:                 llama.attention.head_count u32              = 32
llama_model_loader: - kv   7:              llama.attention.head_count_kv u32              = 8
llama_model_loader: - kv   8:                       llama.rope.freq_base f32              = 500000.000000
llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010
llama_model_loader: - kv  10:                          general.file_type u32              = 2
llama_model_loader: - kv  11:                           llama.vocab_size u32              = 128256
llama_model_loader: - kv  12:                 llama.rope.dimension_count u32              = 128
llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = llama-bpe
llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,128256]  = ["!", "\"", "#", "$", "%", "&", "'", ...
llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,128256]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,280147]  = ["Ġ Ġ", "Ġ ĠĠĠ", "ĠĠ ĠĠ", "...
llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 128000
llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 128009
llama_model_loader: - kv  20:                    tokenizer.chat_template str              = {% set loop_messages = messages %}{% ...
llama_model_loader: - kv  21:               general.quantization_version u32              = 2
llama_model_loader: - type  f32:   65 tensors
llama_model_loader: - type q4_0:  225 tensors
llama_model_loader: - type q6_K:    1 tensors
print_info: file format = GGUF V3 (latest)
print_info: file type   = Q4_0
print_info: file size   = 4.33 GiB (4.64 BPW) 
load: printing all EOG tokens:
load:   - 128001 ('<|end_of_text|>')
load:   - 128009 ('<|eot_id|>')
load: special tokens cache size = 256
load: token to piece cache size = 0.8000 MB
print_info: arch             = llama
print_info: vocab_only       = 1
print_info: model type       = ?B
print_info: model params     = 8.03 B
print_info: general.name     = Meta-Llama-3-8B-Instruct
print_info: vocab type       = BPE
print_info: n_vocab          = 128256
print_info: n_merges         = 280147
print_info: BOS token        = 128000 '<|begin_of_text|>'
print_info: EOS token        = 128009 '<|eot_id|>'
print_info: EOT token        = 128001 '<|end_of_text|>'
print_info: LF token         = 198 'Ċ'
print_info: EOG token        = 128001 '<|end_of_text|>'
print_info: EOG token        = 128009 '<|eot_id|>'
print_info: max token length = 256
llama_model_load: vocab only - skipping tensors
time=2025-10-20T08:43:18.834-04:00 level=INFO source=cpu_windows.go:139 msg=packages count=1
time=2025-10-20T08:43:18.834-04:00 level=INFO source=cpu_windows.go:155 msg="efficiency cores detected" maxEfficiencyClass=1
time=2025-10-20T08:43:18.834-04:00 level=INFO source=cpu_windows.go:186 msg="" package=0 cores=12 efficiency=4 threads=20
time=2025-10-20T08:43:18.835-04:00 level=INFO source=server.go:400 msg="starting runner" cmd="C:\\Users\\frost\\AppData\\Local\\Programs\\Ollama\\ollama.exe runner --model C:\\Users\\frost\\.ollama\\models\\blobs\\sha256-6a0746a1ec1aef3e7ec53868f220ff6e389f6f8ef87a01d77c96807de94ca2aa --port 52298"
time=2025-10-20T08:43:18.842-04:00 level=INFO source=cpu_windows.go:139 msg=packages count=1
time=2025-10-20T08:43:18.842-04:00 level=INFO source=cpu_windows.go:155 msg="efficiency cores detected" maxEfficiencyClass=1
time=2025-10-20T08:43:18.842-04:00 level=INFO source=cpu_windows.go:186 msg="" package=0 cores=12 efficiency=4 threads=20
time=2025-10-20T08:43:18.842-04:00 level=INFO source=server.go:505 msg="system memory" total="63.8 GiB" free="44.5 GiB" free_swap="40.1 GiB"
time=2025-10-20T08:43:18.842-04:00 level=INFO source=memory.go:36 msg="new model will fit in available VRAM across minimum required GPUs, loading" model=C:\Users\frost\.ollama\models\blobs\sha256-6a0746a1ec1aef3e7ec53868f220ff6e389f6f8ef87a01d77c96807de94ca2aa library=CUDA parallel=1 required="6.2 GiB" gpus=1
time=2025-10-20T08:43:18.842-04:00 level=INFO source=server.go:545 msg=offload library=CUDA layers.requested=-1 layers.model=33 layers.offload=33 layers.split=[33] memory.available="[10.1 GiB]" memory.gpu_overhead="0 B" memory.required.full="6.2 GiB" memory.required.partial="6.2 GiB" memory.required.kv="1.0 GiB" memory.required.allocations="[6.2 GiB]" memory.weights.total="4.1 GiB" memory.weights.repeating="3.7 GiB" memory.weights.nonrepeating="411.0 MiB" memory.graph.full="560.0 MiB" memory.graph.partial="677.5 MiB"
time=2025-10-20T08:43:18.904-04:00 level=INFO source=runner.go:893 msg="starting go runner"
load_backend: loaded CPU backend from C:\Users\frost\AppData\Local\Programs\Ollama\lib\ollama\ggml-cpu-alderlake.dll
ggml_cuda_init: GGML_CUDA_FORCE_MMQ:    no
ggml_cuda_init: GGML_CUDA_FORCE_CUBLAS: no
ggml_cuda_init: found 1 CUDA devices:
  Device 0: NVIDIA GeForce RTX 5070, compute capability 12.0, VMM: yes, ID: GPU-b28645fa-aaf6-86a1-ab1b-ef8d61df29f2
load_backend: loaded CUDA backend from C:\Users\frost\AppData\Local\Programs\Ollama\lib\ollama\cuda_v12\ggml-cuda.dll
time=2025-10-20T08:43:19.039-04:00 level=INFO source=ggml.go:104 msg=system CPU.0.SSE3=1 CPU.0.SSSE3=1 CPU.0.AVX=1 CPU.0.AVX_VNNI=1 CPU.0.AVX2=1 CPU.0.F16C=1 CPU.0.FMA=1 CPU.0.BMI2=1 CPU.0.LLAMAFILE=1 CPU.1.LLAMAFILE=1 CUDA.0.ARCHS=500,520,600,610,700,750,800,860,870,890,900,1200 CUDA.0.USE_GRAPHS=1 CUDA.0.PEER_MAX_BATCH_SIZE=128 compiler=cgo(clang)
time=2025-10-20T08:43:19.040-04:00 level=INFO source=runner.go:929 msg="Server listening on 127.0.0.1:52298"
time=2025-10-20T08:43:19.052-04:00 level=INFO source=runner.go:828 msg=load request="{Operation:commit LoraPath:[] Parallel:1 BatchSize:512 FlashAttention:false KvSize:8192 KvCacheType: NumThreads:8 GPULayers:33[ID:GPU-b28645fa-aaf6-86a1-ab1b-ef8d61df29f2 Layers:33(0..32)] MultiUserCache:false ProjectorPath: MainGPU:0 UseMmap:false}"
time=2025-10-20T08:43:19.053-04:00 level=INFO source=server.go:1272 msg="waiting for llama runner to start responding"
time=2025-10-20T08:43:19.053-04:00 level=INFO source=server.go:1306 msg="waiting for server to become available" status="llm server loading model"
ggml_backend_cuda_device_get_memory utilizing NVML memory reporting free: 10876166144 total: 12820938752
llama_model_load_from_file_impl: using device CUDA0 (NVIDIA GeForce RTX 5070) (0000:01:00.0) - 10372 MiB free
llama_model_loader: loaded meta data with 22 key-value pairs and 291 tensors from C:\Users\frost\.ollama\models\blobs\sha256-6a0746a1ec1aef3e7ec53868f220ff6e389f6f8ef87a01d77c96807de94ca2aa (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = llama
llama_model_loader: - kv   1:                               general.name str              = Meta-Llama-3-8B-Instruct
llama_model_loader: - kv   2:                          llama.block_count u32              = 32
llama_model_loader: - kv   3:                       llama.context_length u32              = 8192
llama_model_loader: - kv   4:                     llama.embedding_length u32              = 4096
llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 14336
llama_model_loader: - kv   6:                 llama.attention.head_count u32              = 32
llama_model_loader: - kv   7:              llama.attention.head_count_kv u32              = 8
llama_model_loader: - kv   8:                       llama.rope.freq_base f32              = 500000.000000
llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010
llama_model_loader: - kv  10:                          general.file_type u32              = 2
llama_model_loader: - kv  11:                           llama.vocab_size u32              = 128256
llama_model_loader: - kv  12:                 llama.rope.dimension_count u32              = 128
llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = llama-bpe
llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,128256]  = ["!", "\"", "#", "$", "%", "&", "'", ...
llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,128256]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,280147]  = ["Ġ Ġ", "Ġ ĠĠĠ", "ĠĠ ĠĠ", "...
llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 128000
llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 128009
llama_model_loader: - kv  20:                    tokenizer.chat_template str              = {% set loop_messages = messages %}{% ...
llama_model_loader: - kv  21:               general.quantization_version u32              = 2
llama_model_loader: - type  f32:   65 tensors
llama_model_loader: - type q4_0:  225 tensors
llama_model_loader: - type q6_K:    1 tensors
print_info: file format = GGUF V3 (latest)
print_info: file type   = Q4_0
print_info: file size   = 4.33 GiB (4.64 BPW) 
load: printing all EOG tokens:
load:   - 128001 ('<|end_of_text|>')
load:   - 128009 ('<|eot_id|>')
load: special tokens cache size = 256
load: token to piece cache size = 0.8000 MB
print_info: arch             = llama
print_info: vocab_only       = 0
print_info: n_ctx_train      = 8192
print_info: n_embd           = 4096
print_info: n_layer          = 32
print_info: n_head           = 32
print_info: n_head_kv        = 8
print_info: n_rot            = 128
print_info: n_swa            = 0
print_info: is_swa_any       = 0
print_info: n_embd_head_k    = 128
print_info: n_embd_head_v    = 128
print_info: n_gqa            = 4
print_info: n_embd_k_gqa     = 1024
print_info: n_embd_v_gqa     = 1024
print_info: f_norm_eps       = 0.0e+00
print_info: f_norm_rms_eps   = 1.0e-05
print_info: f_clamp_kqv      = 0.0e+00
print_info: f_max_alibi_bias = 0.0e+00
print_info: f_logit_scale    = 0.0e+00
print_info: f_attn_scale     = 0.0e+00
print_info: n_ff             = 14336
print_info: n_expert         = 0
print_info: n_expert_used    = 0
print_info: causal attn      = 1
print_info: pooling type     = 0
print_info: rope type        = 0
print_info: rope scaling     = linear
print_info: freq_base_train  = 500000.0
print_info: freq_scale_train = 1
print_info: n_ctx_orig_yarn  = 8192
print_info: rope_finetuned   = unknown
print_info: model type       = 8B
print_info: model params     = 8.03 B
print_info: general.name     = Meta-Llama-3-8B-Instruct
print_info: vocab type       = BPE
print_info: n_vocab          = 128256
print_info: n_merges         = 280147
print_info: BOS token        = 128000 '<|begin_of_text|>'
print_info: EOS token        = 128009 '<|eot_id|>'
print_info: EOT token        = 128001 '<|end_of_text|>'
print_info: LF token         = 198 'Ċ'
print_info: EOG token        = 128001 '<|end_of_text|>'
print_info: EOG token        = 128009 '<|eot_id|>'
print_info: max token length = 256
load_tensors: loading model tensors, this can take a while... (mmap = false)
load_tensors: offloading 32 repeating layers to GPU
load_tensors: offloading output layer to GPU
load_tensors: offloaded 33/33 layers to GPU
load_tensors:    CUDA_Host model buffer size =   281.81 MiB
load_tensors:        CUDA0 model buffer size =  4155.99 MiB
llama_init_from_model: model default pooling_type is [0], but [-1] was specified
llama_context: constructing llama_context
llama_context: n_seq_max     = 1
llama_context: n_ctx         = 8192
llama_context: n_ctx_per_seq = 8192
llama_context: n_batch       = 512
llama_context: n_ubatch      = 512
llama_context: causal_attn   = 1
llama_context: flash_attn    = disabled
llama_context: kv_unified    = false
llama_context: freq_base     = 500000.0
llama_context: freq_scale    = 1
llama_context:  CUDA_Host  output buffer size =     0.50 MiB
llama_kv_cache:      CUDA0 KV buffer size =  1024.00 MiB
llama_kv_cache: size = 1024.00 MiB (  8192 cells,  32 layers,  1/1 seqs), K (f16):  512.00 MiB, V (f16):  512.00 MiB
llama_context:      CUDA0 compute buffer size =   564.01 MiB
llama_context:  CUDA_Host compute buffer size =    28.01 MiB
llama_context: graph nodes  = 1158
llama_context: graph splits = 2
time=2025-10-20T08:43:23.079-04:00 level=INFO source=server.go:1310 msg="llama runner started in 4.24 seconds"
time=2025-10-20T08:43:23.079-04:00 level=INFO source=sched.go:482 msg="loaded runners" count=1
time=2025-10-20T08:43:23.079-04:00 level=INFO source=server.go:1272 msg="waiting for llama runner to start responding"
time=2025-10-20T08:43:23.079-04:00 level=INFO source=server.go:1310 msg="llama runner started in 4.24 seconds"
[GIN] 2025/10/20 - 08:43:28 | 200 |    10.668868s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/20 - 08:43:28 | 200 |    427.3452ms |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/20 - 08:43:54 | 200 |    6.8631932s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/20 - 08:44:47 | 200 |    5.3290607s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/20 - 08:45:11 | 200 |    5.8901565s |       127.0.0.1 | POST     "/api/chat"
