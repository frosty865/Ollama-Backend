time=2025-10-21T04:05:31.649-04:00 level=INFO source=routes.go:1511 msg="server config" env="map[CUDA_VISIBLE_DEVICES: GGML_VK_VISIBLE_DEVICES: GPU_DEVICE_ORDINAL: HIP_VISIBLE_DEVICES: HSA_OVERRIDE_GFX_VERSION: HTTPS_PROXY: HTTP_PROXY: NO_PROXY: OLLAMA_CONTEXT_LENGTH:4096 OLLAMA_DEBUG:INFO OLLAMA_FLASH_ATTENTION:false OLLAMA_GPU_OVERHEAD:0 OLLAMA_HOST:http://127.0.0.1:11434 OLLAMA_INTEL_GPU:false OLLAMA_KEEP_ALIVE:5m0s OLLAMA_KV_CACHE_TYPE: OLLAMA_LLM_LIBRARY: OLLAMA_LOAD_TIMEOUT:5m0s OLLAMA_MAX_LOADED_MODELS:0 OLLAMA_MAX_QUEUE:512 OLLAMA_MODELS:C:\\Users\\frost\\.ollama\\models OLLAMA_MULTIUSER_CACHE:false OLLAMA_NEW_ENGINE:false OLLAMA_NOHISTORY:false OLLAMA_NOPRUNE:false OLLAMA_NUM_PARALLEL:1 OLLAMA_ORIGINS:[http://localhost https://localhost http://localhost:* https://localhost:* http://127.0.0.1 https://127.0.0.1 http://127.0.0.1:* https://127.0.0.1:* http://0.0.0.0 https://0.0.0.0 http://0.0.0.0:* https://0.0.0.0:* app://* file://* tauri://* vscode-webview://* vscode-file://*] OLLAMA_REMOTES:[ollama.com] OLLAMA_SCHED_SPREAD:false ROCR_VISIBLE_DEVICES:]"
time=2025-10-21T04:05:31.677-04:00 level=INFO source=images.go:522 msg="total blobs: 10"
time=2025-10-21T04:05:31.679-04:00 level=INFO source=images.go:529 msg="total unused blobs removed: 0"
time=2025-10-21T04:05:31.682-04:00 level=INFO source=routes.go:1564 msg="Listening on 127.0.0.1:11434 (version 0.12.6)"
time=2025-10-21T04:05:31.690-04:00 level=INFO source=runner.go:80 msg="discovering available GPUs..."
time=2025-10-21T04:05:35.419-04:00 level=INFO source=types.go:112 msg="inference compute" id=GPU-b28645fa-aaf6-86a1-ab1b-ef8d61df29f2 library=CUDA compute=12.0 name=CUDA0 description="NVIDIA GeForce RTX 5070" libdirs=ollama,cuda_v12 driver=12.9 pci_id=01:00.0 type=discrete total="11.9 GiB" available="10.7 GiB"
time=2025-10-21T04:05:35.419-04:00 level=INFO source=routes.go:1605 msg="entering low vram mode" "total vram"="11.9 GiB" threshold="20.0 GiB"
[GIN] 2025/10/21 - 16:10:58 | 200 |      2.6042ms |       127.0.0.1 | GET      "/api/version"
[GIN] 2025/10/21 - 16:10:58 | 200 |       4.149ms |       127.0.0.1 | GET      "/api/version"
[GIN] 2025/10/21 - 16:10:58 | 200 |     44.1026ms |       127.0.0.1 | GET      "/api/tags"
[GIN] 2025/10/21 - 16:10:58 | 200 |     89.1516ms |       127.0.0.1 | POST     "/api/show"
[GIN] 2025/10/21 - 16:11:11 | 200 |      7.9948ms |       127.0.0.1 | POST     "/api/show"
[GIN] 2025/10/21 - 16:11:11 | 200 |            0s |       127.0.0.1 | GET      "/api/version"
[GIN] 2025/10/21 - 16:11:11 | 200 |      7.3422ms |       127.0.0.1 | POST     "/api/show"
[GIN] 2025/10/21 - 16:11:11 | 200 |      1.0332ms |       127.0.0.1 | GET      "/api/tags"
llama_model_loader: loaded meta data with 25 key-value pairs and 291 tensors from C:\Users\frost\.ollama\models\blobs\sha256-f5074b1221da0f5a2910d33b642efa5b9eb58cfdddca1c79e16d7ad28aa2b31f (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = llama
llama_model_loader: - kv   1:                               general.name str              = Mistral-7B-Instruct-v0.3
llama_model_loader: - kv   2:                          llama.block_count u32              = 32
llama_model_loader: - kv   3:                       llama.context_length u32              = 32768
llama_model_loader: - kv   4:                     llama.embedding_length u32              = 4096
llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 14336
llama_model_loader: - kv   6:                 llama.attention.head_count u32              = 32
llama_model_loader: - kv   7:              llama.attention.head_count_kv u32              = 8
llama_model_loader: - kv   8:                       llama.rope.freq_base f32              = 1000000.000000
llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010
llama_model_loader: - kv  10:                          general.file_type u32              = 15
llama_model_loader: - kv  11:                           llama.vocab_size u32              = 32768
llama_model_loader: - kv  12:                 llama.rope.dimension_count u32              = 128
llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = llama
llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = default
llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,32768]   = ["<unk>", "<s>", "</s>", "[INST]", "[...
llama_model_loader: - kv  16:                      tokenizer.ggml.scores arr[f32,32768]   = [0.000000, 0.000000, 0.000000, 0.0000...
llama_model_loader: - kv  17:                  tokenizer.ggml.token_type arr[i32,32768]   = [2, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, ...
llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 1
llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 2
llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
llama_model_loader: - kv  21:               tokenizer.ggml.add_bos_token bool             = true
llama_model_loader: - kv  22:               tokenizer.ggml.add_eos_token bool             = false
llama_model_loader: - kv  23:                    tokenizer.chat_template str              = {{ bos_token }}{% for message in mess...
llama_model_loader: - kv  24:               general.quantization_version u32              = 2
llama_model_loader: - type  f32:   65 tensors
llama_model_loader: - type q4_K:  193 tensors
llama_model_loader: - type q6_K:   33 tensors
print_info: file format = GGUF V3 (latest)
print_info: file type   = Q4_K - Medium
print_info: file size   = 4.07 GiB (4.83 BPW) 
load: special_eos_id is not in special_eog_ids - the tokenizer config may be incorrect
load: printing all EOG tokens:
load:   - 2 ('</s>')
load: special tokens cache size = 771
load: token to piece cache size = 0.1731 MB
print_info: arch             = llama
print_info: vocab_only       = 1
print_info: model type       = ?B
print_info: model params     = 7.25 B
print_info: general.name     = Mistral-7B-Instruct-v0.3
print_info: vocab type       = SPM
print_info: n_vocab          = 32768
print_info: n_merges         = 0
print_info: BOS token        = 1 '<s>'
print_info: EOS token        = 2 '</s>'
print_info: UNK token        = 0 '<unk>'
print_info: LF token         = 781 '<0x0A>'
print_info: EOG token        = 2 '</s>'
print_info: max token length = 48
llama_model_load: vocab only - skipping tensors
time=2025-10-21T16:11:12.725-04:00 level=INFO source=cpu_windows.go:139 msg=packages count=1
time=2025-10-21T16:11:12.725-04:00 level=INFO source=cpu_windows.go:155 msg="efficiency cores detected" maxEfficiencyClass=1
time=2025-10-21T16:11:12.725-04:00 level=INFO source=cpu_windows.go:186 msg="" package=0 cores=12 efficiency=4 threads=20
time=2025-10-21T16:11:12.726-04:00 level=INFO source=server.go:400 msg="starting runner" cmd="C:\\Users\\frost\\AppData\\Local\\Programs\\Ollama\\ollama.exe runner --model C:\\Users\\frost\\.ollama\\models\\blobs\\sha256-f5074b1221da0f5a2910d33b642efa5b9eb58cfdddca1c79e16d7ad28aa2b31f --port 63410"
time=2025-10-21T16:11:12.728-04:00 level=INFO source=cpu_windows.go:139 msg=packages count=1
time=2025-10-21T16:11:12.728-04:00 level=INFO source=cpu_windows.go:155 msg="efficiency cores detected" maxEfficiencyClass=1
time=2025-10-21T16:11:12.728-04:00 level=INFO source=cpu_windows.go:186 msg="" package=0 cores=12 efficiency=4 threads=20
time=2025-10-21T16:11:12.728-04:00 level=INFO source=server.go:505 msg="system memory" total="63.8 GiB" free="45.3 GiB" free_swap="45.9 GiB"
time=2025-10-21T16:11:12.729-04:00 level=INFO source=memory.go:36 msg="new model will fit in available VRAM across minimum required GPUs, loading" model=C:\Users\frost\.ollama\models\blobs\sha256-f5074b1221da0f5a2910d33b642efa5b9eb58cfdddca1c79e16d7ad28aa2b31f library=CUDA parallel=1 required="5.4 GiB" gpus=1
time=2025-10-21T16:11:12.729-04:00 level=INFO source=server.go:545 msg=offload library=CUDA layers.requested=-1 layers.model=33 layers.offload=33 layers.split=[33] memory.available="[10.1 GiB]" memory.gpu_overhead="0 B" memory.required.full="5.4 GiB" memory.required.partial="5.4 GiB" memory.required.kv="512.0 MiB" memory.required.allocations="[5.4 GiB]" memory.weights.total="4.0 GiB" memory.weights.repeating="3.9 GiB" memory.weights.nonrepeating="105.0 MiB" memory.graph.full="296.0 MiB" memory.graph.partial="305.0 MiB"
time=2025-10-21T16:11:12.759-04:00 level=INFO source=runner.go:893 msg="starting go runner"
load_backend: loaded CPU backend from C:\Users\frost\AppData\Local\Programs\Ollama\lib\ollama\ggml-cpu-alderlake.dll
ggml_cuda_init: GGML_CUDA_FORCE_MMQ:    no
ggml_cuda_init: GGML_CUDA_FORCE_CUBLAS: no
ggml_cuda_init: found 1 CUDA devices:
  Device 0: NVIDIA GeForce RTX 5070, compute capability 12.0, VMM: yes, ID: GPU-b28645fa-aaf6-86a1-ab1b-ef8d61df29f2
load_backend: loaded CUDA backend from C:\Users\frost\AppData\Local\Programs\Ollama\lib\ollama\cuda_v12\ggml-cuda.dll
time=2025-10-21T16:11:12.859-04:00 level=INFO source=ggml.go:104 msg=system CPU.0.SSE3=1 CPU.0.SSSE3=1 CPU.0.AVX=1 CPU.0.AVX_VNNI=1 CPU.0.AVX2=1 CPU.0.F16C=1 CPU.0.FMA=1 CPU.0.BMI2=1 CPU.0.LLAMAFILE=1 CPU.1.LLAMAFILE=1 CUDA.0.ARCHS=500,520,600,610,700,750,800,860,870,890,900,1200 CUDA.0.USE_GRAPHS=1 CUDA.0.PEER_MAX_BATCH_SIZE=128 compiler=cgo(clang)
time=2025-10-21T16:11:12.860-04:00 level=INFO source=runner.go:929 msg="Server listening on 127.0.0.1:63410"
time=2025-10-21T16:11:12.868-04:00 level=INFO source=runner.go:828 msg=load request="{Operation:commit LoraPath:[] Parallel:1 BatchSize:512 FlashAttention:false KvSize:4096 KvCacheType: NumThreads:8 GPULayers:33[ID:GPU-b28645fa-aaf6-86a1-ab1b-ef8d61df29f2 Layers:33(0..32)] MultiUserCache:false ProjectorPath: MainGPU:0 UseMmap:false}"
time=2025-10-21T16:11:12.869-04:00 level=INFO source=server.go:1272 msg="waiting for llama runner to start responding"
time=2025-10-21T16:11:12.869-04:00 level=INFO source=server.go:1306 msg="waiting for server to become available" status="llm server loading model"
ggml_backend_cuda_device_get_memory utilizing NVML memory reporting free: 10841395200 total: 12820938752
llama_model_load_from_file_impl: using device CUDA0 (NVIDIA GeForce RTX 5070) (0000:01:00.0) - 10339 MiB free
llama_model_loader: loaded meta data with 25 key-value pairs and 291 tensors from C:\Users\frost\.ollama\models\blobs\sha256-f5074b1221da0f5a2910d33b642efa5b9eb58cfdddca1c79e16d7ad28aa2b31f (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = llama
llama_model_loader: - kv   1:                               general.name str              = Mistral-7B-Instruct-v0.3
llama_model_loader: - kv   2:                          llama.block_count u32              = 32
llama_model_loader: - kv   3:                       llama.context_length u32              = 32768
llama_model_loader: - kv   4:                     llama.embedding_length u32              = 4096
llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 14336
llama_model_loader: - kv   6:                 llama.attention.head_count u32              = 32
llama_model_loader: - kv   7:              llama.attention.head_count_kv u32              = 8
llama_model_loader: - kv   8:                       llama.rope.freq_base f32              = 1000000.000000
llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010
llama_model_loader: - kv  10:                          general.file_type u32              = 15
llama_model_loader: - kv  11:                           llama.vocab_size u32              = 32768
llama_model_loader: - kv  12:                 llama.rope.dimension_count u32              = 128
llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = llama
llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = default
llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,32768]   = ["<unk>", "<s>", "</s>", "[INST]", "[...
llama_model_loader: - kv  16:                      tokenizer.ggml.scores arr[f32,32768]   = [0.000000, 0.000000, 0.000000, 0.0000...
llama_model_loader: - kv  17:                  tokenizer.ggml.token_type arr[i32,32768]   = [2, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, ...
llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 1
llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 2
llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
llama_model_loader: - kv  21:               tokenizer.ggml.add_bos_token bool             = true
llama_model_loader: - kv  22:               tokenizer.ggml.add_eos_token bool             = false
llama_model_loader: - kv  23:                    tokenizer.chat_template str              = {{ bos_token }}{% for message in mess...
llama_model_loader: - kv  24:               general.quantization_version u32              = 2
llama_model_loader: - type  f32:   65 tensors
llama_model_loader: - type q4_K:  193 tensors
llama_model_loader: - type q6_K:   33 tensors
print_info: file format = GGUF V3 (latest)
print_info: file type   = Q4_K - Medium
print_info: file size   = 4.07 GiB (4.83 BPW) 
load: special_eos_id is not in special_eog_ids - the tokenizer config may be incorrect
load: printing all EOG tokens:
load:   - 2 ('</s>')
load: special tokens cache size = 771
load: token to piece cache size = 0.1731 MB
print_info: arch             = llama
print_info: vocab_only       = 0
print_info: n_ctx_train      = 32768
print_info: n_embd           = 4096
print_info: n_layer          = 32
print_info: n_head           = 32
print_info: n_head_kv        = 8
print_info: n_rot            = 128
print_info: n_swa            = 0
print_info: is_swa_any       = 0
print_info: n_embd_head_k    = 128
print_info: n_embd_head_v    = 128
print_info: n_gqa            = 4
print_info: n_embd_k_gqa     = 1024
print_info: n_embd_v_gqa     = 1024
print_info: f_norm_eps       = 0.0e+00
print_info: f_norm_rms_eps   = 1.0e-05
print_info: f_clamp_kqv      = 0.0e+00
print_info: f_max_alibi_bias = 0.0e+00
print_info: f_logit_scale    = 0.0e+00
print_info: f_attn_scale     = 0.0e+00
print_info: n_ff             = 14336
print_info: n_expert         = 0
print_info: n_expert_used    = 0
print_info: causal attn      = 1
print_info: pooling type     = 0
print_info: rope type        = 0
print_info: rope scaling     = linear
print_info: freq_base_train  = 1000000.0
print_info: freq_scale_train = 1
print_info: n_ctx_orig_yarn  = 32768
print_info: rope_finetuned   = unknown
print_info: model type       = 7B
print_info: model params     = 7.25 B
print_info: general.name     = Mistral-7B-Instruct-v0.3
print_info: vocab type       = SPM
print_info: n_vocab          = 32768
print_info: n_merges         = 0
print_info: BOS token        = 1 '<s>'
print_info: EOS token        = 2 '</s>'
print_info: UNK token        = 0 '<unk>'
print_info: LF token         = 781 '<0x0A>'
print_info: EOG token        = 2 '</s>'
print_info: max token length = 48
load_tensors: loading model tensors, this can take a while... (mmap = false)
load_tensors: offloading 32 repeating layers to GPU
load_tensors: offloading output layer to GPU
load_tensors: offloaded 33/33 layers to GPU
load_tensors:        CUDA0 model buffer size =  4097.52 MiB
load_tensors:          CPU model buffer size =    72.00 MiB
llama_init_from_model: model default pooling_type is [0], but [-1] was specified
llama_context: constructing llama_context
llama_context: n_seq_max     = 1
llama_context: n_ctx         = 4096
llama_context: n_ctx_per_seq = 4096
llama_context: n_batch       = 512
llama_context: n_ubatch      = 512
llama_context: causal_attn   = 1
llama_context: flash_attn    = disabled
llama_context: kv_unified    = false
llama_context: freq_base     = 1000000.0
llama_context: freq_scale    = 1
llama_context: n_ctx_per_seq (4096) < n_ctx_train (32768) -- the full capacity of the model will not be utilized
llama_context:  CUDA_Host  output buffer size =     0.14 MiB
llama_kv_cache:      CUDA0 KV buffer size =   512.00 MiB
llama_kv_cache: size =  512.00 MiB (  4096 cells,  32 layers,  1/1 seqs), K (f16):  256.00 MiB, V (f16):  256.00 MiB
llama_context:      CUDA0 compute buffer size =   300.01 MiB
llama_context:  CUDA_Host compute buffer size =    20.01 MiB
llama_context: graph nodes  = 1158
llama_context: graph splits = 2
time=2025-10-21T16:11:16.125-04:00 level=INFO source=server.go:1310 msg="llama runner started in 3.40 seconds"
time=2025-10-21T16:11:16.125-04:00 level=INFO source=sched.go:482 msg="loaded runners" count=1
time=2025-10-21T16:11:16.125-04:00 level=INFO source=server.go:1272 msg="waiting for llama runner to start responding"
time=2025-10-21T16:11:16.126-04:00 level=INFO source=server.go:1310 msg="llama runner started in 3.40 seconds"
[GIN] 2025/10/21 - 16:11:19 | 200 |     8.516823s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 16:12:06 | 200 |      8.6919ms |       127.0.0.1 | POST     "/api/show"
[GIN] 2025/10/21 - 16:12:06 | 200 |      7.7816ms |       127.0.0.1 | POST     "/api/show"
[GIN] 2025/10/21 - 16:12:08 | 200 |      2.03699s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 19:45:50 | 404 |     42.5081ms |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 19:46:03 | 200 |     34.5351ms |       127.0.0.1 | GET      "/api/tags"
llama_model_loader: loaded meta data with 22 key-value pairs and 291 tensors from C:\Users\frost\.ollama\models\blobs\sha256-6a0746a1ec1aef3e7ec53868f220ff6e389f6f8ef87a01d77c96807de94ca2aa (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = llama
llama_model_loader: - kv   1:                               general.name str              = Meta-Llama-3-8B-Instruct
llama_model_loader: - kv   2:                          llama.block_count u32              = 32
llama_model_loader: - kv   3:                       llama.context_length u32              = 8192
llama_model_loader: - kv   4:                     llama.embedding_length u32              = 4096
llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 14336
llama_model_loader: - kv   6:                 llama.attention.head_count u32              = 32
llama_model_loader: - kv   7:              llama.attention.head_count_kv u32              = 8
llama_model_loader: - kv   8:                       llama.rope.freq_base f32              = 500000.000000
llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010
llama_model_loader: - kv  10:                          general.file_type u32              = 2
llama_model_loader: - kv  11:                           llama.vocab_size u32              = 128256
llama_model_loader: - kv  12:                 llama.rope.dimension_count u32              = 128
llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = llama-bpe
llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,128256]  = ["!", "\"", "#", "$", "%", "&", "'", ...
llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,128256]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,280147]  = ["Ġ Ġ", "Ġ ĠĠĠ", "ĠĠ ĠĠ", "...
llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 128000
llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 128009
llama_model_loader: - kv  20:                    tokenizer.chat_template str              = {% set loop_messages = messages %}{% ...
llama_model_loader: - kv  21:               general.quantization_version u32              = 2
llama_model_loader: - type  f32:   65 tensors
llama_model_loader: - type q4_0:  225 tensors
llama_model_loader: - type q6_K:    1 tensors
print_info: file format = GGUF V3 (latest)
print_info: file type   = Q4_0
print_info: file size   = 4.33 GiB (4.64 BPW) 
load: printing all EOG tokens:
load:   - 128001 ('<|end_of_text|>')
load:   - 128009 ('<|eot_id|>')
load: special tokens cache size = 256
load: token to piece cache size = 0.8000 MB
print_info: arch             = llama
print_info: vocab_only       = 1
print_info: model type       = ?B
print_info: model params     = 8.03 B
print_info: general.name     = Meta-Llama-3-8B-Instruct
print_info: vocab type       = BPE
print_info: n_vocab          = 128256
print_info: n_merges         = 280147
print_info: BOS token        = 128000 '<|begin_of_text|>'
print_info: EOS token        = 128009 '<|eot_id|>'
print_info: EOT token        = 128001 '<|end_of_text|>'
print_info: LF token         = 198 'Ċ'
print_info: EOG token        = 128001 '<|end_of_text|>'
print_info: EOG token        = 128009 '<|eot_id|>'
print_info: max token length = 256
llama_model_load: vocab only - skipping tensors
time=2025-10-21T19:46:26.296-04:00 level=INFO source=cpu_windows.go:139 msg=packages count=1
time=2025-10-21T19:46:26.297-04:00 level=INFO source=cpu_windows.go:155 msg="efficiency cores detected" maxEfficiencyClass=1
time=2025-10-21T19:46:26.297-04:00 level=INFO source=cpu_windows.go:186 msg="" package=0 cores=12 efficiency=4 threads=20
time=2025-10-21T19:46:26.321-04:00 level=INFO source=server.go:400 msg="starting runner" cmd="C:\\Users\\frost\\AppData\\Local\\Programs\\Ollama\\ollama.exe runner --model C:\\Users\\frost\\.ollama\\models\\blobs\\sha256-6a0746a1ec1aef3e7ec53868f220ff6e389f6f8ef87a01d77c96807de94ca2aa --port 62359"
time=2025-10-21T19:46:26.327-04:00 level=INFO source=cpu_windows.go:139 msg=packages count=1
time=2025-10-21T19:46:26.327-04:00 level=INFO source=cpu_windows.go:155 msg="efficiency cores detected" maxEfficiencyClass=1
time=2025-10-21T19:46:26.327-04:00 level=INFO source=cpu_windows.go:186 msg="" package=0 cores=12 efficiency=4 threads=20
time=2025-10-21T19:46:26.327-04:00 level=INFO source=server.go:505 msg="system memory" total="63.8 GiB" free="43.6 GiB" free_swap="43.6 GiB"
time=2025-10-21T19:46:26.327-04:00 level=INFO source=memory.go:36 msg="new model will fit in available VRAM across minimum required GPUs, loading" model=C:\Users\frost\.ollama\models\blobs\sha256-6a0746a1ec1aef3e7ec53868f220ff6e389f6f8ef87a01d77c96807de94ca2aa library=CUDA parallel=1 required="5.4 GiB" gpus=1
time=2025-10-21T19:46:26.328-04:00 level=INFO source=server.go:545 msg=offload library=CUDA layers.requested=-1 layers.model=33 layers.offload=33 layers.split=[33] memory.available="[10.8 GiB]" memory.gpu_overhead="0 B" memory.required.full="5.4 GiB" memory.required.partial="5.4 GiB" memory.required.kv="512.0 MiB" memory.required.allocations="[5.4 GiB]" memory.weights.total="4.1 GiB" memory.weights.repeating="3.7 GiB" memory.weights.nonrepeating="411.0 MiB" memory.graph.full="296.0 MiB" memory.graph.partial="677.5 MiB"
time=2025-10-21T19:46:26.438-04:00 level=INFO source=runner.go:893 msg="starting go runner"
load_backend: loaded CPU backend from C:\Users\frost\AppData\Local\Programs\Ollama\lib\ollama\ggml-cpu-alderlake.dll
ggml_cuda_init: GGML_CUDA_FORCE_MMQ:    no
ggml_cuda_init: GGML_CUDA_FORCE_CUBLAS: no
ggml_cuda_init: found 1 CUDA devices:
  Device 0: NVIDIA GeForce RTX 5070, compute capability 12.0, VMM: yes, ID: GPU-b28645fa-aaf6-86a1-ab1b-ef8d61df29f2
load_backend: loaded CUDA backend from C:\Users\frost\AppData\Local\Programs\Ollama\lib\ollama\cuda_v12\ggml-cuda.dll
time=2025-10-21T19:46:26.608-04:00 level=INFO source=ggml.go:104 msg=system CPU.0.SSE3=1 CPU.0.SSSE3=1 CPU.0.AVX=1 CPU.0.AVX_VNNI=1 CPU.0.AVX2=1 CPU.0.F16C=1 CPU.0.FMA=1 CPU.0.BMI2=1 CPU.0.LLAMAFILE=1 CPU.1.LLAMAFILE=1 CUDA.0.ARCHS=500,520,600,610,700,750,800,860,870,890,900,1200 CUDA.0.USE_GRAPHS=1 CUDA.0.PEER_MAX_BATCH_SIZE=128 compiler=cgo(clang)
time=2025-10-21T19:46:26.614-04:00 level=INFO source=runner.go:929 msg="Server listening on 127.0.0.1:62359"
time=2025-10-21T19:46:26.634-04:00 level=INFO source=runner.go:828 msg=load request="{Operation:commit LoraPath:[] Parallel:1 BatchSize:512 FlashAttention:false KvSize:4096 KvCacheType: NumThreads:8 GPULayers:33[ID:GPU-b28645fa-aaf6-86a1-ab1b-ef8d61df29f2 Layers:33(0..32)] MultiUserCache:false ProjectorPath: MainGPU:0 UseMmap:false}"
time=2025-10-21T19:46:26.640-04:00 level=INFO source=server.go:1272 msg="waiting for llama runner to start responding"
time=2025-10-21T19:46:26.643-04:00 level=INFO source=server.go:1306 msg="waiting for server to become available" status="llm server loading model"
ggml_backend_cuda_device_get_memory utilizing NVML memory reporting free: 11644145664 total: 12820938752
llama_model_load_from_file_impl: using device CUDA0 (NVIDIA GeForce RTX 5070) (0000:01:00.0) - 11104 MiB free
llama_model_loader: loaded meta data with 22 key-value pairs and 291 tensors from C:\Users\frost\.ollama\models\blobs\sha256-6a0746a1ec1aef3e7ec53868f220ff6e389f6f8ef87a01d77c96807de94ca2aa (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = llama
llama_model_loader: - kv   1:                               general.name str              = Meta-Llama-3-8B-Instruct
llama_model_loader: - kv   2:                          llama.block_count u32              = 32
llama_model_loader: - kv   3:                       llama.context_length u32              = 8192
llama_model_loader: - kv   4:                     llama.embedding_length u32              = 4096
llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 14336
llama_model_loader: - kv   6:                 llama.attention.head_count u32              = 32
llama_model_loader: - kv   7:              llama.attention.head_count_kv u32              = 8
llama_model_loader: - kv   8:                       llama.rope.freq_base f32              = 500000.000000
llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010
llama_model_loader: - kv  10:                          general.file_type u32              = 2
llama_model_loader: - kv  11:                           llama.vocab_size u32              = 128256
llama_model_loader: - kv  12:                 llama.rope.dimension_count u32              = 128
llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = llama-bpe
llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,128256]  = ["!", "\"", "#", "$", "%", "&", "'", ...
llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,128256]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,280147]  = ["Ġ Ġ", "Ġ ĠĠĠ", "ĠĠ ĠĠ", "...
llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 128000
llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 128009
llama_model_loader: - kv  20:                    tokenizer.chat_template str              = {% set loop_messages = messages %}{% ...
llama_model_loader: - kv  21:               general.quantization_version u32              = 2
llama_model_loader: - type  f32:   65 tensors
llama_model_loader: - type q4_0:  225 tensors
llama_model_loader: - type q6_K:    1 tensors
print_info: file format = GGUF V3 (latest)
print_info: file type   = Q4_0
print_info: file size   = 4.33 GiB (4.64 BPW) 
load: printing all EOG tokens:
load:   - 128001 ('<|end_of_text|>')
load:   - 128009 ('<|eot_id|>')
load: special tokens cache size = 256
load: token to piece cache size = 0.8000 MB
print_info: arch             = llama
print_info: vocab_only       = 0
print_info: n_ctx_train      = 8192
print_info: n_embd           = 4096
print_info: n_layer          = 32
print_info: n_head           = 32
print_info: n_head_kv        = 8
print_info: n_rot            = 128
print_info: n_swa            = 0
print_info: is_swa_any       = 0
print_info: n_embd_head_k    = 128
print_info: n_embd_head_v    = 128
print_info: n_gqa            = 4
print_info: n_embd_k_gqa     = 1024
print_info: n_embd_v_gqa     = 1024
print_info: f_norm_eps       = 0.0e+00
print_info: f_norm_rms_eps   = 1.0e-05
print_info: f_clamp_kqv      = 0.0e+00
print_info: f_max_alibi_bias = 0.0e+00
print_info: f_logit_scale    = 0.0e+00
print_info: f_attn_scale     = 0.0e+00
print_info: n_ff             = 14336
print_info: n_expert         = 0
print_info: n_expert_used    = 0
print_info: causal attn      = 1
print_info: pooling type     = 0
print_info: rope type        = 0
print_info: rope scaling     = linear
print_info: freq_base_train  = 500000.0
print_info: freq_scale_train = 1
print_info: n_ctx_orig_yarn  = 8192
print_info: rope_finetuned   = unknown
print_info: model type       = 8B
print_info: model params     = 8.03 B
print_info: general.name     = Meta-Llama-3-8B-Instruct
print_info: vocab type       = BPE
print_info: n_vocab          = 128256
print_info: n_merges         = 280147
print_info: BOS token        = 128000 '<|begin_of_text|>'
print_info: EOS token        = 128009 '<|eot_id|>'
print_info: EOT token        = 128001 '<|end_of_text|>'
print_info: LF token         = 198 'Ċ'
print_info: EOG token        = 128001 '<|end_of_text|>'
print_info: EOG token        = 128009 '<|eot_id|>'
print_info: max token length = 256
load_tensors: loading model tensors, this can take a while... (mmap = false)
load_tensors: offloading 32 repeating layers to GPU
load_tensors: offloading output layer to GPU
load_tensors: offloaded 33/33 layers to GPU
load_tensors:    CUDA_Host model buffer size =   281.81 MiB
load_tensors:        CUDA0 model buffer size =  4155.99 MiB
llama_init_from_model: model default pooling_type is [0], but [-1] was specified
llama_context: constructing llama_context
llama_context: n_seq_max     = 1
llama_context: n_ctx         = 4096
llama_context: n_ctx_per_seq = 4096
llama_context: n_batch       = 512
llama_context: n_ubatch      = 512
llama_context: causal_attn   = 1
llama_context: flash_attn    = disabled
llama_context: kv_unified    = false
llama_context: freq_base     = 500000.0
llama_context: freq_scale    = 1
llama_context: n_ctx_per_seq (4096) < n_ctx_train (8192) -- the full capacity of the model will not be utilized
llama_context:  CUDA_Host  output buffer size =     0.50 MiB
llama_kv_cache:      CUDA0 KV buffer size =   512.00 MiB
llama_kv_cache: size =  512.00 MiB (  4096 cells,  32 layers,  1/1 seqs), K (f16):  256.00 MiB, V (f16):  256.00 MiB
llama_context:      CUDA0 compute buffer size =   300.01 MiB
llama_context:  CUDA_Host compute buffer size =    20.01 MiB
llama_context: graph nodes  = 1158
llama_context: graph splits = 2
time=2025-10-21T19:46:30.711-04:00 level=INFO source=server.go:1310 msg="llama runner started in 4.39 seconds"
time=2025-10-21T19:46:30.711-04:00 level=INFO source=sched.go:482 msg="loaded runners" count=1
time=2025-10-21T19:46:30.711-04:00 level=INFO source=server.go:1272 msg="waiting for llama runner to start responding"
time=2025-10-21T19:46:30.712-04:00 level=INFO source=server.go:1310 msg="llama runner started in 4.39 seconds"
[GIN] 2025/10/21 - 19:46:30 | 200 |    7.8267852s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 19:49:32 | 200 |    230.4493ms |       127.0.0.1 | POST     "/api/chat"
llama_model_loader: loaded meta data with 22 key-value pairs and 291 tensors from C:\Users\frost\.ollama\models\blobs\sha256-6a0746a1ec1aef3e7ec53868f220ff6e389f6f8ef87a01d77c96807de94ca2aa (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = llama
llama_model_loader: - kv   1:                               general.name str              = Meta-Llama-3-8B-Instruct
llama_model_loader: - kv   2:                          llama.block_count u32              = 32
llama_model_loader: - kv   3:                       llama.context_length u32              = 8192
llama_model_loader: - kv   4:                     llama.embedding_length u32              = 4096
llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 14336
llama_model_loader: - kv   6:                 llama.attention.head_count u32              = 32
llama_model_loader: - kv   7:              llama.attention.head_count_kv u32              = 8
llama_model_loader: - kv   8:                       llama.rope.freq_base f32              = 500000.000000
llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010
llama_model_loader: - kv  10:                          general.file_type u32              = 2
llama_model_loader: - kv  11:                           llama.vocab_size u32              = 128256
llama_model_loader: - kv  12:                 llama.rope.dimension_count u32              = 128
llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = llama-bpe
llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,128256]  = ["!", "\"", "#", "$", "%", "&", "'", ...
llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,128256]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,280147]  = ["Ġ Ġ", "Ġ ĠĠĠ", "ĠĠ ĠĠ", "...
llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 128000
llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 128009
llama_model_loader: - kv  20:                    tokenizer.chat_template str              = {% set loop_messages = messages %}{% ...
llama_model_loader: - kv  21:               general.quantization_version u32              = 2
llama_model_loader: - type  f32:   65 tensors
llama_model_loader: - type q4_0:  225 tensors
llama_model_loader: - type q6_K:    1 tensors
print_info: file format = GGUF V3 (latest)
print_info: file type   = Q4_0
print_info: file size   = 4.33 GiB (4.64 BPW) 
load: printing all EOG tokens:
load:   - 128001 ('<|end_of_text|>')
load:   - 128009 ('<|eot_id|>')
load: special tokens cache size = 256
load: token to piece cache size = 0.8000 MB
print_info: arch             = llama
print_info: vocab_only       = 1
print_info: model type       = ?B
print_info: model params     = 8.03 B
print_info: general.name     = Meta-Llama-3-8B-Instruct
print_info: vocab type       = BPE
print_info: n_vocab          = 128256
print_info: n_merges         = 280147
print_info: BOS token        = 128000 '<|begin_of_text|>'
print_info: EOS token        = 128009 '<|eot_id|>'
print_info: EOT token        = 128001 '<|end_of_text|>'
print_info: LF token         = 198 'Ċ'
print_info: EOG token        = 128001 '<|end_of_text|>'
print_info: EOG token        = 128009 '<|eot_id|>'
print_info: max token length = 256
llama_model_load: vocab only - skipping tensors
time=2025-10-21T20:02:54.013-04:00 level=INFO source=cpu_windows.go:139 msg=packages count=1
time=2025-10-21T20:02:54.013-04:00 level=INFO source=cpu_windows.go:155 msg="efficiency cores detected" maxEfficiencyClass=1
time=2025-10-21T20:02:54.013-04:00 level=INFO source=cpu_windows.go:186 msg="" package=0 cores=12 efficiency=4 threads=20
time=2025-10-21T20:02:54.015-04:00 level=INFO source=server.go:400 msg="starting runner" cmd="C:\\Users\\frost\\AppData\\Local\\Programs\\Ollama\\ollama.exe runner --model C:\\Users\\frost\\.ollama\\models\\blobs\\sha256-6a0746a1ec1aef3e7ec53868f220ff6e389f6f8ef87a01d77c96807de94ca2aa --port 52409"
time=2025-10-21T20:02:54.017-04:00 level=INFO source=cpu_windows.go:139 msg=packages count=1
time=2025-10-21T20:02:54.017-04:00 level=INFO source=cpu_windows.go:155 msg="efficiency cores detected" maxEfficiencyClass=1
time=2025-10-21T20:02:54.017-04:00 level=INFO source=cpu_windows.go:186 msg="" package=0 cores=12 efficiency=4 threads=20
time=2025-10-21T20:02:54.017-04:00 level=INFO source=server.go:505 msg="system memory" total="63.8 GiB" free="48.0 GiB" free_swap="48.4 GiB"
time=2025-10-21T20:02:54.018-04:00 level=INFO source=memory.go:36 msg="new model will fit in available VRAM across minimum required GPUs, loading" model=C:\Users\frost\.ollama\models\blobs\sha256-6a0746a1ec1aef3e7ec53868f220ff6e389f6f8ef87a01d77c96807de94ca2aa library=CUDA parallel=1 required="5.4 GiB" gpus=1
time=2025-10-21T20:02:54.018-04:00 level=INFO source=server.go:545 msg=offload library=CUDA layers.requested=-1 layers.model=33 layers.offload=33 layers.split=[33] memory.available="[10.7 GiB]" memory.gpu_overhead="0 B" memory.required.full="5.4 GiB" memory.required.partial="5.4 GiB" memory.required.kv="512.0 MiB" memory.required.allocations="[5.4 GiB]" memory.weights.total="4.1 GiB" memory.weights.repeating="3.7 GiB" memory.weights.nonrepeating="411.0 MiB" memory.graph.full="296.0 MiB" memory.graph.partial="677.5 MiB"
time=2025-10-21T20:02:54.062-04:00 level=INFO source=runner.go:893 msg="starting go runner"
load_backend: loaded CPU backend from C:\Users\frost\AppData\Local\Programs\Ollama\lib\ollama\ggml-cpu-alderlake.dll
ggml_cuda_init: GGML_CUDA_FORCE_MMQ:    no
ggml_cuda_init: GGML_CUDA_FORCE_CUBLAS: no
ggml_cuda_init: found 1 CUDA devices:
  Device 0: NVIDIA GeForce RTX 5070, compute capability 12.0, VMM: yes, ID: GPU-b28645fa-aaf6-86a1-ab1b-ef8d61df29f2
load_backend: loaded CUDA backend from C:\Users\frost\AppData\Local\Programs\Ollama\lib\ollama\cuda_v12\ggml-cuda.dll
time=2025-10-21T20:02:54.214-04:00 level=INFO source=ggml.go:104 msg=system CPU.0.SSE3=1 CPU.0.SSSE3=1 CPU.0.AVX=1 CPU.0.AVX_VNNI=1 CPU.0.AVX2=1 CPU.0.F16C=1 CPU.0.FMA=1 CPU.0.BMI2=1 CPU.0.LLAMAFILE=1 CPU.1.LLAMAFILE=1 CUDA.0.ARCHS=500,520,600,610,700,750,800,860,870,890,900,1200 CUDA.0.USE_GRAPHS=1 CUDA.0.PEER_MAX_BATCH_SIZE=128 compiler=cgo(clang)
time=2025-10-21T20:02:54.215-04:00 level=INFO source=runner.go:929 msg="Server listening on 127.0.0.1:52409"
time=2025-10-21T20:02:54.217-04:00 level=INFO source=runner.go:828 msg=load request="{Operation:commit LoraPath:[] Parallel:1 BatchSize:512 FlashAttention:false KvSize:4096 KvCacheType: NumThreads:8 GPULayers:33[ID:GPU-b28645fa-aaf6-86a1-ab1b-ef8d61df29f2 Layers:33(0..32)] MultiUserCache:false ProjectorPath: MainGPU:0 UseMmap:false}"
time=2025-10-21T20:02:54.218-04:00 level=INFO source=server.go:1272 msg="waiting for llama runner to start responding"
time=2025-10-21T20:02:54.219-04:00 level=INFO source=server.go:1306 msg="waiting for server to become available" status="llm server loading model"
ggml_backend_cuda_device_get_memory utilizing NVML memory reporting free: 11514523648 total: 12820938752
llama_model_load_from_file_impl: using device CUDA0 (NVIDIA GeForce RTX 5070) (0000:01:00.0) - 10981 MiB free
llama_model_loader: loaded meta data with 22 key-value pairs and 291 tensors from C:\Users\frost\.ollama\models\blobs\sha256-6a0746a1ec1aef3e7ec53868f220ff6e389f6f8ef87a01d77c96807de94ca2aa (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = llama
llama_model_loader: - kv   1:                               general.name str              = Meta-Llama-3-8B-Instruct
llama_model_loader: - kv   2:                          llama.block_count u32              = 32
llama_model_loader: - kv   3:                       llama.context_length u32              = 8192
llama_model_loader: - kv   4:                     llama.embedding_length u32              = 4096
llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 14336
llama_model_loader: - kv   6:                 llama.attention.head_count u32              = 32
llama_model_loader: - kv   7:              llama.attention.head_count_kv u32              = 8
llama_model_loader: - kv   8:                       llama.rope.freq_base f32              = 500000.000000
llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010
llama_model_loader: - kv  10:                          general.file_type u32              = 2
llama_model_loader: - kv  11:                           llama.vocab_size u32              = 128256
llama_model_loader: - kv  12:                 llama.rope.dimension_count u32              = 128
llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = llama-bpe
llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,128256]  = ["!", "\"", "#", "$", "%", "&", "'", ...
llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,128256]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,280147]  = ["Ġ Ġ", "Ġ ĠĠĠ", "ĠĠ ĠĠ", "...
llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 128000
llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 128009
llama_model_loader: - kv  20:                    tokenizer.chat_template str              = {% set loop_messages = messages %}{% ...
llama_model_loader: - kv  21:               general.quantization_version u32              = 2
llama_model_loader: - type  f32:   65 tensors
llama_model_loader: - type q4_0:  225 tensors
llama_model_loader: - type q6_K:    1 tensors
print_info: file format = GGUF V3 (latest)
print_info: file type   = Q4_0
print_info: file size   = 4.33 GiB (4.64 BPW) 
load: printing all EOG tokens:
load:   - 128001 ('<|end_of_text|>')
load:   - 128009 ('<|eot_id|>')
load: special tokens cache size = 256
load: token to piece cache size = 0.8000 MB
print_info: arch             = llama
print_info: vocab_only       = 0
print_info: n_ctx_train      = 8192
print_info: n_embd           = 4096
print_info: n_layer          = 32
print_info: n_head           = 32
print_info: n_head_kv        = 8
print_info: n_rot            = 128
print_info: n_swa            = 0
print_info: is_swa_any       = 0
print_info: n_embd_head_k    = 128
print_info: n_embd_head_v    = 128
print_info: n_gqa            = 4
print_info: n_embd_k_gqa     = 1024
print_info: n_embd_v_gqa     = 1024
print_info: f_norm_eps       = 0.0e+00
print_info: f_norm_rms_eps   = 1.0e-05
print_info: f_clamp_kqv      = 0.0e+00
print_info: f_max_alibi_bias = 0.0e+00
print_info: f_logit_scale    = 0.0e+00
print_info: f_attn_scale     = 0.0e+00
print_info: n_ff             = 14336
print_info: n_expert         = 0
print_info: n_expert_used    = 0
print_info: causal attn      = 1
print_info: pooling type     = 0
print_info: rope type        = 0
print_info: rope scaling     = linear
print_info: freq_base_train  = 500000.0
print_info: freq_scale_train = 1
print_info: n_ctx_orig_yarn  = 8192
print_info: rope_finetuned   = unknown
print_info: model type       = 8B
print_info: model params     = 8.03 B
print_info: general.name     = Meta-Llama-3-8B-Instruct
print_info: vocab type       = BPE
print_info: n_vocab          = 128256
print_info: n_merges         = 280147
print_info: BOS token        = 128000 '<|begin_of_text|>'
print_info: EOS token        = 128009 '<|eot_id|>'
print_info: EOT token        = 128001 '<|end_of_text|>'
print_info: LF token         = 198 'Ċ'
print_info: EOG token        = 128001 '<|end_of_text|>'
print_info: EOG token        = 128009 '<|eot_id|>'
print_info: max token length = 256
load_tensors: loading model tensors, this can take a while... (mmap = false)
load_tensors: offloading 32 repeating layers to GPU
load_tensors: offloading output layer to GPU
load_tensors: offloaded 33/33 layers to GPU
load_tensors:    CUDA_Host model buffer size =   281.81 MiB
load_tensors:        CUDA0 model buffer size =  4155.99 MiB
llama_init_from_model: model default pooling_type is [0], but [-1] was specified
llama_context: constructing llama_context
llama_context: n_seq_max     = 1
llama_context: n_ctx         = 4096
llama_context: n_ctx_per_seq = 4096
llama_context: n_batch       = 512
llama_context: n_ubatch      = 512
llama_context: causal_attn   = 1
llama_context: flash_attn    = disabled
llama_context: kv_unified    = false
llama_context: freq_base     = 500000.0
llama_context: freq_scale    = 1
llama_context: n_ctx_per_seq (4096) < n_ctx_train (8192) -- the full capacity of the model will not be utilized
llama_context:  CUDA_Host  output buffer size =     0.50 MiB
llama_kv_cache:      CUDA0 KV buffer size =   512.00 MiB
llama_kv_cache: size =  512.00 MiB (  4096 cells,  32 layers,  1/1 seqs), K (f16):  256.00 MiB, V (f16):  256.00 MiB
llama_context:      CUDA0 compute buffer size =   300.01 MiB
llama_context:  CUDA_Host compute buffer size =    20.01 MiB
llama_context: graph nodes  = 1158
llama_context: graph splits = 2
time=2025-10-21T20:02:56.234-04:00 level=INFO source=server.go:1310 msg="llama runner started in 2.22 seconds"
time=2025-10-21T20:02:56.234-04:00 level=INFO source=sched.go:482 msg="loaded runners" count=1
time=2025-10-21T20:02:56.234-04:00 level=INFO source=server.go:1272 msg="waiting for llama runner to start responding"
time=2025-10-21T20:02:56.234-04:00 level=INFO source=server.go:1310 msg="llama runner started in 2.22 seconds"
[GIN] 2025/10/21 - 20:02:57 | 200 |    4.6469847s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:04:24 | 200 |    4.7080988s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:06:43 | 200 |     6.717286s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:06:45 | 200 |    1.8619371s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:06:48 | 200 |    2.0227842s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:06:50 | 200 |    2.0917393s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:06:52 | 200 |    2.4761588s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:06:54 | 200 |    1.7169701s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:06:57 | 200 |    2.9283792s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:06:59 | 200 |    2.0047769s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:07:02 | 200 |    2.1134875s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:07:09 | 200 |    7.4608942s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:07:13 | 200 |    4.2634781s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:07:16 | 200 |    2.2406953s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:07:17 | 200 |    1.3039119s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:07:20 | 200 |    2.5633677s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:07:22 | 200 |    2.0212726s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:07:24 | 200 |    2.0500328s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:07:26 | 200 |    1.2565682s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:07:28 | 200 |     2.430914s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:07:29 | 200 |    1.2515108s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:07:30 | 200 |    978.7263ms |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:07:32 | 200 |    1.7650683s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:07:34 | 200 |    1.9327555s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:07:40 | 200 |    6.0175748s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:07:43 | 200 |    2.0638273s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:07:45 | 200 |    2.3817048s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:07:47 | 200 |    1.8265801s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:07:50 | 200 |    2.7840183s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:07:52 | 200 |    2.5581985s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:07:55 | 200 |    2.9346486s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:07:57 | 200 |    1.7972441s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:08:01 | 200 |    3.3496025s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:08:03 | 200 |    2.2017759s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:08:04 | 200 |    1.0316756s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:08:08 | 200 |    4.0578217s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:08:11 | 200 |    2.1500414s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:08:13 | 200 |    2.1330506s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:08:15 | 200 |    1.9131926s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:08:16 | 200 |    1.2188489s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:08:18 | 200 |     1.395978s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:11:55 | 200 |     4.852465s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:11:57 | 200 |    2.1969322s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:11:59 | 200 |    1.7027197s |       127.0.0.1 | POST     "/api/chat"
time=2025-10-21T20:12:00.219-04:00 level=WARN source=runner.go:136 msg="truncating input prompt" limit=4096 prompt=5893 keep=25 new=4096
[GIN] 2025/10/21 - 20:12:03 | 200 |    2.9670567s |       127.0.0.1 | POST     "/api/chat"
time=2025-10-21T20:12:03.450-04:00 level=WARN source=runner.go:136 msg="truncating input prompt" limit=4096 prompt=14477 keep=25 new=4096
[GIN] 2025/10/21 - 20:12:06 | 200 |    3.2503565s |       127.0.0.1 | POST     "/api/chat"
llama_model_loader: loaded meta data with 22 key-value pairs and 291 tensors from C:\Users\frost\.ollama\models\blobs\sha256-6a0746a1ec1aef3e7ec53868f220ff6e389f6f8ef87a01d77c96807de94ca2aa (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = llama
llama_model_loader: - kv   1:                               general.name str              = Meta-Llama-3-8B-Instruct
llama_model_loader: - kv   2:                          llama.block_count u32              = 32
llama_model_loader: - kv   3:                       llama.context_length u32              = 8192
llama_model_loader: - kv   4:                     llama.embedding_length u32              = 4096
llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 14336
llama_model_loader: - kv   6:                 llama.attention.head_count u32              = 32
llama_model_loader: - kv   7:              llama.attention.head_count_kv u32              = 8
llama_model_loader: - kv   8:                       llama.rope.freq_base f32              = 500000.000000
llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010
llama_model_loader: - kv  10:                          general.file_type u32              = 2
llama_model_loader: - kv  11:                           llama.vocab_size u32              = 128256
llama_model_loader: - kv  12:                 llama.rope.dimension_count u32              = 128
llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = llama-bpe
llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,128256]  = ["!", "\"", "#", "$", "%", "&", "'", ...
llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,128256]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,280147]  = ["Ġ Ġ", "Ġ ĠĠĠ", "ĠĠ ĠĠ", "...
llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 128000
llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 128009
llama_model_loader: - kv  20:                    tokenizer.chat_template str              = {% set loop_messages = messages %}{% ...
llama_model_loader: - kv  21:               general.quantization_version u32              = 2
llama_model_loader: - type  f32:   65 tensors
llama_model_loader: - type q4_0:  225 tensors
llama_model_loader: - type q6_K:    1 tensors
print_info: file format = GGUF V3 (latest)
print_info: file type   = Q4_0
print_info: file size   = 4.33 GiB (4.64 BPW) 
load: printing all EOG tokens:
load:   - 128001 ('<|end_of_text|>')
load:   - 128009 ('<|eot_id|>')
load: special tokens cache size = 256
load: token to piece cache size = 0.8000 MB
print_info: arch             = llama
print_info: vocab_only       = 1
print_info: model type       = ?B
print_info: model params     = 8.03 B
print_info: general.name     = Meta-Llama-3-8B-Instruct
print_info: vocab type       = BPE
print_info: n_vocab          = 128256
print_info: n_merges         = 280147
print_info: BOS token        = 128000 '<|begin_of_text|>'
print_info: EOS token        = 128009 '<|eot_id|>'
print_info: EOT token        = 128001 '<|end_of_text|>'
print_info: LF token         = 198 'Ċ'
print_info: EOG token        = 128001 '<|end_of_text|>'
print_info: EOG token        = 128009 '<|eot_id|>'
print_info: max token length = 256
llama_model_load: vocab only - skipping tensors
time=2025-10-21T20:18:07.254-04:00 level=INFO source=cpu_windows.go:139 msg=packages count=1
time=2025-10-21T20:18:07.254-04:00 level=INFO source=cpu_windows.go:155 msg="efficiency cores detected" maxEfficiencyClass=1
time=2025-10-21T20:18:07.254-04:00 level=INFO source=cpu_windows.go:186 msg="" package=0 cores=12 efficiency=4 threads=20
time=2025-10-21T20:18:07.255-04:00 level=INFO source=server.go:400 msg="starting runner" cmd="C:\\Users\\frost\\AppData\\Local\\Programs\\Ollama\\ollama.exe runner --model C:\\Users\\frost\\.ollama\\models\\blobs\\sha256-6a0746a1ec1aef3e7ec53868f220ff6e389f6f8ef87a01d77c96807de94ca2aa --port 55184"
time=2025-10-21T20:18:07.260-04:00 level=INFO source=cpu_windows.go:139 msg=packages count=1
time=2025-10-21T20:18:07.260-04:00 level=INFO source=cpu_windows.go:155 msg="efficiency cores detected" maxEfficiencyClass=1
time=2025-10-21T20:18:07.260-04:00 level=INFO source=cpu_windows.go:186 msg="" package=0 cores=12 efficiency=4 threads=20
time=2025-10-21T20:18:07.260-04:00 level=INFO source=server.go:505 msg="system memory" total="63.8 GiB" free="47.7 GiB" free_swap="48.1 GiB"
time=2025-10-21T20:18:07.261-04:00 level=INFO source=memory.go:36 msg="new model will fit in available VRAM across minimum required GPUs, loading" model=C:\Users\frost\.ollama\models\blobs\sha256-6a0746a1ec1aef3e7ec53868f220ff6e389f6f8ef87a01d77c96807de94ca2aa library=CUDA parallel=1 required="5.4 GiB" gpus=1
time=2025-10-21T20:18:07.261-04:00 level=INFO source=server.go:545 msg=offload library=CUDA layers.requested=-1 layers.model=33 layers.offload=33 layers.split=[33] memory.available="[10.7 GiB]" memory.gpu_overhead="0 B" memory.required.full="5.4 GiB" memory.required.partial="5.4 GiB" memory.required.kv="512.0 MiB" memory.required.allocations="[5.4 GiB]" memory.weights.total="4.1 GiB" memory.weights.repeating="3.7 GiB" memory.weights.nonrepeating="411.0 MiB" memory.graph.full="296.0 MiB" memory.graph.partial="677.5 MiB"
time=2025-10-21T20:18:07.305-04:00 level=INFO source=runner.go:893 msg="starting go runner"
load_backend: loaded CPU backend from C:\Users\frost\AppData\Local\Programs\Ollama\lib\ollama\ggml-cpu-alderlake.dll
ggml_cuda_init: GGML_CUDA_FORCE_MMQ:    no
ggml_cuda_init: GGML_CUDA_FORCE_CUBLAS: no
ggml_cuda_init: found 1 CUDA devices:
  Device 0: NVIDIA GeForce RTX 5070, compute capability 12.0, VMM: yes, ID: GPU-b28645fa-aaf6-86a1-ab1b-ef8d61df29f2
load_backend: loaded CUDA backend from C:\Users\frost\AppData\Local\Programs\Ollama\lib\ollama\cuda_v12\ggml-cuda.dll
time=2025-10-21T20:18:07.437-04:00 level=INFO source=ggml.go:104 msg=system CPU.0.SSE3=1 CPU.0.SSSE3=1 CPU.0.AVX=1 CPU.0.AVX_VNNI=1 CPU.0.AVX2=1 CPU.0.F16C=1 CPU.0.FMA=1 CPU.0.BMI2=1 CPU.0.LLAMAFILE=1 CPU.1.LLAMAFILE=1 CUDA.0.ARCHS=500,520,600,610,700,750,800,860,870,890,900,1200 CUDA.0.USE_GRAPHS=1 CUDA.0.PEER_MAX_BATCH_SIZE=128 compiler=cgo(clang)
time=2025-10-21T20:18:07.438-04:00 level=INFO source=runner.go:929 msg="Server listening on 127.0.0.1:55184"
time=2025-10-21T20:18:07.448-04:00 level=INFO source=runner.go:828 msg=load request="{Operation:commit LoraPath:[] Parallel:1 BatchSize:512 FlashAttention:false KvSize:4096 KvCacheType: NumThreads:8 GPULayers:33[ID:GPU-b28645fa-aaf6-86a1-ab1b-ef8d61df29f2 Layers:33(0..32)] MultiUserCache:false ProjectorPath: MainGPU:0 UseMmap:false}"
time=2025-10-21T20:18:07.449-04:00 level=INFO source=server.go:1272 msg="waiting for llama runner to start responding"
time=2025-10-21T20:18:07.449-04:00 level=INFO source=server.go:1306 msg="waiting for server to become available" status="llm server loading model"
ggml_backend_cuda_device_get_memory utilizing NVML memory reporting free: 11485843456 total: 12820938752
llama_model_load_from_file_impl: using device CUDA0 (NVIDIA GeForce RTX 5070) (0000:01:00.0) - 10953 MiB free
llama_model_loader: loaded meta data with 22 key-value pairs and 291 tensors from C:\Users\frost\.ollama\models\blobs\sha256-6a0746a1ec1aef3e7ec53868f220ff6e389f6f8ef87a01d77c96807de94ca2aa (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = llama
llama_model_loader: - kv   1:                               general.name str              = Meta-Llama-3-8B-Instruct
llama_model_loader: - kv   2:                          llama.block_count u32              = 32
llama_model_loader: - kv   3:                       llama.context_length u32              = 8192
llama_model_loader: - kv   4:                     llama.embedding_length u32              = 4096
llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 14336
llama_model_loader: - kv   6:                 llama.attention.head_count u32              = 32
llama_model_loader: - kv   7:              llama.attention.head_count_kv u32              = 8
llama_model_loader: - kv   8:                       llama.rope.freq_base f32              = 500000.000000
llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010
llama_model_loader: - kv  10:                          general.file_type u32              = 2
llama_model_loader: - kv  11:                           llama.vocab_size u32              = 128256
llama_model_loader: - kv  12:                 llama.rope.dimension_count u32              = 128
llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = llama-bpe
llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,128256]  = ["!", "\"", "#", "$", "%", "&", "'", ...
llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,128256]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,280147]  = ["Ġ Ġ", "Ġ ĠĠĠ", "ĠĠ ĠĠ", "...
llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 128000
llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 128009
llama_model_loader: - kv  20:                    tokenizer.chat_template str              = {% set loop_messages = messages %}{% ...
llama_model_loader: - kv  21:               general.quantization_version u32              = 2
llama_model_loader: - type  f32:   65 tensors
llama_model_loader: - type q4_0:  225 tensors
llama_model_loader: - type q6_K:    1 tensors
print_info: file format = GGUF V3 (latest)
print_info: file type   = Q4_0
print_info: file size   = 4.33 GiB (4.64 BPW) 
load: printing all EOG tokens:
load:   - 128001 ('<|end_of_text|>')
load:   - 128009 ('<|eot_id|>')
load: special tokens cache size = 256
load: token to piece cache size = 0.8000 MB
print_info: arch             = llama
print_info: vocab_only       = 0
print_info: n_ctx_train      = 8192
print_info: n_embd           = 4096
print_info: n_layer          = 32
print_info: n_head           = 32
print_info: n_head_kv        = 8
print_info: n_rot            = 128
print_info: n_swa            = 0
print_info: is_swa_any       = 0
print_info: n_embd_head_k    = 128
print_info: n_embd_head_v    = 128
print_info: n_gqa            = 4
print_info: n_embd_k_gqa     = 1024
print_info: n_embd_v_gqa     = 1024
print_info: f_norm_eps       = 0.0e+00
print_info: f_norm_rms_eps   = 1.0e-05
print_info: f_clamp_kqv      = 0.0e+00
print_info: f_max_alibi_bias = 0.0e+00
print_info: f_logit_scale    = 0.0e+00
print_info: f_attn_scale     = 0.0e+00
print_info: n_ff             = 14336
print_info: n_expert         = 0
print_info: n_expert_used    = 0
print_info: causal attn      = 1
print_info: pooling type     = 0
print_info: rope type        = 0
print_info: rope scaling     = linear
print_info: freq_base_train  = 500000.0
print_info: freq_scale_train = 1
print_info: n_ctx_orig_yarn  = 8192
print_info: rope_finetuned   = unknown
print_info: model type       = 8B
print_info: model params     = 8.03 B
print_info: general.name     = Meta-Llama-3-8B-Instruct
print_info: vocab type       = BPE
print_info: n_vocab          = 128256
print_info: n_merges         = 280147
print_info: BOS token        = 128000 '<|begin_of_text|>'
print_info: EOS token        = 128009 '<|eot_id|>'
print_info: EOT token        = 128001 '<|end_of_text|>'
print_info: LF token         = 198 'Ċ'
print_info: EOG token        = 128001 '<|end_of_text|>'
print_info: EOG token        = 128009 '<|eot_id|>'
print_info: max token length = 256
load_tensors: loading model tensors, this can take a while... (mmap = false)
load_tensors: offloading 32 repeating layers to GPU
load_tensors: offloading output layer to GPU
load_tensors: offloaded 33/33 layers to GPU
load_tensors:    CUDA_Host model buffer size =   281.81 MiB
load_tensors:        CUDA0 model buffer size =  4155.99 MiB
llama_init_from_model: model default pooling_type is [0], but [-1] was specified
llama_context: constructing llama_context
llama_context: n_seq_max     = 1
llama_context: n_ctx         = 4096
llama_context: n_ctx_per_seq = 4096
llama_context: n_batch       = 512
llama_context: n_ubatch      = 512
llama_context: causal_attn   = 1
llama_context: flash_attn    = disabled
llama_context: kv_unified    = false
llama_context: freq_base     = 500000.0
llama_context: freq_scale    = 1
llama_context: n_ctx_per_seq (4096) < n_ctx_train (8192) -- the full capacity of the model will not be utilized
llama_context:  CUDA_Host  output buffer size =     0.50 MiB
llama_kv_cache:      CUDA0 KV buffer size =   512.00 MiB
llama_kv_cache: size =  512.00 MiB (  4096 cells,  32 layers,  1/1 seqs), K (f16):  256.00 MiB, V (f16):  256.00 MiB
llama_context:      CUDA0 compute buffer size =   300.01 MiB
llama_context:  CUDA_Host compute buffer size =    20.01 MiB
llama_context: graph nodes  = 1158
llama_context: graph splits = 2
time=2025-10-21T20:18:09.485-04:00 level=INFO source=server.go:1310 msg="llama runner started in 2.23 seconds"
time=2025-10-21T20:18:09.485-04:00 level=INFO source=sched.go:482 msg="loaded runners" count=1
time=2025-10-21T20:18:09.485-04:00 level=INFO source=server.go:1272 msg="waiting for llama runner to start responding"
time=2025-10-21T20:18:09.485-04:00 level=INFO source=server.go:1310 msg="llama runner started in 2.23 seconds"
[GIN] 2025/10/21 - 20:18:14 | 200 |    7.8706555s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:18:17 | 200 |    2.9818946s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:18:20 | 200 |    2.4300864s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:18:22 | 200 |     1.788416s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:18:26 | 200 |    3.4305603s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:18:30 | 200 |    4.1612722s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:18:34 | 200 |    3.5362886s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:18:37 | 200 |    3.3354656s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:18:41 | 200 |    3.3372971s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:18:44 | 200 |    3.1973615s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:18:48 | 200 |    3.2000464s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:18:52 | 200 |    3.3548274s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:18:55 | 200 |    3.1636173s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:18:59 | 200 |    3.6428351s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:19:01 | 200 |    1.7702967s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:19:05 | 200 |    3.7246751s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:19:09 | 200 |    3.3430989s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:19:12 | 200 |    3.3543618s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:19:16 | 200 |    3.1909983s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:19:19 | 200 |    3.0214984s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:19:22 | 200 |    3.1287772s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:19:26 | 200 |     3.110616s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:19:29 | 200 |    3.1854571s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:19:32 | 200 |    2.8427477s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:19:35 | 200 |     2.824968s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:19:36 | 200 |    1.0452714s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:19:40 | 200 |    2.8243956s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:19:42 | 200 |    2.5414686s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:19:45 | 200 |    2.4701121s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:19:48 | 200 |    2.6111535s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:19:51 | 200 |    2.5621295s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:19:53 | 200 |    2.5239659s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:19:56 | 200 |    2.0556744s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:19:58 | 200 |    2.1291222s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:20:01 | 200 |    2.1727878s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:20:03 | 200 |    2.1904204s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:20:06 | 200 |    2.3164878s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:20:08 | 200 |    2.3794884s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:20:11 | 200 |    2.4750916s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:20:14 | 200 |    2.5976734s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:20:17 | 200 |    2.3959065s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:20:19 | 200 |    2.1380836s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:20:21 | 200 |    2.0995334s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:20:24 | 200 |    2.1108142s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:20:26 | 200 |     2.168901s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:20:29 | 200 |    2.2404107s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:20:31 | 200 |    2.1752164s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:20:33 | 200 |    2.0000977s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:20:36 | 200 |    2.0395873s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:20:38 | 200 |    2.1411189s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:20:41 | 200 |    2.2800445s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:20:43 | 200 |    2.2753557s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:20:45 | 200 |    1.9350393s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:20:47 | 200 |    2.0116097s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:20:50 | 200 |    2.0371515s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:20:52 | 200 |    1.9530055s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:20:54 | 200 |    2.1616697s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:20:57 | 200 |    2.0535059s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:20:59 | 200 |    1.9963786s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:21:01 | 200 |     2.050995s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:21:04 | 200 |    2.0388279s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:21:06 | 200 |    1.9618056s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:21:08 | 200 |    1.9676571s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:21:10 | 200 |    1.9736629s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:21:13 | 200 |    1.9658213s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:21:15 | 200 |    1.8834759s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:21:17 | 200 |    1.9219358s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:21:19 | 200 |    1.9200326s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:21:21 | 200 |    1.9015743s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:21:23 | 200 |    1.9448013s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:21:25 | 200 |    1.9553107s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:21:28 | 200 |    1.9335701s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:21:30 | 200 |    1.9301128s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:21:32 | 200 |    1.9707928s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:21:34 | 200 |    2.0165716s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:21:37 | 200 |    1.9067241s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:21:39 | 200 |    1.9822614s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:21:41 | 200 |    1.8992254s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:21:41 | 200 |            0s |       127.0.0.1 | HEAD     "/"
[GIN] 2025/10/21 - 20:21:41 | 200 |     48.4338ms |       127.0.0.1 | POST     "/api/create"
[GIN] 2025/10/21 - 20:21:43 | 200 |    2.1230914s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:21:46 | 200 |    2.1463303s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:21:48 | 200 |    1.9751864s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:21:51 | 200 |    2.3489478s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:21:53 | 200 |    1.9201841s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:21:55 | 200 |    2.1233615s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:21:57 | 200 |    2.0190062s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:21:59 | 200 |            0s |       127.0.0.1 | HEAD     "/"
[GIN] 2025/10/21 - 20:21:59 | 404 |       1.027ms |       127.0.0.1 | POST     "/api/show"
[GIN] 2025/10/21 - 20:22:00 | 200 |    2.0156235s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:22:00 | 200 |    397.4566ms |       127.0.0.1 | POST     "/api/pull"
[GIN] 2025/10/21 - 20:22:02 | 200 |    2.0940364s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:22:04 | 200 |    2.0797873s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:22:06 | 200 |    2.0205038s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:22:07 | 200 |    765.3845ms |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:22:10 | 200 |    2.3363721s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:22:12 | 200 |    2.0483383s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:22:14 | 200 |    1.8135573s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:22:17 | 200 |    1.8227592s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:22:19 | 200 |    2.0825632s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:22:21 | 200 |    2.1413361s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:22:24 | 200 |    2.2527888s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:22:26 | 200 |     2.240826s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:22:28 | 200 |            0s |       127.0.0.1 | HEAD     "/"
[GIN] 2025/10/21 - 20:22:28 | 200 |      2.1522ms |       127.0.0.1 | GET      "/api/tags"
[GIN] 2025/10/21 - 20:22:29 | 200 |     2.253116s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:22:31 | 200 |    2.2585327s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:22:34 | 200 |    2.1528569s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:22:36 | 200 |    2.2663555s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:22:38 | 200 |    2.0810074s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:22:41 | 200 |    2.2337781s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:22:43 | 200 |    2.1188882s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:22:46 | 200 |    2.1651595s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:22:48 | 200 |     2.255234s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:22:51 | 200 |    2.3437017s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:22:54 | 200 |    2.4292249s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:22:56 | 200 |    2.3623924s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:22:59 | 200 |    2.3754426s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:23:01 | 200 |    2.1407118s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:23:03 | 200 |    2.1205091s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:23:05 | 200 |    1.6553131s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:23:07 | 200 |    1.3998974s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:23:09 | 200 |     1.408207s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:23:10 | 200 |    1.4298194s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:23:12 | 200 |    1.4358516s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:23:14 | 200 |    1.3908333s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:23:15 | 200 |      1.45833s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:23:17 | 200 |    1.4462797s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:23:19 | 200 |    1.4653378s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:23:21 | 200 |    1.4808015s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:23:23 | 200 |    1.4234752s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:23:24 | 200 |    1.4044026s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:23:33 | 200 |    8.8859259s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:23:34 | 200 |    9.3619214s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:23:36 | 200 |    2.8803658s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:23:37 | 200 |    2.9202432s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:23:39 | 200 |    3.0236593s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:23:40 | 200 |    3.0359078s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:23:43 | 200 |    3.7209146s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:23:44 | 200 |    3.7558261s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:23:46 | 200 |    2.7770795s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:23:47 | 200 |    2.7335832s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:23:50 | 200 |    4.0202107s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:23:52 | 200 |    4.0033379s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:23:54 | 200 |    3.7125242s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:23:56 | 200 |    3.7845361s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:23:58 | 200 |    3.7034667s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:24:00 | 200 |    3.6200485s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:24:02 | 200 |    3.5342007s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:24:03 | 200 |    3.4494312s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:24:06 | 200 |    3.5105004s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:24:07 | 200 |    3.4906691s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:24:10 | 200 |    3.4976846s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:24:11 | 200 |    3.4997639s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:24:13 | 200 |    3.5840448s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:24:15 | 200 |    3.5486877s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:24:17 | 200 |    3.6482837s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:24:19 | 200 |    3.6872353s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:24:21 | 200 |     3.479171s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:24:22 | 200 |    3.4779914s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:24:25 | 200 |    3.4911987s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:24:26 | 200 |    3.5013438s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:24:28 | 200 |    3.4755453s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:24:30 | 200 |    3.4757516s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:24:32 | 200 |    3.5359119s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:24:34 | 200 |    3.5296014s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:24:36 | 200 |    3.4494374s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:24:37 | 200 |    3.4835508s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:24:40 | 200 |     3.449377s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:24:40 | 200 |            0s |       127.0.0.1 | HEAD     "/"
[GIN] 2025/10/21 - 20:24:40 | 500 |            0s |       127.0.0.1 | POST     "/api/show"
[GIN] 2025/10/21 - 20:24:41 | 200 |    3.4181401s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:24:43 | 200 |    3.5180618s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:24:45 | 200 |    3.4363752s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:24:47 | 200 |    3.4634621s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:24:49 | 200 |    3.5239507s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:24:51 | 200 |    3.4034146s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:24:52 | 200 |    3.4221211s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:24:55 | 200 |    3.4290281s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:24:56 | 200 |    3.4490277s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:24:58 | 200 |    3.4688557s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:25:00 | 200 |    3.4231373s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:25:01 | 200 |     2.650575s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:25:03 | 200 |    2.6934049s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:25:05 | 200 |    3.6147239s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:25:06 | 200 |    3.6444514s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:25:09 | 200 |     3.456725s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:25:10 | 200 |    3.4390237s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:25:12 | 200 |    3.4211106s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:25:14 | 200 |    3.4325179s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:25:16 | 200 |    3.4076821s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:25:18 | 200 |    3.4356618s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:25:20 | 200 |    3.4344569s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:25:21 | 200 |    3.4582664s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:25:24 | 200 |    3.5000781s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:25:25 | 200 |    3.6425521s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:25:28 | 200 |     3.573036s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:25:28 | 200 |            0s |       127.0.0.1 | HEAD     "/"
[GIN] 2025/10/21 - 20:25:29 | 200 |     75.7817ms |       127.0.0.1 | POST     "/api/show"
[GIN] 2025/10/21 - 20:25:29 | 200 |    3.3242738s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:25:31 | 200 |    3.3194825s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:25:32 | 200 |     3.328298s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:25:35 | 200 |    3.3097557s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:25:36 | 200 |    3.3308407s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:25:38 | 200 |    3.3048868s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:25:39 | 200 |    3.2973615s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:25:42 | 200 |    3.1959557s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:25:43 | 200 |    3.1567497s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:25:45 | 200 |     3.142511s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:25:46 | 200 |    3.1325577s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:25:48 | 200 |    3.1695632s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:25:50 | 200 |    3.1390119s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:25:52 | 200 |    3.1810771s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:25:53 | 200 |    3.1531599s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:25:55 | 200 |    3.1088383s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:25:56 | 200 |    3.1881235s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:25:59 | 200 |    3.0535747s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:26:00 | 200 |     3.133294s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:26:02 | 200 |     3.109777s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:26:03 | 200 |    3.1059892s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:26:05 | 200 |    2.7810674s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:26:06 | 200 |    2.8333767s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:26:08 | 200 |    2.8236593s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:26:09 | 200 |    2.8817476s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:26:11 | 200 |    2.8633216s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:26:13 | 200 |    2.8638245s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:26:15 | 200 |    2.8499438s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:26:16 | 200 |    2.8771393s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:26:18 | 200 |    2.8323172s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:26:19 | 200 |    2.8728855s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:26:21 | 200 |    2.8801178s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:26:22 | 200 |    2.8294804s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:26:24 | 200 |    2.4239468s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:26:25 | 200 |    2.3758021s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:26:27 | 200 |     2.934179s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:26:28 | 200 |     3.044629s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:26:30 | 200 |    2.8139268s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:26:31 | 200 |    2.8374573s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:26:33 | 200 |    2.8229115s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:26:34 | 200 |     2.829306s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:26:36 | 200 |    2.7834208s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:26:37 | 200 |    2.8241478s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:26:39 | 200 |    2.8102052s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:26:40 | 200 |    2.8212905s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:26:42 | 200 |    2.8063786s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:26:43 | 200 |    2.8217873s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:26:45 | 200 |    2.8151246s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:26:46 | 200 |    2.8342394s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:26:48 | 200 |    2.8411838s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:26:50 | 200 |    2.9512047s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:26:52 | 200 |    2.9512191s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:26:53 | 200 |    3.1757745s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:26:55 | 200 |    3.1504541s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:26:56 | 200 |    2.9680262s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:26:58 | 200 |    2.9429166s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:27:00 | 200 |    3.1067358s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:27:02 | 200 |     3.081588s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:27:03 | 200 |    2.8939309s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:27:05 | 200 |    2.8692007s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:27:06 | 200 |    2.8632935s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:27:08 | 200 |    2.8690619s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:27:09 | 200 |    2.8825016s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:27:11 | 200 |    2.9407833s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:27:12 | 200 |    2.9519977s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:27:14 | 200 |    2.8075088s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:27:15 | 200 |    2.9036689s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:27:17 | 200 |    2.9232093s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:27:19 | 200 |    2.9350507s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:27:21 | 200 |    2.9007783s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:27:22 | 200 |    2.9453545s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:27:24 | 200 |     2.924268s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:27:25 | 200 |    2.9192446s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:27:27 | 200 |    2.8833003s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:27:28 | 200 |    2.9338962s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:27:30 | 200 |    2.9178425s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:27:32 | 200 |    3.2406616s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:27:34 | 200 |    3.3848684s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:27:36 | 200 |     3.572249s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:27:38 | 200 |    3.6073338s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:27:39 | 200 |    3.4537888s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:27:41 | 200 |    3.3490531s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:27:43 | 200 |    3.4148737s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:27:45 | 200 |    3.4104411s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:27:46 | 200 |    2.7226894s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:27:48 | 200 |    2.6972112s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:27:50 | 200 |    3.5244302s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:27:52 | 200 |    3.5234476s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:27:53 | 200 |    3.3888619s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:27:55 | 200 |    3.3652399s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:27:56 | 200 |    2.7020724s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:27:58 | 200 |    2.6425184s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:28:00 | 200 |    3.5398058s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:28:02 | 200 |    3.5138783s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:28:04 | 200 |    3.4072292s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:28:06 | 200 |    3.4635691s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:28:07 | 200 |    3.0407103s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:28:09 | 200 |    3.0329899s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:28:10 | 200 |    2.9316474s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:28:12 | 200 |    2.9896727s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:28:13 | 200 |     2.892936s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:28:16 | 200 |     3.093205s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:28:17 | 200 |    3.1386487s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:28:19 | 200 |    2.9919221s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:28:20 | 200 |    3.0279215s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:28:21 | 200 |    2.0095069s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:28:22 | 200 |    2.0550425s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:28:24 | 200 |    2.1776619s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:28:25 | 200 |    2.1606052s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:28:26 | 200 |    2.1412754s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:28:27 | 200 |    2.1632811s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:28:29 | 200 |    2.2056152s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:28:30 | 200 |    2.1581547s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:28:31 | 200 |    2.1417782s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:28:32 | 200 |    2.1883886s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:28:33 | 200 |    2.1962816s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:28:35 | 200 |    2.2154304s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:28:36 | 200 |    2.1777037s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:28:37 | 200 |    2.1747623s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:28:38 | 200 |    2.1849443s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:28:40 | 200 |    2.1880126s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:28:41 | 200 |     2.179019s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:28:42 | 200 |    2.1934032s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:28:43 | 200 |    2.2100172s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:28:44 | 200 |    2.1976541s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:28:46 | 200 |    2.1968161s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:28:47 | 200 |    2.1813219s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:28:48 | 200 |    2.1816073s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:28:49 | 200 |    2.2444802s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:28:51 | 200 |     2.194113s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:28:52 | 200 |    2.1751087s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:28:53 | 200 |    2.2093522s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:28:54 | 200 |    2.2002218s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:28:55 | 200 |     2.188777s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:28:57 | 200 |     2.198136s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:28:58 | 200 |    2.0066516s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:28:59 | 200 |    2.1517858s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:29:00 | 200 |    2.1958031s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:29:01 | 200 |    2.1942413s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:29:03 | 200 |    2.2272539s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:29:04 | 200 |    2.2210784s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:29:05 | 200 |    2.2385766s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:29:06 | 200 |    2.2098223s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:29:08 | 200 |     2.170679s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:29:09 | 200 |    2.3035658s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:29:10 | 200 |    2.3186229s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:29:11 | 200 |    2.2229873s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:29:13 | 200 |    2.2067616s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:29:14 | 200 |    2.1996672s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:29:15 | 200 |    2.2004097s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:29:16 | 200 |    2.1723458s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:29:17 | 200 |    2.1422789s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:29:19 | 200 |    2.1355286s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:29:20 | 200 |    2.0282958s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:29:22 | 200 |    2.7088325s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:29:23 | 200 |    2.6967028s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:29:24 | 200 |     2.038643s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:29:25 | 200 |    2.0233328s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:29:26 | 200 |    1.9771263s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:29:26 | 200 |            0s |       127.0.0.1 | HEAD     "/"
[GIN] 2025/10/21 - 20:29:27 | 200 |    2.0252408s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:29:28 | 200 |    2.0303542s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:29:29 | 200 |    1.9992533s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:29:31 | 200 |    2.0010409s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:29:32 | 200 |     2.200453s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:29:33 | 200 |    2.2017093s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:29:34 | 200 |    2.2299974s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:29:36 | 200 |    2.2596722s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:29:37 | 200 |    2.2214189s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:29:38 | 200 |    2.2671245s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:29:39 | 200 |    2.2343904s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:29:41 | 200 |    2.2176337s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:29:42 | 200 |    2.2792003s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:29:43 | 200 |    2.2634274s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:29:44 | 200 |    2.2953939s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:29:46 | 200 |    2.2731612s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:29:47 | 200 |    2.3117163s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:29:48 | 200 |    2.3061521s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:29:50 | 200 |    2.2971559s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:29:51 | 200 |    2.2748353s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:29:52 | 200 |    2.2448977s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:29:53 | 200 |    2.3461084s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:29:55 | 200 |    2.4150851s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:29:56 | 200 |    2.4320562s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:29:57 | 200 |    2.4004096s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:29:59 | 200 |    2.3533888s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:30:00 | 200 |    2.4019312s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:30:02 | 200 |    2.7295297s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:30:03 | 200 |    2.6670412s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:30:05 | 200 |    3.1788967s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:30:06 | 200 |       3.1335s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:30:08 | 200 |    2.3474712s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:30:09 | 200 |    2.3813191s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:30:10 | 200 |    2.3148699s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:30:12 | 200 |    2.2997566s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:30:13 | 200 |    2.3256795s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:30:14 | 200 |    2.3307898s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:30:15 | 200 |    2.3794739s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:30:17 | 200 |    2.3281701s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:30:18 | 200 |    2.3148565s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:30:19 | 200 |    2.3201384s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:30:21 | 200 |    2.3682831s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:30:22 | 200 |    2.3193726s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:30:23 | 200 |    2.3434236s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:30:24 | 200 |    2.3655594s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:30:26 | 200 |    2.3877104s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:30:27 | 200 |    2.2463903s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:30:28 | 200 |    2.2860873s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:30:29 | 200 |    2.1000177s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:30:31 | 200 |    2.1195545s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:30:32 | 200 |     2.245906s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:30:33 | 200 |    2.2141263s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:30:34 | 200 |    2.2368961s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:30:36 | 200 |    2.2962756s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:30:37 | 200 |    2.2255331s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:30:38 | 200 |    2.2207798s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:30:39 | 200 |    2.2283781s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:30:41 | 200 |    2.1915148s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:30:42 | 200 |    2.7961385s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:30:44 | 200 |    2.7925423s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:30:45 | 200 |    2.2303131s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:30:46 | 200 |    2.5167196s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:30:48 | 200 |    3.1910228s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:30:50 | 200 |    3.1985306s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:30:51 | 200 |    2.4084256s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:30:52 | 200 |    2.1139509s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:30:54 | 200 |    2.4669162s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:30:55 | 200 |    2.4529312s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:30:56 | 200 |    2.3716386s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:30:57 | 200 |    2.3365616s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:30:58 | 200 |            0s |       127.0.0.1 | HEAD     "/"
[GIN] 2025/10/21 - 20:30:58 | 200 |     75.6629ms |       127.0.0.1 | POST     "/api/create"
[GIN] 2025/10/21 - 20:30:59 | 200 |    2.3067423s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:31:00 | 200 |    2.2783624s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:31:01 | 200 |     2.296769s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:31:03 | 200 |    2.2977817s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:31:04 | 200 |    2.3459259s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:31:05 | 200 |    2.3067983s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:31:06 | 200 |    2.2877829s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:31:08 | 200 |    2.3138195s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:31:09 | 200 |    2.3257315s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:31:10 | 200 |    2.3225517s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:31:12 | 200 |    2.3294949s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:31:13 | 200 |     2.343954s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:31:14 | 200 |    2.3244282s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:31:15 | 200 |    2.3145857s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:31:17 | 200 |    2.3307143s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:31:18 | 200 |    2.3471618s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:31:19 | 200 |    2.3298031s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:31:20 | 200 |            0s |       127.0.0.1 | HEAD     "/"
[GIN] 2025/10/21 - 20:31:20 | 200 |     75.1747ms |       127.0.0.1 | POST     "/api/show"
[GIN] 2025/10/21 - 20:31:21 | 200 |    2.3466631s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:31:22 | 200 |    2.2839655s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:31:23 | 200 |    2.3044167s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:31:24 | 200 |     2.325828s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:31:26 | 200 |    2.3526756s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:31:27 | 200 |    2.3609254s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:31:28 | 200 |    2.3617431s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:31:30 | 200 |    2.3323074s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:31:31 | 200 |    2.3332117s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:31:32 | 200 |    2.3188361s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:31:34 | 200 |    2.4931218s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:31:35 | 200 |    2.5128682s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:31:36 | 200 |    2.5336776s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:31:38 | 200 |    2.4236136s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:31:39 | 200 |    2.2349002s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:31:40 | 200 |    2.1251971s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:31:42 | 200 |    2.2786072s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:31:43 | 200 |     2.271866s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:31:44 | 200 |     2.269442s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:31:45 | 200 |    2.4340849s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:31:47 | 200 |    2.2160435s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:31:48 | 200 |    2.1496603s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:31:49 | 200 |    2.3374381s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:31:50 | 200 |    2.2830608s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:31:52 | 200 |    2.2922028s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:31:53 | 200 |    2.2699262s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:31:54 | 200 |    2.2552752s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:31:55 | 200 |    2.2872998s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:31:57 | 200 |    2.2800114s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:31:58 | 200 |    2.3179081s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:31:59 | 200 |    2.3343718s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:32:01 | 200 |    2.3359213s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:32:02 | 200 |    2.3601072s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:32:03 | 200 |    2.3660986s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:32:05 | 200 |    2.3761779s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:32:06 | 200 |    2.3385967s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:32:07 | 200 |    2.3103401s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:32:08 | 200 |    2.2351511s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:32:10 | 200 |    2.2716478s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:32:11 | 200 |    2.2770863s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:32:12 | 200 |    2.2535206s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:32:13 | 200 |    2.3631895s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:32:15 | 200 |    2.3775296s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:32:16 | 200 |    2.4720703s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:32:18 | 200 |    2.4095292s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:32:19 | 200 |    2.3850062s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:32:20 | 200 |    2.3773852s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:32:22 | 200 |    2.4826684s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:32:23 | 200 |    2.4638451s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:32:24 | 200 |    2.4119544s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:32:26 | 200 |     2.411943s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:32:27 | 200 |    2.4704154s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:32:28 | 200 |    2.5002796s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:32:30 | 200 |     2.472691s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:32:31 | 200 |    2.4068345s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:32:32 | 200 |    2.3804815s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:32:34 | 200 |    2.4259402s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:32:35 | 200 |    2.4077818s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:32:37 | 200 |    2.4092247s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:32:38 | 200 |    2.3732711s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:32:39 | 200 |    2.3543807s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:32:40 | 200 |    2.4762983s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:32:42 | 200 |    2.5023618s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:32:43 | 200 |    2.5159536s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:32:45 | 200 |    2.5154936s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:32:46 | 200 |    2.3093852s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:32:47 | 200 |    2.2316962s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:32:49 | 200 |    2.4472208s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:32:50 | 200 |     2.487552s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:32:51 | 200 |    2.5411032s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:32:53 | 200 |    2.4896763s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:32:54 | 200 |    2.4674691s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:32:56 | 200 |    2.4621439s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:32:57 | 200 |    2.4538319s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:32:58 | 200 |    2.4314689s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:33:00 | 200 |    2.4297093s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:33:01 | 200 |    2.4154577s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:33:02 | 200 |    2.3929028s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:33:04 | 200 |    2.3691028s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:33:05 | 200 |    2.4268707s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:33:06 | 200 |    2.4309629s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:33:08 | 200 |    2.3229019s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:33:09 | 200 |    2.3921059s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:33:10 | 200 |     2.432556s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:33:12 | 200 |    2.4446698s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:33:13 | 200 |    2.3232615s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:33:14 | 200 |    2.3145073s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:33:16 | 200 |     2.466974s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:33:17 | 200 |     2.490945s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:33:18 | 200 |    2.4943076s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:33:20 | 200 |    2.4847981s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:33:21 | 200 |    2.4575609s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:33:23 | 200 |     2.448909s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:33:24 | 200 |    2.4891312s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:33:26 | 200 |    2.4972476s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:33:27 | 200 |    2.4793396s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:33:28 | 200 |    2.4626005s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:33:30 | 200 |    2.7758806s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:33:31 | 200 |    2.7599517s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:33:33 | 200 |    2.7239915s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:33:34 | 200 |    2.7233357s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:33:36 | 200 |    2.7381554s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:33:37 | 200 |    2.7023112s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:33:39 | 200 |    2.6916265s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:33:40 | 200 |    2.7475558s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:33:42 | 200 |     2.501008s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:33:43 | 200 |    2.4973273s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:33:45 | 200 |    2.4618922s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:33:46 | 200 |    2.4598779s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:33:47 | 200 |    2.4386409s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:33:49 | 200 |    2.4591948s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:33:50 | 200 |     2.485938s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:33:52 | 200 |    2.4778837s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:33:53 | 200 |    2.4997516s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:33:54 | 200 |    2.3991474s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:33:55 | 200 |    2.2806176s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:33:57 | 200 |    2.2994716s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:33:58 | 200 |    2.3737244s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:34:00 | 200 |     2.396291s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:34:01 | 200 |    2.5343455s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:34:02 | 200 |    2.5429638s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:34:04 | 200 |    2.2973704s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:34:05 | 200 |    2.3010608s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:34:06 | 200 |    2.3990457s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:34:08 | 200 |    2.3689004s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:34:09 | 200 |    2.5480166s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:34:11 | 200 |     2.552025s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:34:12 | 200 |    2.8080079s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:34:14 | 200 |    2.8666109s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:34:15 | 200 |    2.8566237s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:34:17 | 200 |    2.8496881s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:34:19 | 200 |    2.8334665s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:34:20 | 200 |    2.8848693s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:34:22 | 200 |    2.9030469s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:34:23 | 200 |    2.8562888s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:34:25 | 200 |     2.878128s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:34:26 | 200 |    2.8834707s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:34:28 | 200 |     2.837964s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:34:30 | 200 |    2.8489719s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:34:31 | 200 |    2.7973676s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:34:33 | 200 |    2.7894647s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:34:34 | 200 |    2.7440417s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:34:36 | 200 |    2.7565192s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:34:37 | 200 |    2.7764925s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:34:39 | 200 |    2.7261152s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:34:40 | 200 |    2.7845015s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:34:42 | 200 |    2.8053172s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:34:43 | 200 |    2.6969754s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:34:45 | 200 |    2.8261162s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:34:47 | 200 |    2.8093939s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:34:48 | 200 |     2.800076s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:34:50 | 200 |    2.7631215s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:34:51 | 200 |    2.7413582s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:34:53 | 200 |     2.740485s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:34:54 | 200 |     2.808386s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:34:56 | 200 |     2.813526s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:34:57 | 200 |    2.7658247s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:34:59 | 200 |    2.7999052s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:35:00 | 200 |    2.8061527s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:35:02 | 200 |    2.7906958s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:35:03 | 200 |    2.7358133s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:35:05 | 200 |    2.7900308s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:35:06 | 200 |    2.7958785s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:35:08 | 200 |     2.771651s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:35:10 | 200 |    2.7116752s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:35:11 | 200 |    2.7217065s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:35:13 | 200 |    2.8038681s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:35:14 | 200 |    2.7793462s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:35:16 | 200 |    2.7937797s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:35:17 | 200 |    2.7835822s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:35:19 | 200 |     2.792344s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:35:20 | 200 |    2.8181571s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:35:22 | 200 |    2.8099946s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:35:23 | 200 |    2.8097983s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:35:25 | 200 |    2.8028636s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:35:26 | 200 |    2.7851424s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:35:28 | 200 |      2.75551s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:35:30 | 200 |    2.6766991s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:35:31 | 200 |    2.8013198s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:35:33 | 200 |    2.8054461s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:35:34 | 200 |    2.7453659s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:35:36 | 200 |    2.7928545s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:35:37 | 200 |    2.8228849s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:35:43 | 200 |    7.6739679s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:35:45 | 200 |    8.7188825s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:35:46 | 200 |     8.729425s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:35:49 | 200 |    5.5703615s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:35:50 | 200 |    5.5935966s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:35:52 | 200 |      5.52805s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:35:54 | 200 |     4.692002s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:35:55 | 200 |    4.7182091s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:35:57 | 200 |    4.7476453s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:35:58 | 200 |    4.0012062s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:36:00 | 200 |     4.378106s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:36:02 | 200 |    4.3877607s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:36:04 | 200 |    5.6170803s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:36:05 | 200 |    4.8492538s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:36:07 | 200 |    4.8768289s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:36:09 | 200 |    4.2991883s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:36:11 | 200 |    5.4395824s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:36:12 | 200 |    5.4442586s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:36:15 | 200 |    5.6981311s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:36:17 | 200 |    5.5537736s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:36:18 | 200 |    5.5203487s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:36:20 | 200 |    5.3676098s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:36:22 | 200 |     5.346713s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:36:24 | 200 |    5.4089788s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:36:26 | 200 |    5.0505403s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:36:28 | 200 |     5.077187s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:36:29 | 200 |    5.0571161s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:36:32 | 200 |    5.8866465s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:36:33 | 200 |    4.9983942s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:36:34 | 200 |    4.9924151s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:36:36 | 200 |    4.2449362s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:36:39 | 200 |    5.3837171s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:36:40 | 200 |    5.4347951s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:36:41 | 200 |    4.6860521s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:36:43 | 200 |    3.7558074s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:36:44 | 200 |    3.7285344s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:36:47 | 200 |    5.0653179s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:36:48 | 200 |      5.05405s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:36:50 | 200 |    5.1531009s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:36:52 | 200 |    5.1996887s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:36:53 | 200 |    5.1608593s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:36:55 | 200 |    5.0969929s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:36:57 | 200 |    5.1328712s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:36:59 | 200 |    5.1240294s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:37:00 | 200 |    5.1259832s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:37:02 | 200 |     4.711143s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:37:04 | 200 |    4.7214755s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:37:05 | 200 |    4.7039339s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:37:08 | 200 |    4.8364671s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:37:09 | 200 |    4.8593724s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:37:10 | 200 |    4.8717528s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:37:13 | 200 |    4.9105012s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:37:14 | 200 |    4.9474689s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:37:16 | 200 |    4.9538947s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:37:18 | 200 |    4.9603887s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:37:19 | 200 |    4.9621386s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:37:21 | 200 |    4.9579848s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:37:23 | 200 |    4.7062961s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:37:24 | 200 |    4.7627331s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:37:26 | 200 |    4.7569391s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:37:28 | 200 |    4.7098871s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:37:29 | 200 |    4.6543701s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:37:31 | 200 |    4.6465512s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:37:33 | 200 |    4.5901366s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:37:34 | 200 |    4.6323535s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:37:36 | 200 |    4.6904157s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:37:38 | 200 |    4.6829142s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:37:39 | 200 |     4.677068s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:37:41 | 200 |    4.6920924s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:37:43 | 200 |    4.6569732s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:37:44 | 200 |    4.6713749s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:37:45 | 200 |    4.6628088s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:37:47 | 200 |    4.6229233s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:37:49 | 200 |     4.659253s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:37:50 | 200 |    4.6544599s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:37:52 | 200 |    4.6300769s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:37:54 | 200 |    4.6971881s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:37:55 | 200 |     4.630745s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:37:57 | 200 |    4.6995174s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:37:59 | 200 |    4.6785192s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:38:00 | 200 |    4.7054258s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:38:02 | 200 |    4.6037789s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:38:04 | 200 |    4.7087456s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:38:05 | 200 |    4.6887199s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:38:07 | 200 |    4.6371153s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:38:09 | 200 |     4.605916s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:38:10 | 200 |    4.6058768s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:38:11 | 200 |    3.8292068s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:38:13 | 200 |     3.851179s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:38:14 | 200 |    3.8948802s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:38:17 | 200 |    4.9017804s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:38:18 | 200 |    4.8884527s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:38:19 | 200 |    4.8284018s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:38:21 | 200 |    4.6331687s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:38:23 | 200 |    4.6143011s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:38:24 | 200 |    4.6381905s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:38:26 | 200 |    4.6034079s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:38:28 | 200 |    4.6504021s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:38:29 | 200 |    4.6073718s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:38:31 | 200 |    4.6351857s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:38:33 | 200 |    4.6520267s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:38:34 | 200 |    4.6596369s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:38:36 | 200 |    4.6211767s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:38:37 | 200 |    4.6695762s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:38:39 | 200 |    4.6690254s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:38:41 | 200 |    4.6973742s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:38:42 | 200 |    4.6814317s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:38:44 | 200 |    4.6319534s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:38:46 | 200 |    4.5114325s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:38:47 | 200 |    4.5401103s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:38:49 | 200 |    4.5276097s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:38:51 | 200 |    4.5445694s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:38:52 | 200 |    4.5539479s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:38:53 | 200 |    4.5063468s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:38:55 | 200 |    4.4836504s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:38:57 | 200 |    4.4816984s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:38:58 | 200 |    4.4962101s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:39:00 | 200 |    4.4776502s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:39:01 | 200 |    4.4342053s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:39:03 | 200 |    4.4169071s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:39:05 | 200 |    4.3266342s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:39:06 | 200 |    4.3229441s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:39:07 | 200 |    4.3240896s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:39:09 | 200 |    4.4076161s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:39:11 | 200 |    4.4290122s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:39:12 | 200 |    4.4010463s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:39:14 | 200 |    4.4161606s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:39:15 | 200 |    4.4136073s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:39:17 | 200 |    4.4220574s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:39:19 | 200 |    4.4186924s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:39:20 | 200 |    4.4861878s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:39:22 | 200 |    4.5214776s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:39:23 | 200 |    4.3383983s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:39:25 | 200 |    4.4584386s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:39:26 | 200 |    4.4291703s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:39:28 | 200 |    4.4404223s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:39:30 | 200 |    4.4262474s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:39:31 | 200 |    4.4450505s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:39:33 | 200 |    4.4805218s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:39:34 | 200 |    4.4859913s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:39:36 | 200 |    4.4951362s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:39:38 | 200 |    4.4952536s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:39:39 | 200 |    4.4351352s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:39:41 | 200 |    4.4409701s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:39:42 | 200 |     4.448574s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:39:44 | 200 |    4.3843642s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:39:45 | 200 |    4.4193401s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:39:47 | 200 |    4.3933127s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:39:48 | 200 |    4.3951391s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:39:50 | 200 |    4.3505816s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:39:52 | 200 |    4.3914461s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:39:53 | 200 |     4.428337s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:39:55 | 200 |    4.4177264s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:39:56 | 200 |    4.3148909s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:39:58 | 200 |    4.3546329s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:39:59 | 200 |    4.4545453s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:40:01 | 200 |    4.3940996s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:40:02 | 200 |    4.4137554s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:40:04 | 200 |    4.3867633s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:40:06 | 200 |    4.4171244s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:40:07 | 200 |    4.4455683s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:40:09 | 200 |    4.4279477s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:40:10 | 200 |    4.4608956s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:40:12 | 200 |    4.4135355s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:40:13 | 200 |    4.3973823s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:40:15 | 200 |    4.4612928s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:40:16 | 200 |    4.4622225s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:40:18 | 200 |    4.4433606s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:40:20 | 200 |    4.4405557s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:40:21 | 200 |    4.4267473s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:40:23 | 200 |    4.3823782s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:40:24 | 200 |    3.9560365s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:40:25 | 200 |    3.9830421s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:40:27 | 200 |    3.9615644s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:40:28 | 200 |    4.1590366s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:40:30 | 200 |    4.1423251s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:40:32 | 200 |    4.3508802s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:40:33 | 200 |    4.5856822s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:40:35 | 200 |      4.52349s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:40:36 | 200 |    4.2925047s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:40:38 | 200 |    4.3621483s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:40:39 | 200 |    4.2694521s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:40:41 | 200 |    4.7160062s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:40:43 | 200 |    4.7041147s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:40:44 | 200 |    4.6519347s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:40:46 | 200 |    4.6918936s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:40:48 | 200 |    4.6620989s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:40:49 | 200 |    4.6481148s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:40:51 | 200 |    4.6519736s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:40:53 | 200 |    4.7117345s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:40:54 | 200 |    4.7154076s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:40:56 | 200 |    4.2428353s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:40:57 | 200 |    4.3006295s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:40:59 | 200 |    4.2963709s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:41:00 | 200 |    4.2711495s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:41:02 | 200 |     4.307393s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:41:03 | 200 |    4.3390282s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:41:05 | 200 |    4.3254019s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:41:07 | 200 |    4.3099367s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:41:08 | 200 |    4.2675926s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:41:09 | 200 |    4.2213368s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:41:11 | 200 |     4.169063s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:41:12 | 200 |    4.2044129s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:41:14 | 200 |    4.1981491s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:41:15 | 200 |    4.2113024s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:41:17 | 200 |    4.1911597s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:41:18 | 200 |    4.2179774s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:41:20 | 200 |    4.2190041s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:41:21 | 200 |     4.205322s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:41:23 | 200 |     4.204609s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:41:24 | 200 |    4.1689768s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:41:26 | 200 |    4.2275209s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:41:27 | 200 |    4.2283576s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:41:29 | 200 |    4.0141427s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:41:30 | 200 |    4.0930688s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:41:32 | 200 |    4.0853019s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:41:33 | 200 |    4.1409258s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:41:34 | 200 |    4.0912357s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:41:36 | 200 |    4.1451014s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:41:38 | 200 |    4.2044037s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:41:39 | 200 |    4.1894207s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:41:40 | 200 |    4.2494633s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:41:42 | 200 |    4.2268485s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:41:43 | 200 |    4.2240168s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:41:45 | 200 |    4.2147761s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:41:46 | 200 |    4.1647277s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:41:48 | 200 |    4.1748541s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:41:49 | 200 |    4.1658767s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:41:51 | 200 |      4.19143s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:41:52 | 200 |    4.1798304s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:41:54 | 200 |    4.1791319s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:41:55 | 200 |    4.1472115s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:41:57 | 200 |    4.2013968s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:41:58 | 200 |     4.171642s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:42:00 | 200 |    4.1640597s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:42:01 | 200 |    4.1634654s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:42:03 | 200 |    4.1595354s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:42:04 | 200 |    4.1155743s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:42:05 | 200 |    4.1748997s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:42:07 | 200 |    4.1512262s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:42:08 | 200 |    4.1736922s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:42:10 | 200 |    4.1641705s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:42:11 | 200 |    4.1811974s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:42:13 | 200 |    4.1656758s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:42:14 | 200 |    4.1587581s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:42:16 | 200 |    4.1748884s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:42:17 | 200 |    4.1689427s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:42:19 | 200 |    4.1740687s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:42:20 | 200 |    4.1851597s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:42:22 | 200 |    4.2190132s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:42:23 | 200 |    4.2193091s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:42:25 | 200 |    4.2319931s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:42:26 | 200 |    4.2889328s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:42:28 | 200 |      4.20157s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:42:29 | 200 |    4.2248331s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:42:31 | 200 |    4.2466614s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:42:32 | 200 |    4.2490411s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:42:34 | 200 |    4.2816817s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:42:36 | 200 |    4.3931934s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:42:37 | 200 |    4.4558642s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:42:38 | 200 |    4.4673228s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:42:40 | 200 |      4.50198s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:42:42 | 200 |    4.4333992s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:42:43 | 200 |    4.4401749s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:42:45 | 200 |    4.4886062s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:42:46 | 200 |    4.4135738s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:42:48 | 200 |    4.4432259s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:42:50 | 200 |    4.4264954s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:42:51 | 200 |    4.4308752s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:42:53 | 200 |    4.4104865s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:42:54 | 200 |    4.4167889s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:42:56 | 200 |    4.3822361s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:42:57 | 200 |    4.4248917s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:42:59 | 200 |    4.3857617s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:43:00 | 200 |     4.325701s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:43:02 | 200 |    4.3730338s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:43:04 | 200 |    4.3021707s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:43:05 | 200 |    4.3714447s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:43:06 | 200 |    4.3597044s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:43:08 | 200 |    4.4180473s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:43:10 | 200 |    4.4261408s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:43:11 | 200 |    4.4103378s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:43:13 | 200 |    4.4156732s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:43:14 | 200 |    4.4217928s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:43:16 | 200 |    4.4377059s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:43:18 | 200 |    4.4136073s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:43:19 | 200 |    4.4126037s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:43:20 | 200 |    4.3903153s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:43:22 | 200 |    4.4162606s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:43:24 | 200 |    4.3697053s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:43:25 | 200 |    4.3982443s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:43:27 | 200 |    4.4006048s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:43:28 | 200 |    4.3850234s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:43:30 | 200 |    4.3798956s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:43:32 | 200 |    4.3736593s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:43:33 | 200 |    4.3826546s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:43:34 | 200 |    4.3400192s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:43:36 | 200 |    4.3498719s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:43:38 | 200 |    4.4086549s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:43:39 | 200 |    4.4014607s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:43:41 | 200 |    4.3567153s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:43:42 | 200 |    4.3787215s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:43:44 | 200 |    4.4029961s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:43:45 | 200 |    4.3558755s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:43:47 | 200 |    4.2811904s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:43:48 | 200 |    4.5048118s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:43:50 | 200 |    4.5227405s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:43:52 | 200 |      4.52032s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:43:53 | 200 |    4.2374727s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:43:54 | 200 |    4.0954467s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:43:56 | 200 |    4.0703458s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:43:57 | 200 |    4.1542592s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:43:59 | 200 |    4.1938557s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:44:00 | 200 |    4.1449576s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:44:02 | 200 |    4.1842164s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:44:04 | 200 |    4.4037079s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:44:05 | 200 |    4.3923493s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:44:06 | 200 |    4.3407614s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:44:08 | 200 |     4.388447s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:44:10 | 200 |    4.4189745s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:44:11 | 200 |    4.4170365s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:44:13 | 200 |    4.4302278s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:44:14 | 200 |    4.3789578s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:44:16 | 200 |    4.4011156s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:44:18 | 200 |      4.36485s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:44:19 | 200 |    4.3623183s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:44:20 | 200 |    4.3776038s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:44:22 | 200 |    4.3863551s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:44:24 | 200 |    4.3662535s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:44:25 | 200 |    4.3866553s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:44:27 | 200 |    4.3762351s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:44:28 | 200 |    4.3741408s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:44:30 | 200 |    4.3665217s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:44:31 | 200 |    4.4195494s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:44:33 | 200 |    4.4292216s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:44:34 | 200 |    4.3854631s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:44:36 | 200 |    4.3803018s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:44:38 | 200 |    4.3489436s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:44:39 | 200 |    4.3579116s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:44:40 | 200 |    4.0920696s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:44:42 | 200 |     4.110368s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:44:43 | 200 |    4.1110506s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:44:45 | 200 |    4.0807565s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:44:46 | 200 |    4.1023588s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:44:48 | 200 |    4.1190691s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:44:49 | 200 |    4.1402316s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:44:51 | 200 |    4.1444478s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:44:52 | 200 |    4.1682634s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:44:54 | 200 |    4.1385667s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:44:55 | 200 |    4.1228055s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:44:56 | 200 |    4.1649339s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:44:58 | 200 |    4.1729921s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:44:59 | 200 |    4.1519591s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:45:01 | 200 |    4.1040974s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:45:02 | 200 |    4.1303828s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:45:04 | 200 |    4.1263663s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:45:05 | 200 |     4.117984s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:45:07 | 200 |    4.1239075s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:45:08 | 200 |    4.1244109s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:45:10 | 200 |    4.1410718s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:45:11 | 200 |    4.1087986s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:45:13 | 200 |    4.1068786s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:45:14 | 200 |    4.0875857s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:45:15 | 200 |    4.0824376s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:45:17 | 200 |    4.0567711s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:45:18 | 200 |     4.063243s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:45:20 | 200 |    4.1211802s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:45:21 | 200 |    4.1139289s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:45:23 | 200 |     4.120477s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:45:24 | 200 |    4.1004292s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:45:26 | 200 |    4.0739137s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:45:27 | 200 |    4.1047296s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:45:28 | 200 |     4.118054s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:45:30 | 200 |    4.1185424s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:45:31 | 200 |    4.1207568s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:45:33 | 200 |    4.1565417s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:45:34 | 200 |      4.09523s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:45:36 | 200 |    4.0782968s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:45:37 | 200 |    4.0829175s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:45:39 | 200 |    4.0803026s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:45:40 | 200 |    4.0867696s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:45:42 | 200 |    4.0931232s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:45:43 | 200 |    4.1095477s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:45:44 | 200 |    4.1039081s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:45:46 | 200 |    4.0772243s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:45:47 | 200 |    4.0661831s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:45:49 | 200 |    4.1248321s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:45:50 | 200 |    4.1445002s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:45:52 | 200 |    4.1360743s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:45:53 | 200 |    4.1599281s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:45:55 | 200 |    4.1496613s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:45:56 | 200 |    4.1428085s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:45:58 | 200 |    4.1412278s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:45:59 | 200 |    4.1053436s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:46:01 | 200 |    4.1646273s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:46:02 | 200 |    4.1205885s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:46:03 | 200 |    4.0360091s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:46:05 | 200 |    4.1075538s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:46:06 | 200 |    4.1562711s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:46:08 | 200 |    4.1662521s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:46:09 | 200 |    4.1025821s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:46:11 | 200 |    4.1280453s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:46:12 | 200 |    3.3409675s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:46:13 | 200 |    3.3349543s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:46:14 | 200 |    3.3497135s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:46:15 | 200 |    3.3615447s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:46:17 | 200 |    3.4280355s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:46:18 | 200 |    3.3619968s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:46:19 | 200 |     3.570449s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:46:20 | 200 |    3.5430434s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:46:22 | 200 |    3.6006927s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:46:23 | 200 |    3.3512915s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:46:24 | 200 |    3.2498373s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:46:26 | 200 |    3.3367689s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:46:26 | 200 |    3.4471569s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:46:28 | 200 |    3.4601509s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:46:29 | 200 |    3.4159592s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:46:30 | 200 |    3.4722146s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:46:32 | 200 |    3.4486094s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:46:33 | 200 |    3.4590311s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:46:34 | 200 |    3.4641924s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:46:35 | 200 |    3.4974826s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:46:37 | 200 |    3.4941214s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:46:38 | 200 |    3.4352723s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:46:39 | 200 |    3.4752113s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:46:41 | 200 |    3.4841276s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:46:41 | 200 |    3.4551121s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:46:43 | 200 |     3.424301s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:46:45 | 200 |    3.8510545s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:46:46 | 200 |    3.8385223s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:46:47 | 200 |    3.8121226s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:46:48 | 200 |    3.3842687s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:46:49 | 200 |    3.4129087s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:46:51 | 200 |    3.4107414s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:46:52 | 200 |    3.4014219s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:46:53 | 200 |     3.360391s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:46:54 | 200 |     3.423112s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:46:56 | 200 |    3.5000604s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:46:57 | 200 |    3.4374355s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:46:58 | 200 |    3.4165156s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:47:00 | 200 |    3.4583611s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:47:00 | 200 |    3.5134467s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:47:02 | 200 |    3.4779946s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:47:03 | 200 |    3.4491913s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:47:04 | 200 |    3.4717164s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:47:06 | 200 |    3.4759522s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:47:07 | 200 |    3.4783319s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:47:08 | 200 |    3.4572789s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:47:09 | 200 |    3.5131538s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:47:11 | 200 |    3.4907152s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:47:12 | 200 |    3.4504822s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:47:13 | 200 |    3.4481666s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:47:15 | 200 |    3.4255205s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:47:15 | 200 |    3.4852572s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:47:17 | 200 |    3.4225348s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:47:18 | 200 |    3.5213467s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:47:19 | 200 |    3.4919725s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:47:21 | 200 |    3.4824481s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:47:22 | 200 |    3.5045347s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:47:23 | 200 |    3.4703916s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:47:24 | 200 |    3.5236998s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:47:26 | 200 |    3.5266984s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:47:28 | 200 |    4.2545426s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:47:29 | 200 |    4.2388653s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:47:30 | 200 |    4.2725072s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:47:32 | 200 |    4.2925182s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:47:34 | 200 |    4.3798505s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:47:35 | 200 |    4.3755877s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:47:37 | 200 |    4.4328353s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:47:38 | 200 |    4.3707727s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:47:40 | 200 |    4.3249843s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:47:41 | 200 |    4.3684827s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:47:43 | 200 |    4.2555756s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:47:44 | 200 |    4.3122811s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:47:46 | 200 |    4.4198495s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:47:48 | 200 |    4.4259418s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:47:49 | 200 |    4.4121932s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:47:51 | 200 |    4.3189718s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:47:52 | 200 |    4.2468596s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:47:54 | 200 |    4.2793911s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:47:55 | 200 |    4.1427731s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:47:57 | 200 |    4.1680281s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:47:58 | 200 |    4.1031721s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:48:00 | 200 |    4.0985229s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:48:01 | 200 |    4.0474012s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:48:02 | 200 |    3.9633704s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:48:04 | 200 |    3.9882289s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:48:05 | 200 |    4.0459608s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:48:07 | 200 |    4.0331054s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:48:08 | 200 |    4.0556981s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:48:10 | 200 |    4.0635114s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:48:11 | 200 |    4.0704839s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:48:12 | 200 |    4.0356352s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:48:14 | 200 |     3.974828s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:48:15 | 200 |    3.9960376s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:48:17 | 200 |    3.9950153s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:48:18 | 200 |    4.0127009s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:48:20 | 200 |    4.0728736s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:48:21 | 200 |    4.0886199s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:48:23 | 200 |     4.074615s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:48:24 | 200 |    4.0640371s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:48:25 | 200 |    4.0964069s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:48:27 | 200 |    4.0823284s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:48:28 | 200 |      4.05735s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:48:30 | 200 |    3.9822191s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:48:31 | 200 |     4.045282s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:48:33 | 200 |    4.0149304s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:48:34 | 200 |    4.0629878s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:48:36 | 200 |    4.0822895s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:48:37 | 200 |    4.2592685s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:48:39 | 200 |    4.2833319s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:48:40 | 200 |    4.3125863s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:48:42 | 200 |     4.287633s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:48:43 | 200 |     4.257705s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:48:45 | 200 |    4.2300929s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:48:46 | 200 |    4.2779701s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:48:48 | 200 |    4.2516449s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:48:49 | 200 |    4.2033584s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:48:51 | 200 |    3.9949012s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:48:52 | 200 |    3.9768619s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:48:53 | 200 |    3.9718929s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:48:55 | 200 |    3.9768068s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:48:56 | 200 |    3.9692951s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:48:58 | 200 |    4.0444226s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:48:59 | 200 |    4.1782349s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:49:01 | 200 |    4.2267168s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:49:02 | 200 |     4.255228s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:49:04 | 200 |    4.1294389s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:49:05 | 200 |    4.1797583s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:49:07 | 200 |     4.228463s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:49:08 | 200 |    4.0285648s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:49:09 | 200 |    3.9648944s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:49:11 | 200 |    4.0109944s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:49:12 | 200 |    4.0238062s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:49:14 | 200 |    4.0314973s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:49:15 | 200 |    4.0041288s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:49:17 | 200 |    4.0044173s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:49:18 | 200 |    3.9939885s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:49:19 | 200 |    3.9025535s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:49:21 | 200 |    3.9502805s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:49:22 | 200 |    3.9886812s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:49:24 | 200 |     3.897639s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:49:25 | 200 |    3.9343924s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:49:26 | 200 |    3.9169002s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:49:28 | 200 |    3.9235802s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:49:29 | 200 |    3.9412538s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:49:31 | 200 |     3.927706s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:49:32 | 200 |    3.9833857s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:49:33 | 200 |    4.0298592s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:49:35 | 200 |    3.9936119s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:49:36 | 200 |    3.9671743s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:49:38 | 200 |     3.958931s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:49:39 | 200 |      3.96431s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:49:41 | 200 |    3.9907086s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:49:42 | 200 |    3.9748258s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:49:43 | 200 |    4.0191927s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:49:45 | 200 |    4.0217106s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:49:46 | 200 |    4.0024472s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:49:47 | 200 |    3.7886376s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:49:49 | 200 |    3.7576443s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:49:50 | 200 |    3.7573449s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:49:51 | 200 |     3.430952s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:49:53 | 200 |    3.4502589s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:49:54 | 200 |    3.4543218s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:49:55 | 200 |    3.5882964s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:49:56 | 200 |     3.601882s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:49:58 | 200 |    3.6293984s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:49:59 | 200 |    3.4584968s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:50:00 | 200 |    3.4790384s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:50:02 | 200 |    3.4910028s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:50:02 | 200 |    3.5013644s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:50:04 | 200 |    3.4644703s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:50:05 | 200 |    3.4745429s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:50:06 | 200 |    3.6943848s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:50:08 | 200 |    3.7625341s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:50:09 | 200 |    3.7922195s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:50:10 | 200 |    3.5949914s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:50:12 | 200 |    3.5672192s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:50:13 | 200 |    3.5113868s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:50:14 | 200 |    3.6870941s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:50:16 | 200 |    3.6690142s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:50:17 | 200 |    3.6709985s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:50:18 | 200 |    3.5116764s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:50:19 | 200 |    3.5917477s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:50:21 | 200 |    3.6351805s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:50:22 | 200 |    3.6436671s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:50:23 | 200 |    3.6745039s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:50:25 | 200 |    3.6738405s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:50:26 | 200 |    3.6281631s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:50:27 | 200 |    3.6421291s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:50:29 | 200 |    3.6278419s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:50:30 | 200 |    3.6677365s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:50:31 | 200 |    3.5907771s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:50:33 | 200 |    3.6231374s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:50:34 | 200 |    3.6141313s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:50:35 | 200 |    3.6471354s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:50:36 | 200 |    3.6792681s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:50:38 | 200 |     3.806465s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:50:39 | 200 |    3.7583192s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:50:41 | 200 |    3.7657203s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:50:42 | 200 |    3.7407041s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:50:43 | 200 |    3.7498524s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:50:45 | 200 |    3.7583345s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:50:46 | 200 |    3.9153096s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:50:47 | 200 |    3.8951683s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:50:49 | 200 |    3.9078902s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:50:50 | 200 |    3.8621156s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:50:51 | 200 |     3.919735s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:50:53 | 200 |    3.9449723s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:50:54 | 200 |    3.9624047s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:50:56 | 200 |    3.9325564s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:50:57 | 200 |    3.9022669s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:50:58 | 200 |    3.7669081s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:51:00 | 200 |    3.7559184s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:51:01 | 200 |     3.735633s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:51:02 | 200 |    3.4873816s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:51:03 | 200 |    3.3863152s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:51:05 | 200 |    3.5023203s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:51:06 | 200 |    3.6565469s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:51:07 | 200 |    3.6547702s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:51:09 | 200 |    3.6321532s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:51:10 | 200 |    3.6723394s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:51:11 | 200 |    3.5983303s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:51:13 | 200 |    3.6268582s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:51:14 | 200 |     3.573268s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:51:15 | 200 |    3.6856362s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:51:17 | 200 |    3.6880119s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:51:18 | 200 |    3.7151754s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:51:19 | 200 |    3.8179304s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:51:21 | 200 |    3.7474568s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:51:22 | 200 |    3.8034608s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:51:23 | 200 |    3.8022055s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:51:25 | 200 |    3.7992528s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:51:26 | 200 |    3.7544481s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:51:27 | 200 |    3.7587638s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:51:29 | 200 |    3.7555831s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:51:30 | 200 |     3.788049s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:51:31 | 200 |    3.8557768s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:51:33 | 200 |    3.8485519s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:51:34 | 200 |    4.3539652s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:51:36 | 200 |    4.3308741s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:51:38 | 200 |    4.4225848s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:51:39 | 200 |    3.9948638s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:51:40 | 200 |    3.8410561s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:51:42 | 200 |     3.757871s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:51:43 | 200 |    3.5264264s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:51:44 | 200 |    3.5044555s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:51:45 | 200 |      3.51831s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:51:46 | 200 |    3.5338216s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:51:48 | 200 |     3.501306s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:51:49 | 200 |    3.4725658s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:51:50 | 200 |    3.5421673s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:51:52 | 200 |    3.5180608s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:51:53 | 200 |    3.4796549s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:51:54 | 200 |    3.2108771s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:51:55 | 200 |    3.2701682s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:51:57 | 200 |     3.190867s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:51:58 | 200 |    3.6807149s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:51:59 | 200 |    3.7041711s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:52:01 | 200 |    3.7602792s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:52:02 | 200 |    3.8249828s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:52:03 | 200 |    3.7158341s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:52:05 | 200 |     3.737637s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:52:06 | 200 |    3.9347609s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:52:07 | 200 |    4.1167632s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:52:09 | 200 |    4.1449387s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:52:10 | 200 |    3.8417021s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:52:12 | 200 |    3.8293576s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:52:13 | 200 |    3.7658958s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:52:14 | 200 |    3.7449037s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:52:15 | 200 |    3.7125209s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:52:17 | 200 |    3.6892457s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:52:18 | 200 |    3.7325949s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:52:19 | 200 |    3.7106678s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:52:21 | 200 |    3.6531562s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:52:22 | 200 |    3.6779784s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:52:23 | 200 |    3.7322163s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:52:25 | 200 |    3.7289017s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:52:26 | 200 |    3.7420791s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:52:27 | 200 |    3.8300347s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:52:29 | 200 |    3.8012755s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:52:30 | 200 |    3.8165324s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:52:32 | 200 |    3.9274214s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:52:33 | 200 |    3.9520467s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:52:34 | 200 |    3.9444763s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:52:36 | 200 |    3.8978518s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:52:37 | 200 |    3.9292016s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:52:39 | 200 |    3.9445519s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:52:40 | 200 |    3.9055534s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:52:42 | 200 |    3.8687235s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:52:43 | 200 |    3.8147132s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:52:44 | 200 |    3.8141403s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:52:46 | 200 |    3.8460432s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:52:47 | 200 |    3.8775032s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:52:48 | 200 |    3.8884029s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:52:50 | 200 |    3.8530224s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:52:51 | 200 |    3.8646581s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:52:52 | 200 |    3.8889632s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:52:54 | 200 |    3.9204839s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:52:55 | 200 |    3.9154969s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:52:57 | 200 |    3.8958524s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:52:58 | 200 |    3.8895062s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:52:59 | 200 |    3.8328423s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:53:01 | 200 |    3.8317952s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:53:02 | 200 |     3.881945s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:53:03 | 200 |    3.9404289s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:53:05 | 200 |    3.9138362s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:53:06 | 200 |    3.8981139s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:53:08 | 200 |    3.8382351s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:53:09 | 200 |    3.8218838s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:53:11 | 200 |    3.8073674s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:53:12 | 200 |    3.9732447s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:53:13 | 200 |    3.9128438s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:53:15 | 200 |    3.9315595s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:53:15 | 200 |    3.3362637s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:53:17 | 200 |    3.2937253s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:53:18 | 200 |    3.3309404s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:53:19 | 200 |    3.5092875s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:53:21 | 200 |    3.4546501s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:53:22 | 200 |    3.4572225s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:53:23 | 200 |    3.5106021s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:53:24 | 200 |    3.5275075s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:53:26 | 200 |    3.5445217s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:53:27 | 200 |    3.5183113s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:53:28 | 200 |    3.5568484s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:53:30 | 200 |    3.5639666s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:53:31 | 200 |    3.5101097s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:53:32 | 200 |    3.4978411s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:53:34 | 200 |    3.4948404s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:53:34 | 200 |    3.5304425s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:53:36 | 200 |    3.5614329s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:53:37 | 200 |    3.5386936s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:53:38 | 200 |     3.524635s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:53:40 | 200 |    3.5611494s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:53:41 | 200 |    3.5652359s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:53:42 | 200 |    3.5083294s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:53:43 | 200 |     3.482814s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:53:45 | 200 |    3.5125589s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:53:46 | 200 |    3.4765222s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:53:47 | 200 |    3.4910339s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:53:49 | 200 |    3.4761021s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:53:50 | 200 |    3.5028784s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:53:51 | 200 |    3.5039182s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:53:52 | 200 |    3.4841715s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:53:53 | 200 |    3.4730105s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:53:55 | 200 |    3.4897948s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:53:56 | 200 |     3.520687s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:53:57 | 200 |    3.5018402s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:53:59 | 200 |    3.4977955s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:54:00 | 200 |    3.5130172s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:54:01 | 200 |    3.4914591s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:54:02 | 200 |     3.573954s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:54:04 | 200 |    3.4488782s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:54:05 | 200 |     3.538022s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:54:06 | 200 |    3.5448953s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:54:08 | 200 |    3.4889204s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:54:09 | 200 |    3.4807913s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:54:10 | 200 |    3.5084592s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:54:11 | 200 |    3.5443796s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:54:12 | 200 |    3.5258849s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:54:14 | 200 |    3.5185574s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:54:15 | 200 |     3.560018s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:54:16 | 200 |    3.5349874s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:54:18 | 200 |    3.5812692s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:54:19 | 200 |    3.5811298s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:54:20 | 200 |    3.5067069s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:54:21 | 200 |    3.4884029s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:54:23 | 200 |     3.481186s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:54:24 | 200 |    3.5016701s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:54:25 | 200 |     3.347307s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:54:27 | 200 |    3.4497388s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:54:27 | 200 |     3.475451s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:54:29 | 200 |    3.4174867s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:54:30 | 200 |    3.4206531s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:54:31 | 200 |    3.4410054s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:54:33 | 200 |    3.4216578s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:54:34 | 200 |    3.4530481s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:54:35 | 200 |    3.4266224s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:54:36 | 200 |    3.4263723s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:54:38 | 200 |    3.3754335s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:54:39 | 200 |    3.3693078s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:54:40 | 200 |    3.4081456s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:54:41 | 200 |    3.3773104s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:54:42 | 200 |    3.3785761s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:54:44 | 200 |    3.3536755s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:54:45 | 200 |    3.3596583s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:54:46 | 200 |    3.3786506s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:54:47 | 200 |    3.4266111s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:54:49 | 200 |    3.3946638s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:54:50 | 200 |    3.4294213s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:54:51 | 200 |    3.4059866s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:54:52 | 200 |    3.3532308s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:54:53 | 200 |    3.4087941s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:54:55 | 200 |     3.423859s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:54:56 | 200 |    3.4413125s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:54:57 | 200 |    3.3918349s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:54:58 | 200 |     3.394145s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:55:00 | 200 |     3.417344s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:55:01 | 200 |    3.4230396s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:55:02 | 200 |    3.3878199s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:55:04 | 200 |     3.372548s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:55:04 | 200 |    3.3979414s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:55:06 | 200 |    3.3372105s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:55:07 | 200 |      3.36748s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:55:08 | 200 |    3.4255309s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:55:09 | 200 |    3.4481602s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:55:11 | 200 |    3.3842188s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:55:12 | 200 |    3.3892114s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:55:13 | 200 |    3.3665892s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:55:14 | 200 |    3.3824872s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:55:15 | 200 |    3.3059527s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:55:17 | 200 |     3.277507s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:55:18 | 200 |    3.3142385s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:55:19 | 200 |    3.3016508s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:55:20 | 200 |    3.3860966s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:55:22 | 200 |    3.3482738s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:55:23 | 200 |    3.3866334s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:55:24 | 200 |    3.3104133s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:55:25 | 200 |    3.3839029s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:55:26 | 200 |     3.341558s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:55:27 | 200 |    3.3386512s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:55:29 | 200 |    3.4874864s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:55:30 | 200 |    3.4499762s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:55:31 | 200 |     3.410132s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:55:33 | 200 |    3.5131061s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:55:34 | 200 |    3.4595167s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:55:35 | 200 |    3.5024829s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:55:37 | 200 |     3.521712s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:55:37 | 200 |    3.4626098s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:55:39 | 200 |    3.3612431s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:55:41 | 200 |    3.6717138s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:55:41 | 200 |    3.7383401s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:55:43 | 200 |    3.7472157s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:55:45 | 200 |    3.6822468s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:55:45 | 200 |    3.6330133s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:55:47 | 200 |     3.624703s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:55:48 | 200 |    3.6529018s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:55:49 | 200 |    3.5474882s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:55:51 | 200 |    3.6616922s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:55:52 | 200 |    3.3967614s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:55:53 | 200 |    3.3963128s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:55:54 | 200 |    3.3843361s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:55:56 | 200 |    3.3738102s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:55:57 | 200 |     3.325645s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:55:58 | 200 |    3.3482823s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:55:59 | 200 |    3.3719068s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:56:00 | 200 |    3.3679831s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:56:02 | 200 |    3.3842757s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:56:03 | 200 |    3.5284079s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:56:04 | 200 |    3.5457017s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:56:05 | 200 |    3.5738755s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:56:07 | 200 |    3.3576682s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:56:08 | 200 |    3.3289446s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:56:09 | 200 |    3.3703397s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:56:11 | 200 |    3.3806429s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:56:11 | 200 |    3.3451844s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:56:13 | 200 |    3.3378531s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:56:14 | 200 |    3.4206332s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:56:15 | 200 |    3.3774992s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:56:16 | 200 |    3.3954316s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:56:18 | 200 |    3.3861737s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:56:19 | 200 |    3.3518454s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:56:20 | 200 |    3.3501321s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:56:22 | 200 |    3.3878083s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:56:22 | 200 |    3.3660966s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:56:24 | 200 |    3.3833873s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:56:25 | 200 |     3.379124s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:56:26 | 200 |     3.355101s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:56:27 | 200 |    3.3876518s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:56:29 | 200 |    3.3138153s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:56:30 | 200 |    3.3941924s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:56:31 | 200 |    3.3518293s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:56:33 | 200 |    3.5213714s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:56:34 | 200 |    3.5619715s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:56:35 | 200 |    3.5467342s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:56:36 | 200 |    3.3499177s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:56:37 | 200 |     3.342586s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:56:38 | 200 |    3.3586849s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:56:40 | 200 |    3.3818998s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:56:41 | 200 |    3.3267689s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:56:42 | 200 |    3.2749976s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:56:44 | 200 |    3.2838738s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:56:44 | 200 |    3.3554227s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:56:46 | 200 |     3.400978s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:56:47 | 200 |    3.3605805s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:56:48 | 200 |    3.4704555s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:56:49 | 200 |    3.4349031s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:56:51 | 200 |     3.397865s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:56:52 | 200 |    3.3819569s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:56:53 | 200 |    3.4319372s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:56:55 | 200 |    3.4430685s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:56:55 | 200 |    3.3630404s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:56:57 | 200 |     3.345656s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:56:58 | 200 |    3.3576484s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:56:59 | 200 |    3.3722122s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:57:00 | 200 |    3.4000394s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:57:02 | 200 |    3.3593288s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:57:03 | 200 |    3.4269464s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:57:04 | 200 |    3.3901624s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:57:06 | 200 |    3.3937072s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:57:06 | 200 |    3.4156609s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:57:08 | 200 |    3.4209036s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:57:09 | 200 |    3.4050812s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:57:10 | 200 |    3.4166827s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:57:11 | 200 |    3.4594444s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:57:13 | 200 |    3.4329854s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:57:14 | 200 |    3.3860115s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:57:15 | 200 |    3.4405659s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:57:17 | 200 |    3.3748615s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:57:17 | 200 |    3.3997998s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:57:19 | 200 |    3.3817901s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:57:20 | 200 |    3.4185463s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:57:21 | 200 |    3.3822432s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:57:22 | 200 |    3.4071409s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:57:24 | 200 |    3.3747431s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:57:25 | 200 |     3.355449s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:57:26 | 200 |    3.4188523s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:57:28 | 200 |    3.3155736s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:57:28 | 200 |    3.4015294s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:57:30 | 200 |    3.4048731s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:57:31 | 200 |    3.3985846s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:57:32 | 200 |    3.3616053s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:57:33 | 200 |    3.3463674s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:57:35 | 200 |    3.3420752s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:57:36 | 200 |    3.3740151s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:57:37 | 200 |    3.4283557s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:57:39 | 200 |    3.3671735s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:57:39 | 200 |    3.4141566s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:57:41 | 200 |    3.4051128s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:57:42 | 200 |    3.4055812s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:57:43 | 200 |     3.352461s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:57:44 | 200 |    3.4177854s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:57:46 | 200 |    3.3721805s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:57:47 | 200 |    3.3570516s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:57:48 | 200 |    3.3530478s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:57:50 | 200 |    3.4100409s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:57:50 | 200 |    3.3729202s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:57:52 | 200 |    3.3424724s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:57:53 | 200 |    3.3217991s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:57:54 | 200 |    3.3654732s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:57:55 | 200 |    3.3157294s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:57:57 | 200 |     3.315591s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:57:58 | 200 |    3.3426014s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:57:59 | 200 |    3.3727836s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:58:00 | 200 |    3.3154665s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:58:01 | 200 |    3.3538419s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:58:03 | 200 |    3.3637614s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:58:04 | 200 |    3.4126608s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:58:05 | 200 |    3.2688234s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:58:06 | 200 |    3.3541906s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:58:08 | 200 |    3.5948839s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:58:09 | 200 |    3.5538908s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:58:10 | 200 |    3.5829916s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:58:11 | 200 |    3.3502846s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:58:12 | 200 |    3.3628176s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:58:14 | 200 |      3.34117s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:58:15 | 200 |    3.3231523s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:58:16 | 200 |    3.2930183s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:58:17 | 200 |    3.2819666s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:58:19 | 200 |    3.2969063s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:58:19 | 200 |    3.2722604s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:58:21 | 200 |     3.271635s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:58:22 | 200 |    3.3166219s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:58:23 | 200 |    3.3015231s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:58:24 | 200 |    3.2733425s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:58:26 | 200 |    3.3200058s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:58:27 | 200 |    3.3578174s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:58:28 | 200 |    3.3671673s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:58:29 | 200 |    3.3209696s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:58:30 | 200 |    3.3300457s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:58:31 | 200 |    3.2797835s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:58:33 | 200 |    3.2991386s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:58:34 | 200 |      3.35135s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:58:36 | 200 |    3.8774644s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:58:37 | 200 |    3.9306635s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:58:38 | 200 |    4.0625042s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:58:40 | 200 |    3.9714837s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:58:42 | 200 |    3.9481963s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:58:42 | 200 |    4.0450823s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:58:44 | 200 |    3.9482462s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:58:45 | 200 |    3.6316792s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:58:46 | 200 |    3.5683934s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:58:48 | 200 |    3.6482525s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:58:49 | 200 |      3.62137s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:58:50 | 200 |    3.6328742s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:58:52 | 200 |    3.6627179s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:58:53 | 200 |    3.6607185s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:58:54 | 200 |    3.6667547s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:58:56 | 200 |    3.6312749s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:58:57 | 200 |    3.6533545s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:58:58 | 200 |    3.6284886s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:59:00 | 200 |     3.653592s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:59:01 | 200 |    3.6115602s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:59:02 | 200 |    3.6818487s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:59:04 | 200 |    3.6627004s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:59:05 | 200 |    3.6380754s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:59:06 | 200 |    3.6357742s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:59:08 | 200 |    3.6612114s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:59:09 | 200 |    3.6756168s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:59:10 | 200 |    3.6646606s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:59:12 | 200 |    3.6415332s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:59:13 | 200 |    3.6553835s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:59:14 | 200 |    3.6426535s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:59:15 | 200 |    3.6322547s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:59:17 | 200 |    3.6615334s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:59:18 | 200 |    3.6454079s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:59:19 | 200 |    3.6173573s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:59:21 | 200 |    3.6364651s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:59:21 | 200 |    3.6195137s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:59:23 | 200 |    3.6364202s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:59:25 | 200 |    3.6522107s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:59:25 | 200 |    3.6617291s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:59:27 | 200 |    3.6287565s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:59:28 | 200 |    3.6625294s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:59:29 | 200 |    3.6290612s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:59:31 | 200 |    3.6197656s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:59:32 | 200 |    3.6406373s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:59:33 | 200 |    3.6178134s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:59:35 | 200 |    3.7086782s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:59:36 | 200 |     3.717472s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:59:37 | 200 |    3.7212213s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:59:39 | 200 |    3.6628465s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:59:40 | 200 |    3.6912187s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:59:41 | 200 |     3.690543s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:59:43 | 200 |    3.7422584s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:59:44 | 200 |    3.7089415s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:59:45 | 200 |    3.6837016s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:59:47 | 200 |    3.7018181s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:59:48 | 200 |    3.6905049s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:59:49 | 200 |    3.7572184s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:59:51 | 200 |    3.7649795s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:59:52 | 200 |    3.7871703s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:59:53 | 200 |    3.7437071s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:59:55 | 200 |    3.7536056s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:59:56 | 200 |     3.741849s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:59:57 | 200 |    3.7708097s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 20:59:59 | 200 |    3.7545109s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 21:00:00 | 200 |    3.8162162s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 21:00:01 | 200 |    3.7754536s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 21:00:03 | 200 |    3.7898943s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 21:00:04 | 200 |    3.8098456s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 21:00:05 | 200 |     3.804527s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 21:00:07 | 200 |    3.7666378s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 21:00:08 | 200 |    3.7769505s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 21:00:09 | 200 |    3.7409106s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 21:00:11 | 200 |    3.7653786s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 21:00:12 | 200 |    3.7559706s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 21:00:13 | 200 |    3.7598886s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 21:00:15 | 200 |    3.7571118s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 21:00:16 | 200 |    3.7918035s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 21:00:17 | 200 |     3.796679s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 21:00:19 | 200 |    3.7020903s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 21:00:20 | 200 |    3.7718038s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 21:00:21 | 200 |    3.7632478s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 21:00:23 | 200 |    3.7494552s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 21:00:24 | 200 |    3.7654776s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 21:00:25 | 200 |    3.7864316s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 21:00:27 | 200 |    3.7834083s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 21:00:28 | 200 |    3.7842582s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 21:00:29 | 200 |    3.7799708s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 21:00:31 | 200 |    3.8017924s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 21:00:32 | 200 |    3.8384415s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 21:00:33 | 200 |    3.8086161s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 21:00:35 | 200 |    3.7821144s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 21:00:36 | 200 |    3.8319875s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 21:00:37 | 200 |    3.8458974s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 21:00:39 | 200 |    3.8222869s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 21:00:40 | 200 |    3.8020166s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 21:00:41 | 200 |    3.8425086s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 21:00:42 | 200 |    2.8844116s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 21:00:44 | 200 |    2.9763848s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 21:00:45 | 200 |    3.0225692s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 21:00:46 | 200 |    4.0527801s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 21:00:48 | 200 |    4.0455613s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 21:00:49 | 200 |    4.0765502s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 21:00:51 | 200 |     3.938011s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 21:00:52 | 200 |    3.8297208s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 21:00:53 | 200 |    3.8141045s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 21:00:55 | 200 |    3.7137295s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 21:00:56 | 200 |    3.8266583s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 21:00:57 | 200 |     3.787013s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 21:00:59 | 200 |    3.7427937s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 21:01:00 | 200 |    3.7804952s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 21:01:01 | 200 |    3.7071446s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 21:01:03 | 200 |    3.7373168s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 21:01:04 | 200 |    3.7579559s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 21:01:05 | 200 |    3.7684858s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 21:01:07 | 200 |    3.7587811s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 21:01:08 | 200 |    3.7188841s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 21:01:09 | 200 |    3.7015781s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 21:01:11 | 200 |    3.7142879s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 21:01:12 | 200 |    3.7367887s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 21:01:13 | 200 |    3.8046854s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 21:01:15 | 200 |    3.7893911s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 21:01:16 | 200 |    3.7797378s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 21:01:17 | 200 |    3.8250078s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 21:01:19 | 200 |    3.8376288s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 21:01:20 | 200 |    3.8245551s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 21:01:21 | 200 |    3.8626729s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 21:01:23 | 200 |    3.8705171s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 21:01:24 | 200 |    3.8789226s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 21:01:26 | 200 |    3.9699151s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 21:01:27 | 200 |    4.0470803s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 21:01:29 | 200 |    4.0635827s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 21:01:30 | 200 |    3.8809277s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 21:01:31 | 200 |    3.8895597s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 21:01:33 | 200 |     3.886752s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 21:01:34 | 200 |    3.8663249s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 21:01:36 | 200 |    3.8107833s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 21:01:37 | 200 |     3.873908s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 21:01:38 | 200 |    3.8141796s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 21:01:40 | 200 |    3.8806929s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 21:01:41 | 200 |    3.8475589s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 21:01:42 | 200 |    3.8828155s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 21:01:44 | 200 |    3.8708454s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 21:01:46 | 200 |    4.5237232s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 21:01:47 | 200 |    4.5329586s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 21:01:49 | 200 |    4.5488572s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 21:01:50 | 200 |    3.5247476s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 21:01:51 | 200 |    3.4589296s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 21:01:52 | 200 |    3.4251961s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 21:01:54 | 200 |    3.9793684s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 21:01:55 | 200 |    4.0124944s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 21:01:57 | 200 |    4.0149515s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 21:01:58 | 200 |    3.8745086s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 21:01:59 | 200 |    3.7181587s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 21:02:01 | 200 |    3.7650551s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 21:02:02 | 200 |    3.6395281s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 21:02:03 | 200 |    3.8068922s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 21:02:05 | 200 |    3.8286934s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 21:02:06 | 200 |    3.8244315s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 21:02:07 | 200 |    3.8158218s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 21:02:09 | 200 |    3.7773401s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 21:02:10 | 200 |    3.7707975s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 21:02:11 | 200 |    3.7785501s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 21:02:13 | 200 |    3.7228063s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 21:02:14 | 200 |    3.9496427s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 21:02:15 | 200 |    3.8844129s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 21:02:17 | 200 |    3.9195233s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 21:02:18 | 200 |     3.715029s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 21:02:19 | 200 |    3.7803843s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 21:02:21 | 200 |    3.7756912s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 21:02:22 | 200 |    3.7613003s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 21:02:23 | 200 |      3.76237s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 21:02:25 | 200 |      3.75934s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 21:02:26 | 200 |    3.7307474s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 21:02:27 | 200 |    3.8362747s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 21:02:29 | 200 |    3.8630982s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 21:02:30 | 200 |    3.8785982s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 21:02:31 | 200 |    3.7196448s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 21:02:33 | 200 |     3.738623s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 21:02:35 | 200 |    3.8635362s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 21:02:35 | 200 |    3.8632285s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 21:02:37 | 200 |    3.9572227s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 21:02:39 | 200 |    3.8035898s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 21:02:40 | 200 |    3.7935556s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 21:02:41 | 200 |    3.7838699s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 21:02:43 | 200 |    3.7249735s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 21:02:44 | 200 |    3.7505908s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 21:02:45 | 200 |    3.7592623s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 21:02:47 | 200 |    3.7384318s |       127.0.0.1 | POST     "/api/chat"
time=2025-10-21T21:02:47.511-04:00 level=INFO source=server.go:1454 msg="aborting completion request due to client closing the connection"
[GIN] 2025/10/21 - 21:02:47 | 500 |    1.2436468s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/21 - 21:02:47 | 500 |    3.0218534s |       127.0.0.1 | POST     "/api/chat"
llama_model_loader: loaded meta data with 22 key-value pairs and 291 tensors from C:\Users\frost\.ollama\models\blobs\sha256-6a0746a1ec1aef3e7ec53868f220ff6e389f6f8ef87a01d77c96807de94ca2aa (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = llama
llama_model_loader: - kv   1:                               general.name str              = Meta-Llama-3-8B-Instruct
llama_model_loader: - kv   2:                          llama.block_count u32              = 32
llama_model_loader: - kv   3:                       llama.context_length u32              = 8192
llama_model_loader: - kv   4:                     llama.embedding_length u32              = 4096
llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 14336
llama_model_loader: - kv   6:                 llama.attention.head_count u32              = 32
llama_model_loader: - kv   7:              llama.attention.head_count_kv u32              = 8
llama_model_loader: - kv   8:                       llama.rope.freq_base f32              = 500000.000000
llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010
llama_model_loader: - kv  10:                          general.file_type u32              = 2
llama_model_loader: - kv  11:                           llama.vocab_size u32              = 128256
llama_model_loader: - kv  12:                 llama.rope.dimension_count u32              = 128
llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = llama-bpe
llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,128256]  = ["!", "\"", "#", "$", "%", "&", "'", ...
llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,128256]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,280147]  = ["Ġ Ġ", "Ġ ĠĠĠ", "ĠĠ ĠĠ", "...
llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 128000
llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 128009
llama_model_loader: - kv  20:                    tokenizer.chat_template str              = {% set loop_messages = messages %}{% ...
llama_model_loader: - kv  21:               general.quantization_version u32              = 2
llama_model_loader: - type  f32:   65 tensors
llama_model_loader: - type q4_0:  225 tensors
llama_model_loader: - type q6_K:    1 tensors
print_info: file format = GGUF V3 (latest)
print_info: file type   = Q4_0
print_info: file size   = 4.33 GiB (4.64 BPW) 
load: printing all EOG tokens:
load:   - 128001 ('<|end_of_text|>')
load:   - 128009 ('<|eot_id|>')
load: special tokens cache size = 256
load: token to piece cache size = 0.8000 MB
print_info: arch             = llama
print_info: vocab_only       = 1
print_info: model type       = ?B
print_info: model params     = 8.03 B
print_info: general.name     = Meta-Llama-3-8B-Instruct
print_info: vocab type       = BPE
print_info: n_vocab          = 128256
print_info: n_merges         = 280147
print_info: BOS token        = 128000 '<|begin_of_text|>'
print_info: EOS token        = 128009 '<|eot_id|>'
print_info: EOT token        = 128001 '<|end_of_text|>'
print_info: LF token         = 198 'Ċ'
print_info: EOG token        = 128001 '<|end_of_text|>'
print_info: EOG token        = 128009 '<|eot_id|>'
print_info: max token length = 256
llama_model_load: vocab only - skipping tensors
time=2025-10-21T21:02:48.931-04:00 level=INFO source=cpu_windows.go:139 msg=packages count=1
time=2025-10-21T21:02:48.932-04:00 level=INFO source=cpu_windows.go:155 msg="efficiency cores detected" maxEfficiencyClass=1
time=2025-10-21T21:02:48.932-04:00 level=INFO source=cpu_windows.go:186 msg="" package=0 cores=12 efficiency=4 threads=20
time=2025-10-21T21:02:48.932-04:00 level=INFO source=server.go:400 msg="starting runner" cmd="C:\\Users\\frost\\AppData\\Local\\Programs\\Ollama\\ollama.exe runner --model C:\\Users\\frost\\.ollama\\models\\blobs\\sha256-6a0746a1ec1aef3e7ec53868f220ff6e389f6f8ef87a01d77c96807de94ca2aa --port 49720"
time=2025-10-21T21:02:48.938-04:00 level=INFO source=cpu_windows.go:139 msg=packages count=1
time=2025-10-21T21:02:48.938-04:00 level=INFO source=cpu_windows.go:155 msg="efficiency cores detected" maxEfficiencyClass=1
time=2025-10-21T21:02:48.938-04:00 level=INFO source=cpu_windows.go:186 msg="" package=0 cores=12 efficiency=4 threads=20
time=2025-10-21T21:02:48.938-04:00 level=INFO source=server.go:505 msg="system memory" total="63.8 GiB" free="46.8 GiB" free_swap="46.9 GiB"
time=2025-10-21T21:02:48.939-04:00 level=INFO source=memory.go:36 msg="new model will fit in available VRAM across minimum required GPUs, loading" model=C:\Users\frost\.ollama\models\blobs\sha256-6a0746a1ec1aef3e7ec53868f220ff6e389f6f8ef87a01d77c96807de94ca2aa library=CUDA parallel=1 required="6.2 GiB" gpus=1
time=2025-10-21T21:02:48.939-04:00 level=INFO source=server.go:545 msg=offload library=CUDA layers.requested=-1 layers.model=33 layers.offload=33 layers.split=[33] memory.available="[10.6 GiB]" memory.gpu_overhead="0 B" memory.required.full="6.2 GiB" memory.required.partial="6.2 GiB" memory.required.kv="1.0 GiB" memory.required.allocations="[6.2 GiB]" memory.weights.total="4.1 GiB" memory.weights.repeating="3.7 GiB" memory.weights.nonrepeating="411.0 MiB" memory.graph.full="560.0 MiB" memory.graph.partial="677.5 MiB"
time=2025-10-21T21:02:48.939-04:00 level=INFO source=sched.go:450 msg="Load failed" model=C:\Users\frost\.ollama\models\blobs\sha256-6a0746a1ec1aef3e7ec53868f220ff6e389f6f8ef87a01d77c96807de94ca2aa error="context canceled"
[GIN] 2025/10/21 - 21:02:48 | 499 |        37m19s |       127.0.0.1 | POST     "/api/generate"
llama_model_loader: loaded meta data with 22 key-value pairs and 291 tensors from C:\Users\frost\.ollama\models\blobs\sha256-6a0746a1ec1aef3e7ec53868f220ff6e389f6f8ef87a01d77c96807de94ca2aa (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = llama
llama_model_loader: - kv   1:                               general.name str              = Meta-Llama-3-8B-Instruct
llama_model_loader: - kv   2:                          llama.block_count u32              = 32
llama_model_loader: - kv   3:                       llama.context_length u32              = 8192
llama_model_loader: - kv   4:                     llama.embedding_length u32              = 4096
llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 14336
llama_model_loader: - kv   6:                 llama.attention.head_count u32              = 32
llama_model_loader: - kv   7:              llama.attention.head_count_kv u32              = 8
llama_model_loader: - kv   8:                       llama.rope.freq_base f32              = 500000.000000
llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010
llama_model_loader: - kv  10:                          general.file_type u32              = 2
llama_model_loader: - kv  11:                           llama.vocab_size u32              = 128256
llama_model_loader: - kv  12:                 llama.rope.dimension_count u32              = 128
llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = llama-bpe
llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,128256]  = ["!", "\"", "#", "$", "%", "&", "'", ...
llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,128256]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,280147]  = ["Ġ Ġ", "Ġ ĠĠĠ", "ĠĠ ĠĠ", "...
llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 128000
llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 128009
llama_model_loader: - kv  20:                    tokenizer.chat_template str              = {% set loop_messages = messages %}{% ...
llama_model_loader: - kv  21:               general.quantization_version u32              = 2
llama_model_loader: - type  f32:   65 tensors
llama_model_loader: - type q4_0:  225 tensors
llama_model_loader: - type q6_K:    1 tensors
print_info: file format = GGUF V3 (latest)
print_info: file type   = Q4_0
print_info: file size   = 4.33 GiB (4.64 BPW) 
load: printing all EOG tokens:
load:   - 128001 ('<|end_of_text|>')
load:   - 128009 ('<|eot_id|>')
load: special tokens cache size = 256
load: token to piece cache size = 0.8000 MB
print_info: arch             = llama
print_info: vocab_only       = 1
print_info: model type       = ?B
print_info: model params     = 8.03 B
print_info: general.name     = Meta-Llama-3-8B-Instruct
print_info: vocab type       = BPE
print_info: n_vocab          = 128256
print_info: n_merges         = 280147
print_info: BOS token        = 128000 '<|begin_of_text|>'
print_info: EOS token        = 128009 '<|eot_id|>'
print_info: EOT token        = 128001 '<|end_of_text|>'
print_info: LF token         = 198 'Ċ'
print_info: EOG token        = 128001 '<|end_of_text|>'
print_info: EOG token        = 128009 '<|eot_id|>'
print_info: max token length = 256
llama_model_load: vocab only - skipping tensors
time=2025-10-21T21:02:49.636-04:00 level=INFO source=cpu_windows.go:139 msg=packages count=1
time=2025-10-21T21:02:49.636-04:00 level=INFO source=cpu_windows.go:155 msg="efficiency cores detected" maxEfficiencyClass=1
time=2025-10-21T21:02:49.636-04:00 level=INFO source=cpu_windows.go:186 msg="" package=0 cores=12 efficiency=4 threads=20
time=2025-10-21T21:02:49.638-04:00 level=INFO source=server.go:400 msg="starting runner" cmd="C:\\Users\\frost\\AppData\\Local\\Programs\\Ollama\\ollama.exe runner --model C:\\Users\\frost\\.ollama\\models\\blobs\\sha256-6a0746a1ec1aef3e7ec53868f220ff6e389f6f8ef87a01d77c96807de94ca2aa --port 49726"
time=2025-10-21T21:02:49.642-04:00 level=INFO source=cpu_windows.go:139 msg=packages count=1
time=2025-10-21T21:02:49.642-04:00 level=INFO source=cpu_windows.go:155 msg="efficiency cores detected" maxEfficiencyClass=1
time=2025-10-21T21:02:49.642-04:00 level=INFO source=cpu_windows.go:186 msg="" package=0 cores=12 efficiency=4 threads=20
time=2025-10-21T21:02:49.642-04:00 level=INFO source=server.go:505 msg="system memory" total="63.8 GiB" free="46.8 GiB" free_swap="46.8 GiB"
time=2025-10-21T21:02:49.642-04:00 level=INFO source=memory.go:36 msg="new model will fit in available VRAM across minimum required GPUs, loading" model=C:\Users\frost\.ollama\models\blobs\sha256-6a0746a1ec1aef3e7ec53868f220ff6e389f6f8ef87a01d77c96807de94ca2aa library=CUDA parallel=1 required="6.2 GiB" gpus=1
time=2025-10-21T21:02:49.642-04:00 level=INFO source=server.go:545 msg=offload library=CUDA layers.requested=-1 layers.model=33 layers.offload=33 layers.split=[33] memory.available="[10.6 GiB]" memory.gpu_overhead="0 B" memory.required.full="6.2 GiB" memory.required.partial="6.2 GiB" memory.required.kv="1.0 GiB" memory.required.allocations="[6.2 GiB]" memory.weights.total="4.1 GiB" memory.weights.repeating="3.7 GiB" memory.weights.nonrepeating="411.0 MiB" memory.graph.full="560.0 MiB" memory.graph.partial="677.5 MiB"
time=2025-10-21T21:02:49.690-04:00 level=INFO source=runner.go:893 msg="starting go runner"
load_backend: loaded CPU backend from C:\Users\frost\AppData\Local\Programs\Ollama\lib\ollama\ggml-cpu-alderlake.dll
ggml_cuda_init: GGML_CUDA_FORCE_MMQ:    no
ggml_cuda_init: GGML_CUDA_FORCE_CUBLAS: no
ggml_cuda_init: found 1 CUDA devices:
  Device 0: NVIDIA GeForce RTX 5070, compute capability 12.0, VMM: yes, ID: GPU-b28645fa-aaf6-86a1-ab1b-ef8d61df29f2
load_backend: loaded CUDA backend from C:\Users\frost\AppData\Local\Programs\Ollama\lib\ollama\cuda_v12\ggml-cuda.dll
time=2025-10-21T21:02:49.843-04:00 level=INFO source=ggml.go:104 msg=system CPU.0.SSE3=1 CPU.0.SSSE3=1 CPU.0.AVX=1 CPU.0.AVX_VNNI=1 CPU.0.AVX2=1 CPU.0.F16C=1 CPU.0.FMA=1 CPU.0.BMI2=1 CPU.0.LLAMAFILE=1 CPU.1.LLAMAFILE=1 CUDA.0.ARCHS=500,520,600,610,700,750,800,860,870,890,900,1200 CUDA.0.USE_GRAPHS=1 CUDA.0.PEER_MAX_BATCH_SIZE=128 compiler=cgo(clang)
time=2025-10-21T21:02:49.844-04:00 level=INFO source=runner.go:929 msg="Server listening on 127.0.0.1:49726"
time=2025-10-21T21:02:49.848-04:00 level=INFO source=runner.go:828 msg=load request="{Operation:commit LoraPath:[] Parallel:1 BatchSize:512 FlashAttention:false KvSize:8192 KvCacheType: NumThreads:8 GPULayers:33[ID:GPU-b28645fa-aaf6-86a1-ab1b-ef8d61df29f2 Layers:33(0..32)] MultiUserCache:false ProjectorPath: MainGPU:0 UseMmap:false}"
time=2025-10-21T21:02:49.849-04:00 level=INFO source=server.go:1272 msg="waiting for llama runner to start responding"
time=2025-10-21T21:02:49.852-04:00 level=INFO source=server.go:1306 msg="waiting for server to become available" status="llm server loading model"
ggml_backend_cuda_device_get_memory utilizing NVML memory reporting free: 11432259584 total: 12820938752
llama_model_load_from_file_impl: using device CUDA0 (NVIDIA GeForce RTX 5070) (0000:01:00.0) - 10902 MiB free
llama_model_loader: loaded meta data with 22 key-value pairs and 291 tensors from C:\Users\frost\.ollama\models\blobs\sha256-6a0746a1ec1aef3e7ec53868f220ff6e389f6f8ef87a01d77c96807de94ca2aa (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = llama
llama_model_loader: - kv   1:                               general.name str              = Meta-Llama-3-8B-Instruct
llama_model_loader: - kv   2:                          llama.block_count u32              = 32
llama_model_loader: - kv   3:                       llama.context_length u32              = 8192
llama_model_loader: - kv   4:                     llama.embedding_length u32              = 4096
llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 14336
llama_model_loader: - kv   6:                 llama.attention.head_count u32              = 32
llama_model_loader: - kv   7:              llama.attention.head_count_kv u32              = 8
llama_model_loader: - kv   8:                       llama.rope.freq_base f32              = 500000.000000
llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010
llama_model_loader: - kv  10:                          general.file_type u32              = 2
llama_model_loader: - kv  11:                           llama.vocab_size u32              = 128256
llama_model_loader: - kv  12:                 llama.rope.dimension_count u32              = 128
llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = llama-bpe
llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,128256]  = ["!", "\"", "#", "$", "%", "&", "'", ...
llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,128256]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,280147]  = ["Ġ Ġ", "Ġ ĠĠĠ", "ĠĠ ĠĠ", "...
llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 128000
llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 128009
llama_model_loader: - kv  20:                    tokenizer.chat_template str              = {% set loop_messages = messages %}{% ...
llama_model_loader: - kv  21:               general.quantization_version u32              = 2
llama_model_loader: - type  f32:   65 tensors
llama_model_loader: - type q4_0:  225 tensors
llama_model_loader: - type q6_K:    1 tensors
print_info: file format = GGUF V3 (latest)
print_info: file type   = Q4_0
print_info: file size   = 4.33 GiB (4.64 BPW) 
load: printing all EOG tokens:
load:   - 128001 ('<|end_of_text|>')
load:   - 128009 ('<|eot_id|>')
load: special tokens cache size = 256
load: token to piece cache size = 0.8000 MB
print_info: arch             = llama
print_info: vocab_only       = 0
print_info: n_ctx_train      = 8192
print_info: n_embd           = 4096
print_info: n_layer          = 32
print_info: n_head           = 32
print_info: n_head_kv        = 8
print_info: n_rot            = 128
print_info: n_swa            = 0
print_info: is_swa_any       = 0
print_info: n_embd_head_k    = 128
print_info: n_embd_head_v    = 128
print_info: n_gqa            = 4
print_info: n_embd_k_gqa     = 1024
print_info: n_embd_v_gqa     = 1024
print_info: f_norm_eps       = 0.0e+00
print_info: f_norm_rms_eps   = 1.0e-05
print_info: f_clamp_kqv      = 0.0e+00
print_info: f_max_alibi_bias = 0.0e+00
print_info: f_logit_scale    = 0.0e+00
print_info: f_attn_scale     = 0.0e+00
print_info: n_ff             = 14336
print_info: n_expert         = 0
print_info: n_expert_used    = 0
print_info: causal attn      = 1
print_info: pooling type     = 0
print_info: rope type        = 0
print_info: rope scaling     = linear
print_info: freq_base_train  = 500000.0
print_info: freq_scale_train = 1
print_info: n_ctx_orig_yarn  = 8192
print_info: rope_finetuned   = unknown
print_info: model type       = 8B
print_info: model params     = 8.03 B
print_info: general.name     = Meta-Llama-3-8B-Instruct
print_info: vocab type       = BPE
print_info: n_vocab          = 128256
print_info: n_merges         = 280147
print_info: BOS token        = 128000 '<|begin_of_text|>'
print_info: EOS token        = 128009 '<|eot_id|>'
print_info: EOT token        = 128001 '<|end_of_text|>'
print_info: LF token         = 198 'Ċ'
print_info: EOG token        = 128001 '<|end_of_text|>'
print_info: EOG token        = 128009 '<|eot_id|>'
print_info: max token length = 256
load_tensors: loading model tensors, this can take a while... (mmap = false)
load_tensors: offloading 32 repeating layers to GPU
load_tensors: offloading output layer to GPU
load_tensors: offloaded 33/33 layers to GPU
load_tensors:    CUDA_Host model buffer size =   281.81 MiB
load_tensors:        CUDA0 model buffer size =  4155.99 MiB
llama_init_from_model: model default pooling_type is [0], but [-1] was specified
llama_context: constructing llama_context
llama_context: n_seq_max     = 1
llama_context: n_ctx         = 8192
llama_context: n_ctx_per_seq = 8192
llama_context: n_batch       = 512
llama_context: n_ubatch      = 512
llama_context: causal_attn   = 1
llama_context: flash_attn    = disabled
llama_context: kv_unified    = false
llama_context: freq_base     = 500000.0
llama_context: freq_scale    = 1
llama_context:  CUDA_Host  output buffer size =     0.50 MiB
llama_kv_cache:      CUDA0 KV buffer size =  1024.00 MiB
llama_kv_cache: size = 1024.00 MiB (  8192 cells,  32 layers,  1/1 seqs), K (f16):  512.00 MiB, V (f16):  512.00 MiB
llama_context:      CUDA0 compute buffer size =   564.01 MiB
llama_context:  CUDA_Host compute buffer size =    28.01 MiB
llama_context: graph nodes  = 1158
llama_context: graph splits = 2
time=2025-10-21T21:02:52.124-04:00 level=INFO source=server.go:1310 msg="llama runner started in 2.49 seconds"
time=2025-10-21T21:02:52.124-04:00 level=INFO source=sched.go:482 msg="loaded runners" count=1
time=2025-10-21T21:02:52.124-04:00 level=INFO source=server.go:1272 msg="waiting for llama runner to start responding"
time=2025-10-21T21:02:52.124-04:00 level=INFO source=server.go:1310 msg="llama runner started in 2.49 seconds"
[GIN] 2025/10/21 - 21:02:52 | 200 |        31m31s |       127.0.0.1 | POST     "/api/generate"
llama_model_loader: loaded meta data with 22 key-value pairs and 291 tensors from C:\Users\frost\.ollama\models\blobs\sha256-6a0746a1ec1aef3e7ec53868f220ff6e389f6f8ef87a01d77c96807de94ca2aa (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = llama
llama_model_loader: - kv   1:                               general.name str              = Meta-Llama-3-8B-Instruct
llama_model_loader: - kv   2:                          llama.block_count u32              = 32
llama_model_loader: - kv   3:                       llama.context_length u32              = 8192
llama_model_loader: - kv   4:                     llama.embedding_length u32              = 4096
llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 14336
llama_model_loader: - kv   6:                 llama.attention.head_count u32              = 32
llama_model_loader: - kv   7:              llama.attention.head_count_kv u32              = 8
llama_model_loader: - kv   8:                       llama.rope.freq_base f32              = 500000.000000
llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010
llama_model_loader: - kv  10:                          general.file_type u32              = 2
llama_model_loader: - kv  11:                           llama.vocab_size u32              = 128256
llama_model_loader: - kv  12:                 llama.rope.dimension_count u32              = 128
llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = llama-bpe
llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,128256]  = ["!", "\"", "#", "$", "%", "&", "'", ...
llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,128256]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,280147]  = ["Ġ Ġ", "Ġ ĠĠĠ", "ĠĠ ĠĠ", "...
llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 128000
llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 128009
llama_model_loader: - kv  20:                    tokenizer.chat_template str              = {% set loop_messages = messages %}{% ...
llama_model_loader: - kv  21:               general.quantization_version u32              = 2
llama_model_loader: - type  f32:   65 tensors
llama_model_loader: - type q4_0:  225 tensors
llama_model_loader: - type q6_K:    1 tensors
print_info: file format = GGUF V3 (latest)
print_info: file type   = Q4_0
print_info: file size   = 4.33 GiB (4.64 BPW) 
load: printing all EOG tokens:
load:   - 128001 ('<|end_of_text|>')
load:   - 128009 ('<|eot_id|>')
load: special tokens cache size = 256
load: token to piece cache size = 0.8000 MB
print_info: arch             = llama
print_info: vocab_only       = 1
print_info: model type       = ?B
print_info: model params     = 8.03 B
print_info: general.name     = Meta-Llama-3-8B-Instruct
print_info: vocab type       = BPE
print_info: n_vocab          = 128256
print_info: n_merges         = 280147
print_info: BOS token        = 128000 '<|begin_of_text|>'
print_info: EOS token        = 128009 '<|eot_id|>'
print_info: EOT token        = 128001 '<|end_of_text|>'
print_info: LF token         = 198 'Ċ'
print_info: EOG token        = 128001 '<|end_of_text|>'
print_info: EOG token        = 128009 '<|eot_id|>'
print_info: max token length = 256
llama_model_load: vocab only - skipping tensors
time=2025-10-22T12:16:43.808-04:00 level=INFO source=cpu_windows.go:139 msg=packages count=1
time=2025-10-22T12:16:43.808-04:00 level=INFO source=cpu_windows.go:155 msg="efficiency cores detected" maxEfficiencyClass=1
time=2025-10-22T12:16:43.808-04:00 level=INFO source=cpu_windows.go:186 msg="" package=0 cores=12 efficiency=4 threads=20
time=2025-10-22T12:16:43.809-04:00 level=INFO source=server.go:400 msg="starting runner" cmd="C:\\Users\\frost\\AppData\\Local\\Programs\\Ollama\\ollama.exe runner --model C:\\Users\\frost\\.ollama\\models\\blobs\\sha256-6a0746a1ec1aef3e7ec53868f220ff6e389f6f8ef87a01d77c96807de94ca2aa --port 49723"
time=2025-10-22T12:16:43.814-04:00 level=INFO source=cpu_windows.go:139 msg=packages count=1
time=2025-10-22T12:16:43.814-04:00 level=INFO source=cpu_windows.go:155 msg="efficiency cores detected" maxEfficiencyClass=1
time=2025-10-22T12:16:43.814-04:00 level=INFO source=cpu_windows.go:186 msg="" package=0 cores=12 efficiency=4 threads=20
time=2025-10-22T12:16:43.814-04:00 level=INFO source=server.go:505 msg="system memory" total="63.8 GiB" free="45.9 GiB" free_swap="45.2 GiB"
time=2025-10-22T12:16:43.814-04:00 level=INFO source=memory.go:36 msg="new model will fit in available VRAM across minimum required GPUs, loading" model=C:\Users\frost\.ollama\models\blobs\sha256-6a0746a1ec1aef3e7ec53868f220ff6e389f6f8ef87a01d77c96807de94ca2aa library=CUDA parallel=1 required="5.4 GiB" gpus=1
time=2025-10-22T12:16:43.814-04:00 level=INFO source=server.go:545 msg=offload library=CUDA layers.requested=-1 layers.model=33 layers.offload=33 layers.split=[33] memory.available="[10.3 GiB]" memory.gpu_overhead="0 B" memory.required.full="5.4 GiB" memory.required.partial="5.4 GiB" memory.required.kv="512.0 MiB" memory.required.allocations="[5.4 GiB]" memory.weights.total="4.1 GiB" memory.weights.repeating="3.7 GiB" memory.weights.nonrepeating="411.0 MiB" memory.graph.full="296.0 MiB" memory.graph.partial="677.5 MiB"
time=2025-10-22T12:16:43.854-04:00 level=INFO source=runner.go:893 msg="starting go runner"
load_backend: loaded CPU backend from C:\Users\frost\AppData\Local\Programs\Ollama\lib\ollama\ggml-cpu-alderlake.dll
ggml_cuda_init: GGML_CUDA_FORCE_MMQ:    no
ggml_cuda_init: GGML_CUDA_FORCE_CUBLAS: no
ggml_cuda_init: found 1 CUDA devices:
  Device 0: NVIDIA GeForce RTX 5070, compute capability 12.0, VMM: yes, ID: GPU-b28645fa-aaf6-86a1-ab1b-ef8d61df29f2
load_backend: loaded CUDA backend from C:\Users\frost\AppData\Local\Programs\Ollama\lib\ollama\cuda_v12\ggml-cuda.dll
time=2025-10-22T12:16:43.975-04:00 level=INFO source=ggml.go:104 msg=system CPU.0.SSE3=1 CPU.0.SSSE3=1 CPU.0.AVX=1 CPU.0.AVX_VNNI=1 CPU.0.AVX2=1 CPU.0.F16C=1 CPU.0.FMA=1 CPU.0.BMI2=1 CPU.0.LLAMAFILE=1 CPU.1.LLAMAFILE=1 CUDA.0.ARCHS=500,520,600,610,700,750,800,860,870,890,900,1200 CUDA.0.USE_GRAPHS=1 CUDA.0.PEER_MAX_BATCH_SIZE=128 compiler=cgo(clang)
time=2025-10-22T12:16:43.975-04:00 level=INFO source=runner.go:929 msg="Server listening on 127.0.0.1:49723"
time=2025-10-22T12:16:43.977-04:00 level=INFO source=runner.go:828 msg=load request="{Operation:commit LoraPath:[] Parallel:1 BatchSize:512 FlashAttention:false KvSize:4096 KvCacheType: NumThreads:8 GPULayers:33[ID:GPU-b28645fa-aaf6-86a1-ab1b-ef8d61df29f2 Layers:33(0..32)] MultiUserCache:false ProjectorPath: MainGPU:0 UseMmap:false}"
time=2025-10-22T12:16:43.978-04:00 level=INFO source=server.go:1272 msg="waiting for llama runner to start responding"
time=2025-10-22T12:16:43.978-04:00 level=INFO source=server.go:1306 msg="waiting for server to become available" status="llm server loading model"
ggml_backend_cuda_device_get_memory utilizing NVML memory reporting free: 11032993792 total: 12820938752
llama_model_load_from_file_impl: using device CUDA0 (NVIDIA GeForce RTX 5070) (0000:01:00.0) - 10521 MiB free
llama_model_loader: loaded meta data with 22 key-value pairs and 291 tensors from C:\Users\frost\.ollama\models\blobs\sha256-6a0746a1ec1aef3e7ec53868f220ff6e389f6f8ef87a01d77c96807de94ca2aa (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = llama
llama_model_loader: - kv   1:                               general.name str              = Meta-Llama-3-8B-Instruct
llama_model_loader: - kv   2:                          llama.block_count u32              = 32
llama_model_loader: - kv   3:                       llama.context_length u32              = 8192
llama_model_loader: - kv   4:                     llama.embedding_length u32              = 4096
llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 14336
llama_model_loader: - kv   6:                 llama.attention.head_count u32              = 32
llama_model_loader: - kv   7:              llama.attention.head_count_kv u32              = 8
llama_model_loader: - kv   8:                       llama.rope.freq_base f32              = 500000.000000
llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010
llama_model_loader: - kv  10:                          general.file_type u32              = 2
llama_model_loader: - kv  11:                           llama.vocab_size u32              = 128256
llama_model_loader: - kv  12:                 llama.rope.dimension_count u32              = 128
llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = llama-bpe
llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,128256]  = ["!", "\"", "#", "$", "%", "&", "'", ...
llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,128256]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,280147]  = ["Ġ Ġ", "Ġ ĠĠĠ", "ĠĠ ĠĠ", "...
llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 128000
llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 128009
llama_model_loader: - kv  20:                    tokenizer.chat_template str              = {% set loop_messages = messages %}{% ...
llama_model_loader: - kv  21:               general.quantization_version u32              = 2
llama_model_loader: - type  f32:   65 tensors
llama_model_loader: - type q4_0:  225 tensors
llama_model_loader: - type q6_K:    1 tensors
print_info: file format = GGUF V3 (latest)
print_info: file type   = Q4_0
print_info: file size   = 4.33 GiB (4.64 BPW) 
load: printing all EOG tokens:
load:   - 128001 ('<|end_of_text|>')
load:   - 128009 ('<|eot_id|>')
load: special tokens cache size = 256
load: token to piece cache size = 0.8000 MB
print_info: arch             = llama
print_info: vocab_only       = 0
print_info: n_ctx_train      = 8192
print_info: n_embd           = 4096
print_info: n_layer          = 32
print_info: n_head           = 32
print_info: n_head_kv        = 8
print_info: n_rot            = 128
print_info: n_swa            = 0
print_info: is_swa_any       = 0
print_info: n_embd_head_k    = 128
print_info: n_embd_head_v    = 128
print_info: n_gqa            = 4
print_info: n_embd_k_gqa     = 1024
print_info: n_embd_v_gqa     = 1024
print_info: f_norm_eps       = 0.0e+00
print_info: f_norm_rms_eps   = 1.0e-05
print_info: f_clamp_kqv      = 0.0e+00
print_info: f_max_alibi_bias = 0.0e+00
print_info: f_logit_scale    = 0.0e+00
print_info: f_attn_scale     = 0.0e+00
print_info: n_ff             = 14336
print_info: n_expert         = 0
print_info: n_expert_used    = 0
print_info: causal attn      = 1
print_info: pooling type     = 0
print_info: rope type        = 0
print_info: rope scaling     = linear
print_info: freq_base_train  = 500000.0
print_info: freq_scale_train = 1
print_info: n_ctx_orig_yarn  = 8192
print_info: rope_finetuned   = unknown
print_info: model type       = 8B
print_info: model params     = 8.03 B
print_info: general.name     = Meta-Llama-3-8B-Instruct
print_info: vocab type       = BPE
print_info: n_vocab          = 128256
print_info: n_merges         = 280147
print_info: BOS token        = 128000 '<|begin_of_text|>'
print_info: EOS token        = 128009 '<|eot_id|>'
print_info: EOT token        = 128001 '<|end_of_text|>'
print_info: LF token         = 198 'Ċ'
print_info: EOG token        = 128001 '<|end_of_text|>'
print_info: EOG token        = 128009 '<|eot_id|>'
print_info: max token length = 256
load_tensors: loading model tensors, this can take a while... (mmap = false)
load_tensors: offloading 32 repeating layers to GPU
load_tensors: offloading output layer to GPU
load_tensors: offloaded 33/33 layers to GPU
load_tensors:    CUDA_Host model buffer size =   281.81 MiB
load_tensors:        CUDA0 model buffer size =  4155.99 MiB
llama_init_from_model: model default pooling_type is [0], but [-1] was specified
llama_context: constructing llama_context
llama_context: n_seq_max     = 1
llama_context: n_ctx         = 4096
llama_context: n_ctx_per_seq = 4096
llama_context: n_batch       = 512
llama_context: n_ubatch      = 512
llama_context: causal_attn   = 1
llama_context: flash_attn    = disabled
llama_context: kv_unified    = false
llama_context: freq_base     = 500000.0
llama_context: freq_scale    = 1
llama_context: n_ctx_per_seq (4096) < n_ctx_train (8192) -- the full capacity of the model will not be utilized
llama_context:  CUDA_Host  output buffer size =     0.50 MiB
llama_kv_cache:      CUDA0 KV buffer size =   512.00 MiB
llama_kv_cache: size =  512.00 MiB (  4096 cells,  32 layers,  1/1 seqs), K (f16):  256.00 MiB, V (f16):  256.00 MiB
llama_context:      CUDA0 compute buffer size =   300.01 MiB
llama_context:  CUDA_Host compute buffer size =    20.01 MiB
llama_context: graph nodes  = 1158
llama_context: graph splits = 2
time=2025-10-22T12:16:47.744-04:00 level=INFO source=server.go:1310 msg="llama runner started in 3.93 seconds"
time=2025-10-22T12:16:47.744-04:00 level=INFO source=sched.go:482 msg="loaded runners" count=1
time=2025-10-22T12:16:47.744-04:00 level=INFO source=server.go:1272 msg="waiting for llama runner to start responding"
time=2025-10-22T12:16:47.745-04:00 level=INFO source=server.go:1310 msg="llama runner started in 3.94 seconds"
[GIN] 2025/10/22 - 12:16:47 | 200 |    6.7438531s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/10/22 - 20:10:19 | 200 |     36.7963ms |       127.0.0.1 | GET      "/api/tags"
[GIN] 2025/10/22 - 20:10:54 | 200 |       1.232ms |       127.0.0.1 | GET      "/api/tags"
[GIN] 2025/10/22 - 20:11:04 | 200 |       2.222ms |       127.0.0.1 | GET      "/api/tags"
[GIN] 2025/10/22 - 20:11:05 | 200 |            0s |       127.0.0.1 | HEAD     "/"
[GIN] 2025/10/22 - 20:12:28 | 200 |       576.9µs |       127.0.0.1 | GET      "/api/tags"
[GIN] 2025/10/22 - 20:12:28 | 200 |            0s |       127.0.0.1 | HEAD     "/"
[GIN] 2025/10/22 - 20:15:13 | 200 |       505.6µs |       127.0.0.1 | GET      "/api/tags"
time=2025-10-22T20:15:16.102-04:00 level=WARN source=types.go:753 msg="invalid option provided" option=max_tokens
llama_model_loader: loaded meta data with 22 key-value pairs and 291 tensors from C:\Users\frost\.ollama\models\blobs\sha256-6a0746a1ec1aef3e7ec53868f220ff6e389f6f8ef87a01d77c96807de94ca2aa (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = llama
llama_model_loader: - kv   1:                               general.name str              = Meta-Llama-3-8B-Instruct
llama_model_loader: - kv   2:                          llama.block_count u32              = 32
llama_model_loader: - kv   3:                       llama.context_length u32              = 8192
llama_model_loader: - kv   4:                     llama.embedding_length u32              = 4096
llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 14336
llama_model_loader: - kv   6:                 llama.attention.head_count u32              = 32
llama_model_loader: - kv   7:              llama.attention.head_count_kv u32              = 8
llama_model_loader: - kv   8:                       llama.rope.freq_base f32              = 500000.000000
llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010
llama_model_loader: - kv  10:                          general.file_type u32              = 2
llama_model_loader: - kv  11:                           llama.vocab_size u32              = 128256
llama_model_loader: - kv  12:                 llama.rope.dimension_count u32              = 128
llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = llama-bpe
llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,128256]  = ["!", "\"", "#", "$", "%", "&", "'", ...
llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,128256]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,280147]  = ["Ġ Ġ", "Ġ ĠĠĠ", "ĠĠ ĠĠ", "...
llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 128000
llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 128009
llama_model_loader: - kv  20:                    tokenizer.chat_template str              = {% set loop_messages = messages %}{% ...
llama_model_loader: - kv  21:               general.quantization_version u32              = 2
llama_model_loader: - type  f32:   65 tensors
llama_model_loader: - type q4_0:  225 tensors
llama_model_loader: - type q6_K:    1 tensors
print_info: file format = GGUF V3 (latest)
print_info: file type   = Q4_0
print_info: file size   = 4.33 GiB (4.64 BPW) 
load: printing all EOG tokens:
load:   - 128001 ('<|end_of_text|>')
load:   - 128009 ('<|eot_id|>')
load: special tokens cache size = 256
load: token to piece cache size = 0.8000 MB
print_info: arch             = llama
print_info: vocab_only       = 1
print_info: model type       = ?B
print_info: model params     = 8.03 B
print_info: general.name     = Meta-Llama-3-8B-Instruct
print_info: vocab type       = BPE
print_info: n_vocab          = 128256
print_info: n_merges         = 280147
print_info: BOS token        = 128000 '<|begin_of_text|>'
print_info: EOS token        = 128009 '<|eot_id|>'
print_info: EOT token        = 128001 '<|end_of_text|>'
print_info: LF token         = 198 'Ċ'
print_info: EOG token        = 128001 '<|end_of_text|>'
print_info: EOG token        = 128009 '<|eot_id|>'
print_info: max token length = 256
llama_model_load: vocab only - skipping tensors
time=2025-10-22T20:15:18.221-04:00 level=INFO source=cpu_windows.go:139 msg=packages count=1
time=2025-10-22T20:15:18.221-04:00 level=INFO source=cpu_windows.go:155 msg="efficiency cores detected" maxEfficiencyClass=1
time=2025-10-22T20:15:18.221-04:00 level=INFO source=cpu_windows.go:186 msg="" package=0 cores=12 efficiency=4 threads=20
time=2025-10-22T20:15:18.222-04:00 level=INFO source=server.go:400 msg="starting runner" cmd="C:\\Users\\frost\\AppData\\Local\\Programs\\Ollama\\ollama.exe runner --model C:\\Users\\frost\\.ollama\\models\\blobs\\sha256-6a0746a1ec1aef3e7ec53868f220ff6e389f6f8ef87a01d77c96807de94ca2aa --port 51527"
time=2025-10-22T20:15:18.225-04:00 level=INFO source=cpu_windows.go:139 msg=packages count=1
time=2025-10-22T20:15:18.225-04:00 level=INFO source=cpu_windows.go:155 msg="efficiency cores detected" maxEfficiencyClass=1
time=2025-10-22T20:15:18.225-04:00 level=INFO source=cpu_windows.go:186 msg="" package=0 cores=12 efficiency=4 threads=20
time=2025-10-22T20:15:18.225-04:00 level=INFO source=server.go:505 msg="system memory" total="63.8 GiB" free="43.0 GiB" free_swap="41.4 GiB"
time=2025-10-22T20:15:18.225-04:00 level=INFO source=memory.go:36 msg="new model will fit in available VRAM across minimum required GPUs, loading" model=C:\Users\frost\.ollama\models\blobs\sha256-6a0746a1ec1aef3e7ec53868f220ff6e389f6f8ef87a01d77c96807de94ca2aa library=CUDA parallel=1 required="6.2 GiB" gpus=1
time=2025-10-22T20:15:18.226-04:00 level=INFO source=server.go:545 msg=offload library=CUDA layers.requested=-1 layers.model=33 layers.offload=33 layers.split=[33] memory.available="[10.4 GiB]" memory.gpu_overhead="0 B" memory.required.full="6.2 GiB" memory.required.partial="6.2 GiB" memory.required.kv="1.0 GiB" memory.required.allocations="[6.2 GiB]" memory.weights.total="4.1 GiB" memory.weights.repeating="3.7 GiB" memory.weights.nonrepeating="411.0 MiB" memory.graph.full="560.0 MiB" memory.graph.partial="677.5 MiB"
time=2025-10-22T20:15:18.269-04:00 level=INFO source=runner.go:893 msg="starting go runner"
load_backend: loaded CPU backend from C:\Users\frost\AppData\Local\Programs\Ollama\lib\ollama\ggml-cpu-alderlake.dll
ggml_cuda_init: GGML_CUDA_FORCE_MMQ:    no
ggml_cuda_init: GGML_CUDA_FORCE_CUBLAS: no
ggml_cuda_init: found 1 CUDA devices:
  Device 0: NVIDIA GeForce RTX 5070, compute capability 12.0, VMM: yes, ID: GPU-b28645fa-aaf6-86a1-ab1b-ef8d61df29f2
load_backend: loaded CUDA backend from C:\Users\frost\AppData\Local\Programs\Ollama\lib\ollama\cuda_v12\ggml-cuda.dll
time=2025-10-22T20:15:18.399-04:00 level=INFO source=ggml.go:104 msg=system CPU.0.SSE3=1 CPU.0.SSSE3=1 CPU.0.AVX=1 CPU.0.AVX_VNNI=1 CPU.0.AVX2=1 CPU.0.F16C=1 CPU.0.FMA=1 CPU.0.BMI2=1 CPU.0.LLAMAFILE=1 CPU.1.LLAMAFILE=1 CUDA.0.ARCHS=500,520,600,610,700,750,800,860,870,890,900,1200 CUDA.0.USE_GRAPHS=1 CUDA.0.PEER_MAX_BATCH_SIZE=128 compiler=cgo(clang)
time=2025-10-22T20:15:18.401-04:00 level=INFO source=runner.go:929 msg="Server listening on 127.0.0.1:51527"
time=2025-10-22T20:15:18.406-04:00 level=INFO source=runner.go:828 msg=load request="{Operation:commit LoraPath:[] Parallel:1 BatchSize:512 FlashAttention:false KvSize:8192 KvCacheType: NumThreads:8 GPULayers:33[ID:GPU-b28645fa-aaf6-86a1-ab1b-ef8d61df29f2 Layers:33(0..32)] MultiUserCache:false ProjectorPath: MainGPU:0 UseMmap:false}"
time=2025-10-22T20:15:18.406-04:00 level=INFO source=server.go:1272 msg="waiting for llama runner to start responding"
time=2025-10-22T20:15:18.406-04:00 level=INFO source=server.go:1306 msg="waiting for server to become available" status="llm server loading model"
ggml_backend_cuda_device_get_memory utilizing NVML memory reporting free: 11201507328 total: 12820938752
llama_model_load_from_file_impl: using device CUDA0 (NVIDIA GeForce RTX 5070) (0000:01:00.0) - 10682 MiB free
llama_model_loader: loaded meta data with 22 key-value pairs and 291 tensors from C:\Users\frost\.ollama\models\blobs\sha256-6a0746a1ec1aef3e7ec53868f220ff6e389f6f8ef87a01d77c96807de94ca2aa (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = llama
llama_model_loader: - kv   1:                               general.name str              = Meta-Llama-3-8B-Instruct
llama_model_loader: - kv   2:                          llama.block_count u32              = 32
llama_model_loader: - kv   3:                       llama.context_length u32              = 8192
llama_model_loader: - kv   4:                     llama.embedding_length u32              = 4096
llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 14336
llama_model_loader: - kv   6:                 llama.attention.head_count u32              = 32
llama_model_loader: - kv   7:              llama.attention.head_count_kv u32              = 8
llama_model_loader: - kv   8:                       llama.rope.freq_base f32              = 500000.000000
llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010
llama_model_loader: - kv  10:                          general.file_type u32              = 2
llama_model_loader: - kv  11:                           llama.vocab_size u32              = 128256
llama_model_loader: - kv  12:                 llama.rope.dimension_count u32              = 128
llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = llama-bpe
llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,128256]  = ["!", "\"", "#", "$", "%", "&", "'", ...
llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,128256]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,280147]  = ["Ġ Ġ", "Ġ ĠĠĠ", "ĠĠ ĠĠ", "...
llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 128000
llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 128009
llama_model_loader: - kv  20:                    tokenizer.chat_template str              = {% set loop_messages = messages %}{% ...
llama_model_loader: - kv  21:               general.quantization_version u32              = 2
llama_model_loader: - type  f32:   65 tensors
llama_model_loader: - type q4_0:  225 tensors
llama_model_loader: - type q6_K:    1 tensors
print_info: file format = GGUF V3 (latest)
print_info: file type   = Q4_0
print_info: file size   = 4.33 GiB (4.64 BPW) 
load: printing all EOG tokens:
load:   - 128001 ('<|end_of_text|>')
load:   - 128009 ('<|eot_id|>')
load: special tokens cache size = 256
load: token to piece cache size = 0.8000 MB
print_info: arch             = llama
print_info: vocab_only       = 0
print_info: n_ctx_train      = 8192
print_info: n_embd           = 4096
print_info: n_layer          = 32
print_info: n_head           = 32
print_info: n_head_kv        = 8
print_info: n_rot            = 128
print_info: n_swa            = 0
print_info: is_swa_any       = 0
print_info: n_embd_head_k    = 128
print_info: n_embd_head_v    = 128
print_info: n_gqa            = 4
print_info: n_embd_k_gqa     = 1024
print_info: n_embd_v_gqa     = 1024
print_info: f_norm_eps       = 0.0e+00
print_info: f_norm_rms_eps   = 1.0e-05
print_info: f_clamp_kqv      = 0.0e+00
print_info: f_max_alibi_bias = 0.0e+00
print_info: f_logit_scale    = 0.0e+00
print_info: f_attn_scale     = 0.0e+00
print_info: n_ff             = 14336
print_info: n_expert         = 0
print_info: n_expert_used    = 0
print_info: causal attn      = 1
print_info: pooling type     = 0
print_info: rope type        = 0
print_info: rope scaling     = linear
print_info: freq_base_train  = 500000.0
print_info: freq_scale_train = 1
print_info: n_ctx_orig_yarn  = 8192
print_info: rope_finetuned   = unknown
print_info: model type       = 8B
print_info: model params     = 8.03 B
print_info: general.name     = Meta-Llama-3-8B-Instruct
print_info: vocab type       = BPE
print_info: n_vocab          = 128256
print_info: n_merges         = 280147
print_info: BOS token        = 128000 '<|begin_of_text|>'
print_info: EOS token        = 128009 '<|eot_id|>'
print_info: EOT token        = 128001 '<|end_of_text|>'
print_info: LF token         = 198 'Ċ'
print_info: EOG token        = 128001 '<|end_of_text|>'
print_info: EOG token        = 128009 '<|eot_id|>'
print_info: max token length = 256
load_tensors: loading model tensors, this can take a while... (mmap = false)
load_tensors: offloading 32 repeating layers to GPU
load_tensors: offloading output layer to GPU
load_tensors: offloaded 33/33 layers to GPU
load_tensors:    CUDA_Host model buffer size =   281.81 MiB
load_tensors:        CUDA0 model buffer size =  4155.99 MiB
llama_init_from_model: model default pooling_type is [0], but [-1] was specified
llama_context: constructing llama_context
llama_context: n_seq_max     = 1
llama_context: n_ctx         = 8192
llama_context: n_ctx_per_seq = 8192
llama_context: n_batch       = 512
llama_context: n_ubatch      = 512
llama_context: causal_attn   = 1
llama_context: flash_attn    = disabled
llama_context: kv_unified    = false
llama_context: freq_base     = 500000.0
llama_context: freq_scale    = 1
llama_context:  CUDA_Host  output buffer size =     0.50 MiB
llama_kv_cache:      CUDA0 KV buffer size =  1024.00 MiB
llama_kv_cache: size = 1024.00 MiB (  8192 cells,  32 layers,  1/1 seqs), K (f16):  512.00 MiB, V (f16):  512.00 MiB
llama_context:      CUDA0 compute buffer size =   564.01 MiB
llama_context:  CUDA_Host compute buffer size =    28.01 MiB
llama_context: graph nodes  = 1158
llama_context: graph splits = 2
time=2025-10-22T20:15:22.196-04:00 level=INFO source=server.go:1310 msg="llama runner started in 3.97 seconds"
time=2025-10-22T20:15:22.196-04:00 level=INFO source=sched.go:482 msg="loaded runners" count=1
time=2025-10-22T20:15:22.196-04:00 level=INFO source=server.go:1272 msg="waiting for llama runner to start responding"
time=2025-10-22T20:15:22.196-04:00 level=INFO source=server.go:1310 msg="llama runner started in 3.97 seconds"
[GIN] 2025/10/22 - 20:15:22 | 200 |    6.9274753s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/10/22 - 20:15:43 | 200 |      2.5111ms |       127.0.0.1 | GET      "/api/tags"
time=2025-10-22T20:15:45.492-04:00 level=WARN source=types.go:753 msg="invalid option provided" option=max_tokens
[GIN] 2025/10/22 - 20:15:45 | 200 |    338.1303ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/10/22 - 20:15:58 | 200 |      1.5078ms |       127.0.0.1 | GET      "/api/tags"
time=2025-10-22T20:16:01.299-04:00 level=WARN source=types.go:753 msg="invalid option provided" option=max_tokens
[GIN] 2025/10/22 - 20:16:01 | 200 |     351.174ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/10/22 - 20:16:18 | 200 |      1.3436ms |       127.0.0.1 | GET      "/api/tags"
time=2025-10-22T20:16:21.092-04:00 level=WARN source=types.go:753 msg="invalid option provided" option=max_tokens
llama_model_loader: loaded meta data with 22 key-value pairs and 291 tensors from C:\Users\frost\.ollama\models\blobs\sha256-6a0746a1ec1aef3e7ec53868f220ff6e389f6f8ef87a01d77c96807de94ca2aa (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = llama
llama_model_loader: - kv   1:                               general.name str              = Meta-Llama-3-8B-Instruct
llama_model_loader: - kv   2:                          llama.block_count u32              = 32
llama_model_loader: - kv   3:                       llama.context_length u32              = 8192
llama_model_loader: - kv   4:                     llama.embedding_length u32              = 4096
llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 14336
llama_model_loader: - kv   6:                 llama.attention.head_count u32              = 32
llama_model_loader: - kv   7:              llama.attention.head_count_kv u32              = 8
llama_model_loader: - kv   8:                       llama.rope.freq_base f32              = 500000.000000
llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010
llama_model_loader: - kv  10:                          general.file_type u32              = 2
llama_model_loader: - kv  11:                           llama.vocab_size u32              = 128256
llama_model_loader: - kv  12:                 llama.rope.dimension_count u32              = 128
llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = llama-bpe
llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,128256]  = ["!", "\"", "#", "$", "%", "&", "'", ...
llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,128256]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,280147]  = ["Ġ Ġ", "Ġ ĠĠĠ", "ĠĠ ĠĠ", "...
llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 128000
llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 128009
llama_model_loader: - kv  20:                    tokenizer.chat_template str              = {% set loop_messages = messages %}{% ...
llama_model_loader: - kv  21:               general.quantization_version u32              = 2
llama_model_loader: - type  f32:   65 tensors
llama_model_loader: - type q4_0:  225 tensors
llama_model_loader: - type q6_K:    1 tensors
print_info: file format = GGUF V3 (latest)
print_info: file type   = Q4_0
print_info: file size   = 4.33 GiB (4.64 BPW) 
load: printing all EOG tokens:
load:   - 128001 ('<|end_of_text|>')
load:   - 128009 ('<|eot_id|>')
load: special tokens cache size = 256
load: token to piece cache size = 0.8000 MB
print_info: arch             = llama
print_info: vocab_only       = 1
print_info: model type       = ?B
print_info: model params     = 8.03 B
print_info: general.name     = Meta-Llama-3-8B-Instruct
print_info: vocab type       = BPE
print_info: n_vocab          = 128256
print_info: n_merges         = 280147
print_info: BOS token        = 128000 '<|begin_of_text|>'
print_info: EOS token        = 128009 '<|eot_id|>'
print_info: EOT token        = 128001 '<|end_of_text|>'
print_info: LF token         = 198 'Ċ'
print_info: EOG token        = 128001 '<|end_of_text|>'
print_info: EOG token        = 128009 '<|eot_id|>'
print_info: max token length = 256
llama_model_load: vocab only - skipping tensors
time=2025-10-22T20:16:22.387-04:00 level=INFO source=cpu_windows.go:139 msg=packages count=1
time=2025-10-22T20:16:22.387-04:00 level=INFO source=cpu_windows.go:155 msg="efficiency cores detected" maxEfficiencyClass=1
time=2025-10-22T20:16:22.387-04:00 level=INFO source=cpu_windows.go:186 msg="" package=0 cores=12 efficiency=4 threads=20
time=2025-10-22T20:16:22.389-04:00 level=INFO source=server.go:400 msg="starting runner" cmd="C:\\Users\\frost\\AppData\\Local\\Programs\\Ollama\\ollama.exe runner --model C:\\Users\\frost\\.ollama\\models\\blobs\\sha256-6a0746a1ec1aef3e7ec53868f220ff6e389f6f8ef87a01d77c96807de94ca2aa --port 60029"
time=2025-10-22T20:16:22.393-04:00 level=INFO source=cpu_windows.go:139 msg=packages count=1
time=2025-10-22T20:16:22.393-04:00 level=INFO source=cpu_windows.go:155 msg="efficiency cores detected" maxEfficiencyClass=1
time=2025-10-22T20:16:22.393-04:00 level=INFO source=cpu_windows.go:186 msg="" package=0 cores=12 efficiency=4 threads=20
time=2025-10-22T20:16:22.393-04:00 level=INFO source=server.go:505 msg="system memory" total="63.8 GiB" free="43.1 GiB" free_swap="41.6 GiB"
time=2025-10-22T20:16:22.393-04:00 level=INFO source=memory.go:36 msg="new model will fit in available VRAM across minimum required GPUs, loading" model=C:\Users\frost\.ollama\models\blobs\sha256-6a0746a1ec1aef3e7ec53868f220ff6e389f6f8ef87a01d77c96807de94ca2aa library=CUDA parallel=1 required="5.4 GiB" gpus=1
time=2025-10-22T20:16:22.394-04:00 level=INFO source=server.go:545 msg=offload library=CUDA layers.requested=-1 layers.model=33 layers.offload=33 layers.split=[33] memory.available="[10.4 GiB]" memory.gpu_overhead="0 B" memory.required.full="5.4 GiB" memory.required.partial="5.4 GiB" memory.required.kv="512.0 MiB" memory.required.allocations="[5.4 GiB]" memory.weights.total="4.1 GiB" memory.weights.repeating="3.7 GiB" memory.weights.nonrepeating="411.0 MiB" memory.graph.full="296.0 MiB" memory.graph.partial="677.5 MiB"
time=2025-10-22T20:16:22.436-04:00 level=INFO source=runner.go:893 msg="starting go runner"
load_backend: loaded CPU backend from C:\Users\frost\AppData\Local\Programs\Ollama\lib\ollama\ggml-cpu-alderlake.dll
ggml_cuda_init: GGML_CUDA_FORCE_MMQ:    no
ggml_cuda_init: GGML_CUDA_FORCE_CUBLAS: no
ggml_cuda_init: found 1 CUDA devices:
  Device 0: NVIDIA GeForce RTX 5070, compute capability 12.0, VMM: yes, ID: GPU-b28645fa-aaf6-86a1-ab1b-ef8d61df29f2
load_backend: loaded CUDA backend from C:\Users\frost\AppData\Local\Programs\Ollama\lib\ollama\cuda_v12\ggml-cuda.dll
time=2025-10-22T20:16:22.562-04:00 level=INFO source=ggml.go:104 msg=system CPU.0.SSE3=1 CPU.0.SSSE3=1 CPU.0.AVX=1 CPU.0.AVX_VNNI=1 CPU.0.AVX2=1 CPU.0.F16C=1 CPU.0.FMA=1 CPU.0.BMI2=1 CPU.0.LLAMAFILE=1 CPU.1.LLAMAFILE=1 CUDA.0.ARCHS=500,520,600,610,700,750,800,860,870,890,900,1200 CUDA.0.USE_GRAPHS=1 CUDA.0.PEER_MAX_BATCH_SIZE=128 compiler=cgo(clang)
time=2025-10-22T20:16:22.564-04:00 level=INFO source=runner.go:929 msg="Server listening on 127.0.0.1:60029"
time=2025-10-22T20:16:22.569-04:00 level=INFO source=server.go:1272 msg="waiting for llama runner to start responding"
time=2025-10-22T20:16:22.569-04:00 level=INFO source=server.go:1306 msg="waiting for server to become available" status="llm server loading model"
time=2025-10-22T20:16:22.567-04:00 level=INFO source=runner.go:828 msg=load request="{Operation:commit LoraPath:[] Parallel:1 BatchSize:512 FlashAttention:false KvSize:4096 KvCacheType: NumThreads:8 GPULayers:33[ID:GPU-b28645fa-aaf6-86a1-ab1b-ef8d61df29f2 Layers:33(0..32)] MultiUserCache:false ProjectorPath: MainGPU:0 UseMmap:false}"
ggml_backend_cuda_device_get_memory utilizing NVML memory reporting free: 11200786432 total: 12820938752
llama_model_load_from_file_impl: using device CUDA0 (NVIDIA GeForce RTX 5070) (0000:01:00.0) - 10681 MiB free
llama_model_loader: loaded meta data with 22 key-value pairs and 291 tensors from C:\Users\frost\.ollama\models\blobs\sha256-6a0746a1ec1aef3e7ec53868f220ff6e389f6f8ef87a01d77c96807de94ca2aa (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = llama
llama_model_loader: - kv   1:                               general.name str              = Meta-Llama-3-8B-Instruct
llama_model_loader: - kv   2:                          llama.block_count u32              = 32
llama_model_loader: - kv   3:                       llama.context_length u32              = 8192
llama_model_loader: - kv   4:                     llama.embedding_length u32              = 4096
llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 14336
llama_model_loader: - kv   6:                 llama.attention.head_count u32              = 32
llama_model_loader: - kv   7:              llama.attention.head_count_kv u32              = 8
llama_model_loader: - kv   8:                       llama.rope.freq_base f32              = 500000.000000
llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010
llama_model_loader: - kv  10:                          general.file_type u32              = 2
llama_model_loader: - kv  11:                           llama.vocab_size u32              = 128256
llama_model_loader: - kv  12:                 llama.rope.dimension_count u32              = 128
llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = llama-bpe
llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,128256]  = ["!", "\"", "#", "$", "%", "&", "'", ...
llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,128256]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,280147]  = ["Ġ Ġ", "Ġ ĠĠĠ", "ĠĠ ĠĠ", "...
llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 128000
llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 128009
llama_model_loader: - kv  20:                    tokenizer.chat_template str              = {% set loop_messages = messages %}{% ...
llama_model_loader: - kv  21:               general.quantization_version u32              = 2
llama_model_loader: - type  f32:   65 tensors
llama_model_loader: - type q4_0:  225 tensors
llama_model_loader: - type q6_K:    1 tensors
print_info: file format = GGUF V3 (latest)
print_info: file type   = Q4_0
print_info: file size   = 4.33 GiB (4.64 BPW) 
load: printing all EOG tokens:
load:   - 128001 ('<|end_of_text|>')
load:   - 128009 ('<|eot_id|>')
load: special tokens cache size = 256
load: token to piece cache size = 0.8000 MB
print_info: arch             = llama
print_info: vocab_only       = 0
print_info: n_ctx_train      = 8192
print_info: n_embd           = 4096
print_info: n_layer          = 32
print_info: n_head           = 32
print_info: n_head_kv        = 8
print_info: n_rot            = 128
print_info: n_swa            = 0
print_info: is_swa_any       = 0
print_info: n_embd_head_k    = 128
print_info: n_embd_head_v    = 128
print_info: n_gqa            = 4
print_info: n_embd_k_gqa     = 1024
print_info: n_embd_v_gqa     = 1024
print_info: f_norm_eps       = 0.0e+00
print_info: f_norm_rms_eps   = 1.0e-05
print_info: f_clamp_kqv      = 0.0e+00
print_info: f_max_alibi_bias = 0.0e+00
print_info: f_logit_scale    = 0.0e+00
print_info: f_attn_scale     = 0.0e+00
print_info: n_ff             = 14336
print_info: n_expert         = 0
print_info: n_expert_used    = 0
print_info: causal attn      = 1
print_info: pooling type     = 0
print_info: rope type        = 0
print_info: rope scaling     = linear
print_info: freq_base_train  = 500000.0
print_info: freq_scale_train = 1
print_info: n_ctx_orig_yarn  = 8192
print_info: rope_finetuned   = unknown
print_info: model type       = 8B
print_info: model params     = 8.03 B
print_info: general.name     = Meta-Llama-3-8B-Instruct
print_info: vocab type       = BPE
print_info: n_vocab          = 128256
print_info: n_merges         = 280147
print_info: BOS token        = 128000 '<|begin_of_text|>'
print_info: EOS token        = 128009 '<|eot_id|>'
print_info: EOT token        = 128001 '<|end_of_text|>'
print_info: LF token         = 198 'Ċ'
print_info: EOG token        = 128001 '<|end_of_text|>'
print_info: EOG token        = 128009 '<|eot_id|>'
print_info: max token length = 256
load_tensors: loading model tensors, this can take a while... (mmap = false)
load_tensors: offloading 32 repeating layers to GPU
load_tensors: offloading output layer to GPU
load_tensors: offloaded 33/33 layers to GPU
load_tensors:    CUDA_Host model buffer size =   281.81 MiB
load_tensors:        CUDA0 model buffer size =  4155.99 MiB
llama_init_from_model: model default pooling_type is [0], but [-1] was specified
llama_context: constructing llama_context
llama_context: n_seq_max     = 1
llama_context: n_ctx         = 4096
llama_context: n_ctx_per_seq = 4096
llama_context: n_batch       = 512
llama_context: n_ubatch      = 512
llama_context: causal_attn   = 1
llama_context: flash_attn    = disabled
llama_context: kv_unified    = false
llama_context: freq_base     = 500000.0
llama_context: freq_scale    = 1
llama_context: n_ctx_per_seq (4096) < n_ctx_train (8192) -- the full capacity of the model will not be utilized
llama_context:  CUDA_Host  output buffer size =     0.50 MiB
llama_kv_cache:      CUDA0 KV buffer size =   512.00 MiB
llama_kv_cache: size =  512.00 MiB (  4096 cells,  32 layers,  1/1 seqs), K (f16):  256.00 MiB, V (f16):  256.00 MiB
llama_context:      CUDA0 compute buffer size =   300.01 MiB
llama_context:  CUDA_Host compute buffer size =    20.01 MiB
llama_context: graph nodes  = 1158
llama_context: graph splits = 2
time=2025-10-22T20:16:24.614-04:00 level=INFO source=server.go:1310 msg="llama runner started in 2.23 seconds"
time=2025-10-22T20:16:24.614-04:00 level=INFO source=sched.go:482 msg="loaded runners" count=1
time=2025-10-22T20:16:24.614-04:00 level=INFO source=server.go:1272 msg="waiting for llama runner to start responding"
time=2025-10-22T20:16:24.614-04:00 level=INFO source=server.go:1310 msg="llama runner started in 2.23 seconds"
[GIN] 2025/10/22 - 20:16:27 | 200 |    6.4822126s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/10/22 - 20:16:48 | 200 |       505.1µs |       127.0.0.1 | GET      "/api/tags"
time=2025-10-22T20:16:50.923-04:00 level=WARN source=types.go:753 msg="invalid option provided" option=max_tokens
[GIN] 2025/10/22 - 20:16:54 | 200 |    3.4801779s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/10/22 - 20:17:10 | 200 |      2.6562ms |       127.0.0.1 | GET      "/api/tags"
time=2025-10-22T20:17:13.404-04:00 level=WARN source=types.go:753 msg="invalid option provided" option=max_tokens
[GIN] 2025/10/22 - 20:17:13 | 200 |    504.3514ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/10/22 - 20:17:28 | 200 |      1.9249ms |       127.0.0.1 | GET      "/api/tags"
time=2025-10-22T20:17:31.605-04:00 level=WARN source=types.go:753 msg="invalid option provided" option=max_tokens
time=2025-10-22T20:17:31.817-04:00 level=INFO source=sched.go:545 msg="updated VRAM based on existing loaded models" gpu=GPU-b28645fa-aaf6-86a1-ab1b-ef8d61df29f2 library=CUDA total="11.9 GiB" available="5.3 GiB"
llama_model_loader: loaded meta data with 25 key-value pairs and 291 tensors from C:\Users\frost\.ollama\models\blobs\sha256-f5074b1221da0f5a2910d33b642efa5b9eb58cfdddca1c79e16d7ad28aa2b31f (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = llama
llama_model_loader: - kv   1:                               general.name str              = Mistral-7B-Instruct-v0.3
llama_model_loader: - kv   2:                          llama.block_count u32              = 32
llama_model_loader: - kv   3:                       llama.context_length u32              = 32768
llama_model_loader: - kv   4:                     llama.embedding_length u32              = 4096
llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 14336
llama_model_loader: - kv   6:                 llama.attention.head_count u32              = 32
llama_model_loader: - kv   7:              llama.attention.head_count_kv u32              = 8
llama_model_loader: - kv   8:                       llama.rope.freq_base f32              = 1000000.000000
llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010
llama_model_loader: - kv  10:                          general.file_type u32              = 15
llama_model_loader: - kv  11:                           llama.vocab_size u32              = 32768
llama_model_loader: - kv  12:                 llama.rope.dimension_count u32              = 128
llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = llama
llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = default
llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,32768]   = ["<unk>", "<s>", "</s>", "[INST]", "[...
llama_model_loader: - kv  16:                      tokenizer.ggml.scores arr[f32,32768]   = [0.000000, 0.000000, 0.000000, 0.0000...
llama_model_loader: - kv  17:                  tokenizer.ggml.token_type arr[i32,32768]   = [2, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, ...
llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 1
llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 2
llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
llama_model_loader: - kv  21:               tokenizer.ggml.add_bos_token bool             = true
llama_model_loader: - kv  22:               tokenizer.ggml.add_eos_token bool             = false
llama_model_loader: - kv  23:                    tokenizer.chat_template str              = {{ bos_token }}{% for message in mess...
llama_model_loader: - kv  24:               general.quantization_version u32              = 2
llama_model_loader: - type  f32:   65 tensors
llama_model_loader: - type q4_K:  193 tensors
llama_model_loader: - type q6_K:   33 tensors
print_info: file format = GGUF V3 (latest)
print_info: file type   = Q4_K - Medium
print_info: file size   = 4.07 GiB (4.83 BPW) 
load: special_eos_id is not in special_eog_ids - the tokenizer config may be incorrect
load: printing all EOG tokens:
load:   - 2 ('</s>')
load: special tokens cache size = 771
load: token to piece cache size = 0.1731 MB
print_info: arch             = llama
print_info: vocab_only       = 1
print_info: model type       = ?B
print_info: model params     = 7.25 B
print_info: general.name     = Mistral-7B-Instruct-v0.3
print_info: vocab type       = SPM
print_info: n_vocab          = 32768
print_info: n_merges         = 0
print_info: BOS token        = 1 '<s>'
print_info: EOS token        = 2 '</s>'
print_info: UNK token        = 0 '<unk>'
print_info: LF token         = 781 '<0x0A>'
print_info: EOG token        = 2 '</s>'
print_info: max token length = 48
llama_model_load: vocab only - skipping tensors
time=2025-10-22T20:17:31.876-04:00 level=INFO source=cpu_windows.go:139 msg=packages count=1
time=2025-10-22T20:17:31.876-04:00 level=INFO source=cpu_windows.go:155 msg="efficiency cores detected" maxEfficiencyClass=1
time=2025-10-22T20:17:31.876-04:00 level=INFO source=cpu_windows.go:186 msg="" package=0 cores=12 efficiency=4 threads=20
time=2025-10-22T20:17:31.878-04:00 level=INFO source=server.go:400 msg="starting runner" cmd="C:\\Users\\frost\\AppData\\Local\\Programs\\Ollama\\ollama.exe runner --model C:\\Users\\frost\\.ollama\\models\\blobs\\sha256-f5074b1221da0f5a2910d33b642efa5b9eb58cfdddca1c79e16d7ad28aa2b31f --port 55205"
time=2025-10-22T20:17:31.882-04:00 level=INFO source=cpu_windows.go:139 msg=packages count=1
time=2025-10-22T20:17:31.882-04:00 level=INFO source=cpu_windows.go:155 msg="efficiency cores detected" maxEfficiencyClass=1
time=2025-10-22T20:17:31.882-04:00 level=INFO source=cpu_windows.go:186 msg="" package=0 cores=12 efficiency=4 threads=20
time=2025-10-22T20:17:31.882-04:00 level=INFO source=server.go:505 msg="system memory" total="63.8 GiB" free="42.1 GiB" free_swap="35.2 GiB"
time=2025-10-22T20:17:31.883-04:00 level=INFO source=server.go:512 msg="model requires more memory than is currently available, evicting a model to make space" estimate.library="" estimate.layers.requested=0 estimate.layers.model=0 estimate.layers.offload=0 estimate.layers.split=[] estimate.memory.available=[] estimate.memory.gpu_overhead="0 B" estimate.memory.required.full="0 B" estimate.memory.required.partial="0 B" estimate.memory.required.kv="0 B" estimate.memory.required.allocations=[] estimate.memory.weights.total="0 B" estimate.memory.weights.repeating="0 B" estimate.memory.weights.nonrepeating="0 B" estimate.memory.graph.full="0 B" estimate.memory.graph.partial="0 B"
time=2025-10-22T20:17:31.928-04:00 level=INFO source=runner.go:893 msg="starting go runner"
load_backend: loaded CPU backend from C:\Users\frost\AppData\Local\Programs\Ollama\lib\ollama\ggml-cpu-alderlake.dll
ggml_cuda_init: GGML_CUDA_FORCE_MMQ:    no
ggml_cuda_init: GGML_CUDA_FORCE_CUBLAS: no
ggml_cuda_init: found 1 CUDA devices:
  Device 0: NVIDIA GeForce RTX 5070, compute capability 12.0, VMM: yes, ID: GPU-b28645fa-aaf6-86a1-ab1b-ef8d61df29f2
load_backend: loaded CUDA backend from C:\Users\frost\AppData\Local\Programs\Ollama\lib\ollama\cuda_v12\ggml-cuda.dll
time=2025-10-22T20:17:32.034-04:00 level=INFO source=ggml.go:104 msg=system CPU.0.SSE3=1 CPU.0.SSSE3=1 CPU.0.AVX=1 CPU.0.AVX_VNNI=1 CPU.0.AVX2=1 CPU.0.F16C=1 CPU.0.FMA=1 CPU.0.BMI2=1 CPU.0.LLAMAFILE=1 CPU.1.LLAMAFILE=1 CUDA.0.ARCHS=500,520,600,610,700,750,800,860,870,890,900,1200 CUDA.0.USE_GRAPHS=1 CUDA.0.PEER_MAX_BATCH_SIZE=128 compiler=cgo(clang)
time=2025-10-22T20:17:32.035-04:00 level=INFO source=runner.go:929 msg="Server listening on 127.0.0.1:55205"
time=2025-10-22T20:17:32.775-04:00 level=INFO source=cpu_windows.go:139 msg=packages count=1
time=2025-10-22T20:17:32.775-04:00 level=INFO source=cpu_windows.go:155 msg="efficiency cores detected" maxEfficiencyClass=1
time=2025-10-22T20:17:32.775-04:00 level=INFO source=cpu_windows.go:186 msg="" package=0 cores=12 efficiency=4 threads=20
time=2025-10-22T20:17:32.775-04:00 level=INFO source=server.go:505 msg="system memory" total="63.8 GiB" free="42.7 GiB" free_swap="40.9 GiB"
time=2025-10-22T20:17:32.775-04:00 level=INFO source=memory.go:36 msg="new model will fit in available VRAM across minimum required GPUs, loading" model=C:\Users\frost\.ollama\models\blobs\sha256-f5074b1221da0f5a2910d33b642efa5b9eb58cfdddca1c79e16d7ad28aa2b31f library=CUDA parallel=1 required="5.4 GiB" gpus=1
time=2025-10-22T20:17:32.776-04:00 level=INFO source=server.go:545 msg=offload library=CUDA layers.requested=-1 layers.model=33 layers.offload=33 layers.split=[33] memory.available="[10.4 GiB]" memory.gpu_overhead="0 B" memory.required.full="5.4 GiB" memory.required.partial="5.4 GiB" memory.required.kv="512.0 MiB" memory.required.allocations="[5.4 GiB]" memory.weights.total="4.0 GiB" memory.weights.repeating="3.9 GiB" memory.weights.nonrepeating="105.0 MiB" memory.graph.full="296.0 MiB" memory.graph.partial="305.0 MiB"
time=2025-10-22T20:17:32.777-04:00 level=INFO source=runner.go:828 msg=load request="{Operation:commit LoraPath:[] Parallel:1 BatchSize:512 FlashAttention:false KvSize:4096 KvCacheType: NumThreads:8 GPULayers:33[ID:GPU-b28645fa-aaf6-86a1-ab1b-ef8d61df29f2 Layers:33(0..32)] MultiUserCache:false ProjectorPath: MainGPU:0 UseMmap:false}"
time=2025-10-22T20:17:32.777-04:00 level=INFO source=server.go:1272 msg="waiting for llama runner to start responding"
time=2025-10-22T20:17:32.778-04:00 level=INFO source=server.go:1306 msg="waiting for server to become available" status="llm server loading model"
ggml_backend_cuda_device_get_memory utilizing NVML memory reporting free: 11200196608 total: 12820938752
llama_model_load_from_file_impl: using device CUDA0 (NVIDIA GeForce RTX 5070) (0000:01:00.0) - 10681 MiB free
llama_model_loader: loaded meta data with 25 key-value pairs and 291 tensors from C:\Users\frost\.ollama\models\blobs\sha256-f5074b1221da0f5a2910d33b642efa5b9eb58cfdddca1c79e16d7ad28aa2b31f (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = llama
llama_model_loader: - kv   1:                               general.name str              = Mistral-7B-Instruct-v0.3
llama_model_loader: - kv   2:                          llama.block_count u32              = 32
llama_model_loader: - kv   3:                       llama.context_length u32              = 32768
llama_model_loader: - kv   4:                     llama.embedding_length u32              = 4096
llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 14336
llama_model_loader: - kv   6:                 llama.attention.head_count u32              = 32
llama_model_loader: - kv   7:              llama.attention.head_count_kv u32              = 8
llama_model_loader: - kv   8:                       llama.rope.freq_base f32              = 1000000.000000
llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010
llama_model_loader: - kv  10:                          general.file_type u32              = 15
llama_model_loader: - kv  11:                           llama.vocab_size u32              = 32768
llama_model_loader: - kv  12:                 llama.rope.dimension_count u32              = 128
llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = llama
llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = default
llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,32768]   = ["<unk>", "<s>", "</s>", "[INST]", "[...
llama_model_loader: - kv  16:                      tokenizer.ggml.scores arr[f32,32768]   = [0.000000, 0.000000, 0.000000, 0.0000...
llama_model_loader: - kv  17:                  tokenizer.ggml.token_type arr[i32,32768]   = [2, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, ...
llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 1
llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 2
llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
llama_model_loader: - kv  21:               tokenizer.ggml.add_bos_token bool             = true
llama_model_loader: - kv  22:               tokenizer.ggml.add_eos_token bool             = false
llama_model_loader: - kv  23:                    tokenizer.chat_template str              = {{ bos_token }}{% for message in mess...
llama_model_loader: - kv  24:               general.quantization_version u32              = 2
llama_model_loader: - type  f32:   65 tensors
llama_model_loader: - type q4_K:  193 tensors
llama_model_loader: - type q6_K:   33 tensors
print_info: file format = GGUF V3 (latest)
print_info: file type   = Q4_K - Medium
print_info: file size   = 4.07 GiB (4.83 BPW) 
load: special_eos_id is not in special_eog_ids - the tokenizer config may be incorrect
load: printing all EOG tokens:
load:   - 2 ('</s>')
load: special tokens cache size = 771
load: token to piece cache size = 0.1731 MB
print_info: arch             = llama
print_info: vocab_only       = 0
print_info: n_ctx_train      = 32768
print_info: n_embd           = 4096
print_info: n_layer          = 32
print_info: n_head           = 32
print_info: n_head_kv        = 8
print_info: n_rot            = 128
print_info: n_swa            = 0
print_info: is_swa_any       = 0
print_info: n_embd_head_k    = 128
print_info: n_embd_head_v    = 128
print_info: n_gqa            = 4
print_info: n_embd_k_gqa     = 1024
print_info: n_embd_v_gqa     = 1024
print_info: f_norm_eps       = 0.0e+00
print_info: f_norm_rms_eps   = 1.0e-05
print_info: f_clamp_kqv      = 0.0e+00
print_info: f_max_alibi_bias = 0.0e+00
print_info: f_logit_scale    = 0.0e+00
print_info: f_attn_scale     = 0.0e+00
print_info: n_ff             = 14336
print_info: n_expert         = 0
print_info: n_expert_used    = 0
print_info: causal attn      = 1
print_info: pooling type     = 0
print_info: rope type        = 0
print_info: rope scaling     = linear
print_info: freq_base_train  = 1000000.0
print_info: freq_scale_train = 1
print_info: n_ctx_orig_yarn  = 32768
print_info: rope_finetuned   = unknown
print_info: model type       = 7B
print_info: model params     = 7.25 B
print_info: general.name     = Mistral-7B-Instruct-v0.3
print_info: vocab type       = SPM
print_info: n_vocab          = 32768
print_info: n_merges         = 0
print_info: BOS token        = 1 '<s>'
print_info: EOS token        = 2 '</s>'
print_info: UNK token        = 0 '<unk>'
print_info: LF token         = 781 '<0x0A>'
print_info: EOG token        = 2 '</s>'
print_info: max token length = 48
load_tensors: loading model tensors, this can take a while... (mmap = false)
load_tensors: offloading 32 repeating layers to GPU
load_tensors: offloading output layer to GPU
load_tensors: offloaded 33/33 layers to GPU
load_tensors:        CUDA0 model buffer size =  4097.52 MiB
load_tensors:          CPU model buffer size =    72.00 MiB
llama_init_from_model: model default pooling_type is [0], but [-1] was specified
llama_context: constructing llama_context
llama_context: n_seq_max     = 1
llama_context: n_ctx         = 4096
llama_context: n_ctx_per_seq = 4096
llama_context: n_batch       = 512
llama_context: n_ubatch      = 512
llama_context: causal_attn   = 1
llama_context: flash_attn    = disabled
llama_context: kv_unified    = false
llama_context: freq_base     = 1000000.0
llama_context: freq_scale    = 1
llama_context: n_ctx_per_seq (4096) < n_ctx_train (32768) -- the full capacity of the model will not be utilized
llama_context:  CUDA_Host  output buffer size =     0.14 MiB
llama_kv_cache:      CUDA0 KV buffer size =   512.00 MiB
llama_kv_cache: size =  512.00 MiB (  4096 cells,  32 layers,  1/1 seqs), K (f16):  256.00 MiB, V (f16):  256.00 MiB
llama_context:      CUDA0 compute buffer size =   300.01 MiB
llama_context:  CUDA_Host compute buffer size =    20.01 MiB
llama_context: graph nodes  = 1158
llama_context: graph splits = 2
time=2025-10-22T20:17:36.055-04:00 level=INFO source=server.go:1310 msg="llama runner started in 4.18 seconds"
time=2025-10-22T20:17:36.055-04:00 level=INFO source=sched.go:482 msg="loaded runners" count=1
time=2025-10-22T20:17:36.055-04:00 level=INFO source=server.go:1272 msg="waiting for llama runner to start responding"
time=2025-10-22T20:17:36.055-04:00 level=INFO source=server.go:1310 msg="llama runner started in 4.18 seconds"
[GIN] 2025/10/22 - 20:17:40 | 200 |    9.1398271s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/10/22 - 20:17:53 | 200 |      2.7558ms |       127.0.0.1 | GET      "/api/tags"
time=2025-10-22T20:17:55.434-04:00 level=WARN source=types.go:753 msg="invalid option provided" option=max_tokens
[GIN] 2025/10/22 - 20:17:59 | 200 |    4.2839921s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/10/22 - 20:19:11 | 200 |      2.7193ms |       127.0.0.1 | GET      "/api/tags"
time=2025-10-22T20:19:13.460-04:00 level=WARN source=types.go:753 msg="invalid option provided" option=max_tokens
[GIN] 2025/10/22 - 20:19:17 | 200 |    4.1355572s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/10/22 - 21:08:05 | 200 |            0s |       127.0.0.1 | HEAD     "/"
[GIN] 2025/10/22 - 21:08:05 | 404 |      1.0248ms |       127.0.0.1 | POST     "/api/show"
[GIN] 2025/10/22 - 21:08:06 | 200 |     792.408ms |       127.0.0.1 | POST     "/api/pull"
[GIN] 2025/10/22 - 21:09:10 | 200 |            0s |       127.0.0.1 | HEAD     "/"
[GIN] 2025/10/22 - 21:10:08 | 200 |            0s |       127.0.0.1 | HEAD     "/"
[GIN] 2025/10/22 - 21:10:59 | 200 |            0s |       127.0.0.1 | HEAD     "/"
[GIN] 2025/10/22 - 21:16:00 | 404 |       506.1µs |       127.0.0.1 | POST     "/api/embeddings"
[GIN] 2025/10/22 - 21:17:06 | 200 |            0s |       127.0.0.1 | HEAD     "/"
[GIN] 2025/10/22 - 21:17:06 | 200 |      1.0016ms |       127.0.0.1 | GET      "/api/tags"
[GIN] 2025/10/22 - 21:17:22 | 200 |            0s |       127.0.0.1 | HEAD     "/"
time=2025-10-22T21:17:23.618-04:00 level=INFO source=download.go:177 msg="downloading 970aa74c0a90 in 3 100 MB part(s)"
time=2025-10-22T21:17:53.941-04:00 level=INFO source=download.go:177 msg="downloading c71d239df917 in 1 11 KB part(s)"
time=2025-10-22T21:17:55.194-04:00 level=INFO source=download.go:177 msg="downloading ce4a164fc046 in 1 17 B part(s)"
time=2025-10-22T21:17:56.373-04:00 level=INFO source=download.go:177 msg="downloading 31df23ea7daa in 1 420 B part(s)"
[GIN] 2025/10/22 - 21:17:58 | 200 |   35.2534181s |       127.0.0.1 | POST     "/api/pull"
llama_model_loader: loaded meta data with 24 key-value pairs and 112 tensors from C:\Users\frost\.ollama\models\blobs\sha256-970aa74c0a90ef7482477cf803618e776e173c007bf957f635f1015bfcfef0e6 (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = nomic-bert
llama_model_loader: - kv   1:                               general.name str              = nomic-embed-text-v1.5
llama_model_loader: - kv   2:                     nomic-bert.block_count u32              = 12
llama_model_loader: - kv   3:                  nomic-bert.context_length u32              = 2048
llama_model_loader: - kv   4:                nomic-bert.embedding_length u32              = 768
llama_model_loader: - kv   5:             nomic-bert.feed_forward_length u32              = 3072
llama_model_loader: - kv   6:            nomic-bert.attention.head_count u32              = 12
llama_model_loader: - kv   7:    nomic-bert.attention.layer_norm_epsilon f32              = 0.000000
llama_model_loader: - kv   8:                          general.file_type u32              = 1
llama_model_loader: - kv   9:                nomic-bert.attention.causal bool             = false
llama_model_loader: - kv  10:                    nomic-bert.pooling_type u32              = 1
llama_model_loader: - kv  11:                  nomic-bert.rope.freq_base f32              = 1000.000000
llama_model_loader: - kv  12:            tokenizer.ggml.token_type_count u32              = 2
llama_model_loader: - kv  13:                tokenizer.ggml.bos_token_id u32              = 101
llama_model_loader: - kv  14:                tokenizer.ggml.eos_token_id u32              = 102
llama_model_loader: - kv  15:                       tokenizer.ggml.model str              = bert
llama_model_loader: - kv  16:                      tokenizer.ggml.tokens arr[str,30522]   = ["[PAD]", "[unused0]", "[unused1]", "...
llama_model_loader: - kv  17:                      tokenizer.ggml.scores arr[f32,30522]   = [-1000.000000, -1000.000000, -1000.00...
llama_model_loader: - kv  18:                  tokenizer.ggml.token_type arr[i32,30522]   = [3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  19:            tokenizer.ggml.unknown_token_id u32              = 100
llama_model_loader: - kv  20:          tokenizer.ggml.seperator_token_id u32              = 102
llama_model_loader: - kv  21:            tokenizer.ggml.padding_token_id u32              = 0
llama_model_loader: - kv  22:                tokenizer.ggml.cls_token_id u32              = 101
llama_model_loader: - kv  23:               tokenizer.ggml.mask_token_id u32              = 103
llama_model_loader: - type  f32:   51 tensors
llama_model_loader: - type  f16:   61 tensors
print_info: file format = GGUF V3 (latest)
print_info: file type   = F16
print_info: file size   = 260.86 MiB (16.00 BPW) 
load: special_eos_id is not in special_eog_ids - the tokenizer config may be incorrect
load: printing all EOG tokens:
load:   - 102 ('[SEP]')
load: special tokens cache size = 5
load: token to piece cache size = 0.2032 MB
print_info: arch             = nomic-bert
print_info: vocab_only       = 1
print_info: model type       = ?B
print_info: model params     = 136.73 M
print_info: general.name     = nomic-embed-text-v1.5
print_info: vocab type       = WPM
print_info: n_vocab          = 30522
print_info: n_merges         = 0
print_info: BOS token        = 101 '[CLS]'
print_info: EOS token        = 102 '[SEP]'
print_info: UNK token        = 100 '[UNK]'
print_info: SEP token        = 102 '[SEP]'
print_info: PAD token        = 0 '[PAD]'
print_info: MASK token       = 103 '[MASK]'
print_info: LF token         = 0 '[PAD]'
print_info: EOG token        = 102 '[SEP]'
print_info: max token length = 21
llama_model_load: vocab only - skipping tensors
time=2025-10-22T21:18:39.690-04:00 level=WARN source=server.go:174 msg="requested context size too large for model" num_ctx=8192 n_ctx_train=2048
time=2025-10-22T21:18:39.690-04:00 level=INFO source=cpu_windows.go:139 msg=packages count=1
time=2025-10-22T21:18:39.690-04:00 level=INFO source=cpu_windows.go:155 msg="efficiency cores detected" maxEfficiencyClass=1
time=2025-10-22T21:18:39.690-04:00 level=INFO source=cpu_windows.go:186 msg="" package=0 cores=12 efficiency=4 threads=20
time=2025-10-22T21:18:39.692-04:00 level=INFO source=server.go:400 msg="starting runner" cmd="C:\\Users\\frost\\AppData\\Local\\Programs\\Ollama\\ollama.exe runner --model C:\\Users\\frost\\.ollama\\models\\blobs\\sha256-970aa74c0a90ef7482477cf803618e776e173c007bf957f635f1015bfcfef0e6 --port 55803"
time=2025-10-22T21:18:39.696-04:00 level=INFO source=cpu_windows.go:139 msg=packages count=1
time=2025-10-22T21:18:39.696-04:00 level=INFO source=cpu_windows.go:155 msg="efficiency cores detected" maxEfficiencyClass=1
time=2025-10-22T21:18:39.696-04:00 level=INFO source=cpu_windows.go:186 msg="" package=0 cores=12 efficiency=4 threads=20
time=2025-10-22T21:18:39.696-04:00 level=INFO source=server.go:505 msg="system memory" total="63.8 GiB" free="42.6 GiB" free_swap="40.7 GiB"
time=2025-10-22T21:18:39.696-04:00 level=INFO source=memory.go:36 msg="new model will fit in available VRAM across minimum required GPUs, loading" model=C:\Users\frost\.ollama\models\blobs\sha256-970aa74c0a90ef7482477cf803618e776e173c007bf957f635f1015bfcfef0e6 library=CUDA parallel=1 required="754.4 MiB" gpus=1
time=2025-10-22T21:18:39.696-04:00 level=INFO source=server.go:545 msg=offload library=CUDA layers.requested=-1 layers.model=13 layers.offload=13 layers.split=[13] memory.available="[10.3 GiB]" memory.gpu_overhead="0 B" memory.required.full="754.4 MiB" memory.required.partial="754.4 MiB" memory.required.kv="6.0 MiB" memory.required.allocations="[754.4 MiB]" memory.weights.total="260.9 MiB" memory.weights.repeating="216.1 MiB" memory.weights.nonrepeating="44.7 MiB" memory.graph.full="12.0 MiB" memory.graph.partial="12.0 MiB"
time=2025-10-22T21:18:39.740-04:00 level=INFO source=runner.go:893 msg="starting go runner"
load_backend: loaded CPU backend from C:\Users\frost\AppData\Local\Programs\Ollama\lib\ollama\ggml-cpu-alderlake.dll
ggml_cuda_init: GGML_CUDA_FORCE_MMQ:    no
ggml_cuda_init: GGML_CUDA_FORCE_CUBLAS: no
ggml_cuda_init: found 1 CUDA devices:
  Device 0: NVIDIA GeForce RTX 5070, compute capability 12.0, VMM: yes, ID: GPU-b28645fa-aaf6-86a1-ab1b-ef8d61df29f2
load_backend: loaded CUDA backend from C:\Users\frost\AppData\Local\Programs\Ollama\lib\ollama\cuda_v12\ggml-cuda.dll
time=2025-10-22T21:18:39.875-04:00 level=INFO source=ggml.go:104 msg=system CPU.0.SSE3=1 CPU.0.SSSE3=1 CPU.0.AVX=1 CPU.0.AVX_VNNI=1 CPU.0.AVX2=1 CPU.0.F16C=1 CPU.0.FMA=1 CPU.0.BMI2=1 CPU.0.LLAMAFILE=1 CPU.1.LLAMAFILE=1 CUDA.0.ARCHS=500,520,600,610,700,750,800,860,870,890,900,1200 CUDA.0.USE_GRAPHS=1 CUDA.0.PEER_MAX_BATCH_SIZE=128 compiler=cgo(clang)
time=2025-10-22T21:18:39.876-04:00 level=INFO source=runner.go:929 msg="Server listening on 127.0.0.1:55803"
time=2025-10-22T21:18:39.884-04:00 level=INFO source=runner.go:828 msg=load request="{Operation:commit LoraPath:[] Parallel:1 BatchSize:512 FlashAttention:false KvSize:2048 KvCacheType: NumThreads:8 GPULayers:13[ID:GPU-b28645fa-aaf6-86a1-ab1b-ef8d61df29f2 Layers:13(0..12)] MultiUserCache:false ProjectorPath: MainGPU:0 UseMmap:false}"
time=2025-10-22T21:18:39.884-04:00 level=INFO source=server.go:1272 msg="waiting for llama runner to start responding"
time=2025-10-22T21:18:39.884-04:00 level=INFO source=server.go:1306 msg="waiting for server to become available" status="llm server loading model"
ggml_backend_cuda_device_get_memory utilizing NVML memory reporting free: 11025649664 total: 12820938752
llama_model_load_from_file_impl: using device CUDA0 (NVIDIA GeForce RTX 5070) (0000:01:00.0) - 10514 MiB free
llama_model_loader: loaded meta data with 24 key-value pairs and 112 tensors from C:\Users\frost\.ollama\models\blobs\sha256-970aa74c0a90ef7482477cf803618e776e173c007bf957f635f1015bfcfef0e6 (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = nomic-bert
llama_model_loader: - kv   1:                               general.name str              = nomic-embed-text-v1.5
llama_model_loader: - kv   2:                     nomic-bert.block_count u32              = 12
llama_model_loader: - kv   3:                  nomic-bert.context_length u32              = 2048
llama_model_loader: - kv   4:                nomic-bert.embedding_length u32              = 768
llama_model_loader: - kv   5:             nomic-bert.feed_forward_length u32              = 3072
llama_model_loader: - kv   6:            nomic-bert.attention.head_count u32              = 12
llama_model_loader: - kv   7:    nomic-bert.attention.layer_norm_epsilon f32              = 0.000000
llama_model_loader: - kv   8:                          general.file_type u32              = 1
llama_model_loader: - kv   9:                nomic-bert.attention.causal bool             = false
llama_model_loader: - kv  10:                    nomic-bert.pooling_type u32              = 1
llama_model_loader: - kv  11:                  nomic-bert.rope.freq_base f32              = 1000.000000
llama_model_loader: - kv  12:            tokenizer.ggml.token_type_count u32              = 2
llama_model_loader: - kv  13:                tokenizer.ggml.bos_token_id u32              = 101
llama_model_loader: - kv  14:                tokenizer.ggml.eos_token_id u32              = 102
llama_model_loader: - kv  15:                       tokenizer.ggml.model str              = bert
llama_model_loader: - kv  16:                      tokenizer.ggml.tokens arr[str,30522]   = ["[PAD]", "[unused0]", "[unused1]", "...
llama_model_loader: - kv  17:                      tokenizer.ggml.scores arr[f32,30522]   = [-1000.000000, -1000.000000, -1000.00...
llama_model_loader: - kv  18:                  tokenizer.ggml.token_type arr[i32,30522]   = [3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  19:            tokenizer.ggml.unknown_token_id u32              = 100
llama_model_loader: - kv  20:          tokenizer.ggml.seperator_token_id u32              = 102
llama_model_loader: - kv  21:            tokenizer.ggml.padding_token_id u32              = 0
llama_model_loader: - kv  22:                tokenizer.ggml.cls_token_id u32              = 101
llama_model_loader: - kv  23:               tokenizer.ggml.mask_token_id u32              = 103
llama_model_loader: - type  f32:   51 tensors
llama_model_loader: - type  f16:   61 tensors
print_info: file format = GGUF V3 (latest)
print_info: file type   = F16
print_info: file size   = 260.86 MiB (16.00 BPW) 
load: special_eos_id is not in special_eog_ids - the tokenizer config may be incorrect
load: printing all EOG tokens:
load:   - 102 ('[SEP]')
load: special tokens cache size = 5
load: token to piece cache size = 0.2032 MB
print_info: arch             = nomic-bert
print_info: vocab_only       = 0
print_info: n_ctx_train      = 2048
print_info: n_embd           = 768
print_info: n_layer          = 12
print_info: n_head           = 12
print_info: n_head_kv        = 12
print_info: n_rot            = 64
print_info: n_swa            = 0
print_info: is_swa_any       = 0
print_info: n_embd_head_k    = 64
print_info: n_embd_head_v    = 64
print_info: n_gqa            = 1
print_info: n_embd_k_gqa     = 768
print_info: n_embd_v_gqa     = 768
print_info: f_norm_eps       = 1.0e-12
print_info: f_norm_rms_eps   = 0.0e+00
print_info: f_clamp_kqv      = 0.0e+00
print_info: f_max_alibi_bias = 0.0e+00
print_info: f_logit_scale    = 0.0e+00
print_info: f_attn_scale     = 0.0e+00
print_info: n_ff             = 3072
print_info: n_expert         = 0
print_info: n_expert_used    = 0
print_info: causal attn      = 0
print_info: pooling type     = 1
print_info: rope type        = 2
print_info: rope scaling     = linear
print_info: freq_base_train  = 1000.0
print_info: freq_scale_train = 1
print_info: n_ctx_orig_yarn  = 2048
print_info: rope_finetuned   = unknown
print_info: model type       = 137M
print_info: model params     = 136.73 M
print_info: general.name     = nomic-embed-text-v1.5
print_info: vocab type       = WPM
print_info: n_vocab          = 30522
print_info: n_merges         = 0
print_info: BOS token        = 101 '[CLS]'
print_info: EOS token        = 102 '[SEP]'
print_info: UNK token        = 100 '[UNK]'
print_info: SEP token        = 102 '[SEP]'
print_info: PAD token        = 0 '[PAD]'
print_info: MASK token       = 103 '[MASK]'
print_info: LF token         = 0 '[PAD]'
print_info: EOG token        = 102 '[SEP]'
print_info: max token length = 21
load_tensors: loading model tensors, this can take a while... (mmap = false)
load_tensors: offloading 12 repeating layers to GPU
load_tensors: offloading output layer to GPU
load_tensors: offloaded 13/13 layers to GPU
load_tensors:    CUDA_Host model buffer size =    44.72 MiB
load_tensors:        CUDA0 model buffer size =   216.14 MiB
llama_init_from_model: model default pooling_type is [1], but [-1] was specified
llama_context: constructing llama_context
llama_context: n_seq_max     = 1
llama_context: n_ctx         = 2048
llama_context: n_ctx_per_seq = 2048
llama_context: n_batch       = 512
llama_context: n_ubatch      = 512
llama_context: causal_attn   = 0
llama_context: flash_attn    = disabled
llama_context: kv_unified    = false
llama_context: freq_base     = 1000.0
llama_context: freq_scale    = 1
llama_context:  CUDA_Host  output buffer size =     0.12 MiB
llama_context:      CUDA0 compute buffer size =    24.00 MiB
llama_context:  CUDA_Host compute buffer size =     2.51 MiB
llama_context: graph nodes  = 371
llama_context: graph splits = 4 (with bs=512), 2 (with bs=1)
time=2025-10-22T21:18:40.389-04:00 level=INFO source=server.go:1310 msg="llama runner started in 0.70 seconds"
time=2025-10-22T21:18:40.389-04:00 level=INFO source=sched.go:482 msg="loaded runners" count=1
time=2025-10-22T21:18:40.389-04:00 level=INFO source=server.go:1272 msg="waiting for llama runner to start responding"
time=2025-10-22T21:18:40.389-04:00 level=INFO source=server.go:1310 msg="llama runner started in 0.70 seconds"
[GIN] 2025/10/22 - 21:18:40 | 200 |    958.1115ms |       127.0.0.1 | POST     "/api/embeddings"
[GIN] 2025/10/22 - 21:19:17 | 200 |            0s |       127.0.0.1 | HEAD     "/"
[GIN] 2025/10/22 - 21:19:17 | 200 |      1.0194ms |       127.0.0.1 | GET      "/api/tags"
[GIN] 2025/10/22 - 21:19:30 | 200 |            0s |       127.0.0.1 | HEAD     "/"
[GIN] 2025/10/22 - 21:19:30 | 200 |     12.0087ms |       127.0.0.1 | POST     "/api/show"
[GIN] 2025/10/22 - 21:24:43 | 400 |            0s |       127.0.0.1 | POST     "/api/embeddings"
[GIN] 2025/10/23 - 06:00:18 | 200 |     74.9951ms |       127.0.0.1 | GET      "/api/tags"
[GIN] 2025/10/23 - 06:00:21 | 200 |      3.3693ms |       127.0.0.1 | GET      "/api/tags"
[GIN] 2025/10/23 - 06:00:27 | 200 |       2.008ms |       127.0.0.1 | GET      "/api/tags"
[GIN] 2025/10/23 - 06:00:31 | 200 |      2.3427ms |       127.0.0.1 | GET      "/api/tags"
[GIN] 2025/10/23 - 06:00:36 | 200 |      2.3477ms |       127.0.0.1 | GET      "/api/tags"
[GIN] 2025/10/23 - 06:00:41 | 200 |      2.0347ms |       127.0.0.1 | GET      "/api/tags"
[GIN] 2025/10/23 - 06:00:46 | 200 |      4.4888ms |       127.0.0.1 | GET      "/api/tags"
[GIN] 2025/10/23 - 06:00:51 | 200 |      2.4452ms |       127.0.0.1 | GET      "/api/tags"
[GIN] 2025/10/23 - 06:00:56 | 200 |       3.302ms |       127.0.0.1 | GET      "/api/tags"
[GIN] 2025/10/23 - 06:01:01 | 200 |       2.936ms |       127.0.0.1 | GET      "/api/tags"
[GIN] 2025/10/23 - 06:01:06 | 200 |      2.3667ms |       127.0.0.1 | GET      "/api/tags"
[GIN] 2025/10/23 - 06:01:11 | 200 |      2.7122ms |       127.0.0.1 | GET      "/api/tags"
[GIN] 2025/10/23 - 06:01:16 | 200 |      2.1631ms |       127.0.0.1 | GET      "/api/tags"
[GIN] 2025/10/23 - 06:01:21 | 200 |      2.3322ms |       127.0.0.1 | GET      "/api/tags"
[GIN] 2025/10/23 - 06:01:26 | 200 |      2.9417ms |       127.0.0.1 | GET      "/api/tags"
[GIN] 2025/10/23 - 06:01:31 | 200 |      1.2845ms |       127.0.0.1 | GET      "/api/tags"
[GIN] 2025/10/23 - 06:01:36 | 200 |      3.6927ms |       127.0.0.1 | GET      "/api/tags"
[GIN] 2025/10/23 - 06:01:41 | 200 |      2.5562ms |       127.0.0.1 | GET      "/api/tags"
[GIN] 2025/10/23 - 06:01:46 | 200 |      1.6995ms |       127.0.0.1 | GET      "/api/tags"
[GIN] 2025/10/23 - 06:01:51 | 200 |      2.6673ms |       127.0.0.1 | GET      "/api/tags"
[GIN] 2025/10/23 - 06:01:56 | 200 |      2.3105ms |       127.0.0.1 | GET      "/api/tags"
[GIN] 2025/10/23 - 06:02:02 | 200 |      1.5496ms |       127.0.0.1 | GET      "/api/tags"
[GIN] 2025/10/23 - 06:02:06 | 200 |      2.0595ms |       127.0.0.1 | GET      "/api/tags"
[GIN] 2025/10/23 - 06:02:11 | 200 |      2.2034ms |       127.0.0.1 | GET      "/api/tags"
[GIN] 2025/10/23 - 06:02:16 | 200 |      3.6469ms |       127.0.0.1 | GET      "/api/tags"
[GIN] 2025/10/23 - 06:02:21 | 200 |      3.2191ms |       127.0.0.1 | GET      "/api/tags"
[GIN] 2025/10/23 - 06:02:27 | 200 |      1.8589ms |       127.0.0.1 | GET      "/api/tags"
[GIN] 2025/10/23 - 06:02:31 | 200 |      1.5077ms |       127.0.0.1 | GET      "/api/tags"
[GIN] 2025/10/23 - 06:02:36 | 200 |      2.5025ms |       127.0.0.1 | GET      "/api/tags"
[GIN] 2025/10/23 - 06:02:41 | 200 |      2.4725ms |       127.0.0.1 | GET      "/api/tags"
[GIN] 2025/10/23 - 06:02:46 | 200 |      2.1243ms |       127.0.0.1 | GET      "/api/tags"
[GIN] 2025/10/23 - 06:02:52 | 200 |      2.5878ms |       127.0.0.1 | GET      "/api/tags"
[GIN] 2025/10/23 - 06:02:57 | 200 |      3.1282ms |       127.0.0.1 | GET      "/api/tags"
[GIN] 2025/10/23 - 06:03:02 | 200 |      4.9365ms |       127.0.0.1 | GET      "/api/tags"
[GIN] 2025/10/23 - 06:03:07 | 200 |      3.5557ms |       127.0.0.1 | GET      "/api/tags"
[GIN] 2025/10/23 - 06:03:12 | 200 |      2.4009ms |       127.0.0.1 | GET      "/api/tags"
[GIN] 2025/10/23 - 06:03:16 | 200 |      2.4222ms |       127.0.0.1 | GET      "/api/tags"
[GIN] 2025/10/23 - 06:03:22 | 200 |      2.7772ms |       127.0.0.1 | GET      "/api/tags"
[GIN] 2025/10/23 - 06:03:26 | 200 |      1.8627ms |       127.0.0.1 | GET      "/api/tags"
[GIN] 2025/10/23 - 06:03:31 | 200 |      4.3227ms |       127.0.0.1 | GET      "/api/tags"
[GIN] 2025/10/23 - 06:03:37 | 200 |      2.1347ms |       127.0.0.1 | GET      "/api/tags"
[GIN] 2025/10/23 - 06:03:37 | 200 |      2.5029ms |       127.0.0.1 | GET      "/api/tags"
[GIN] 2025/10/23 - 10:56:54 | 200 |      1.5244ms |       127.0.0.1 | GET      "/api/tags"
[GIN] 2025/10/23 - 10:58:41 | 200 |      4.0765ms |       127.0.0.1 | GET      "/api/tags"
[GIN] 2025/10/23 - 10:58:42 | 200 |    699.7065ms |       127.0.0.1 | POST     "/api/show"
[GIN] 2025/10/23 - 10:58:42 | 200 |    401.4365ms |       127.0.0.1 | POST     "/api/show"
[GIN] 2025/10/23 - 10:58:42 | 200 |    560.7796ms |       127.0.0.1 | POST     "/api/show"
[GIN] 2025/10/23 - 11:30:23 | 200 |      1.7992ms |       127.0.0.1 | GET      "/api/tags"
[GIN] 2025/10/23 - 11:32:41 | 403 |            0s | 2601:589:4c80:5840:2032:fbcc:c907:df82 | GET      "/"
[GIN] 2025/10/23 - 11:35:56 | 403 |            0s | 2601:589:4c80:5840:2032:fbcc:c907:df82 | GET      "/api/tags"
[GIN] 2025/10/23 - 11:39:57 | 200 |      2.5179ms |       127.0.0.1 | GET      "/api/tags"
[GIN] 2025/10/23 - 11:40:12 | 403 |       534.5µs | 2601:589:4c80:5840:2032:fbcc:c907:df82 | GET      "/api/tags"
[GIN] 2025/10/23 - 12:01:36 | 403 |            0s | 2601:589:4c80:5840:2032:fbcc:c907:df82 | GET      "/api/tags"
Error: listen tcp 127.0.0.1:11434: bind: Only one usage of each socket address (protocol/network address/port) is normally permitted.
Error: listen tcp 127.0.0.1:11434: bind: Only one usage of each socket address (protocol/network address/port) is normally permitted.
Error: listen tcp 127.0.0.1:11434: bind: Only one usage of each socket address (protocol/network address/port) is normally permitted.
Error: listen tcp 127.0.0.1:11434: bind: Only one usage of each socket address (protocol/network address/port) is normally permitted.
Error: listen tcp 127.0.0.1:11434: bind: Only one usage of each socket address (protocol/network address/port) is normally permitted.
Error: listen tcp 127.0.0.1:11434: bind: Only one usage of each socket address (protocol/network address/port) is normally permitted.
Error: listen tcp 127.0.0.1:11434: bind: Only one usage of each socket address (protocol/network address/port) is normally permitted.
Error: listen tcp 127.0.0.1:11434: bind: Only one usage of each socket address (protocol/network address/port) is normally permitted.
Error: listen tcp 127.0.0.1:11434: bind: Only one usage of each socket address (protocol/network address/port) is normally permitted.
Error: listen tcp 127.0.0.1:11434: bind: Only one usage of each socket address (protocol/network address/port) is normally permitted.
Error: listen tcp 127.0.0.1:11434: bind: Only one usage of each socket address (protocol/network address/port) is normally permitted.
Error: listen tcp 127.0.0.1:11434: bind: Only one usage of each socket address (protocol/network address/port) is normally permitted.
Error: listen tcp 127.0.0.1:11434: bind: Only one usage of each socket address (protocol/network address/port) is normally permitted.
Error: listen tcp 127.0.0.1:11434: bind: Only one usage of each socket address (protocol/network address/port) is normally permitted.
Error: listen tcp 127.0.0.1:11434: bind: Only one usage of each socket address (protocol/network address/port) is normally permitted.
Error: listen tcp 127.0.0.1:11434: bind: Only one usage of each socket address (protocol/network address/port) is normally permitted.
Error: listen tcp 127.0.0.1:11434: bind: Only one usage of each socket address (protocol/network address/port) is normally permitted.
Error: listen tcp 127.0.0.1:11434: bind: Only one usage of each socket address (protocol/network address/port) is normally permitted.
Error: listen tcp 127.0.0.1:11434: bind: Only one usage of each socket address (protocol/network address/port) is normally permitted.
Error: listen tcp 127.0.0.1:11434: bind: Only one usage of each socket address (protocol/network address/port) is normally permitted.
Error: listen tcp 127.0.0.1:11434: bind: Only one usage of each socket address (protocol/network address/port) is normally permitted.
Error: listen tcp 127.0.0.1:11434: bind: Only one usage of each socket address (protocol/network address/port) is normally permitted.
Error: listen tcp 127.0.0.1:11434: bind: Only one usage of each socket address (protocol/network address/port) is normally permitted.
Error: listen tcp 127.0.0.1:11434: bind: Only one usage of each socket address (protocol/network address/port) is normally permitted.
Error: listen tcp 127.0.0.1:11434: bind: Only one usage of each socket address (protocol/network address/port) is normally permitted.
Error: listen tcp 127.0.0.1:11434: bind: Only one usage of each socket address (protocol/network address/port) is normally permitted.
Error: listen tcp 127.0.0.1:11434: bind: Only one usage of each socket address (protocol/network address/port) is normally permitted.
Error: listen tcp 127.0.0.1:11434: bind: Only one usage of each socket address (protocol/network address/port) is normally permitted.
Error: listen tcp 127.0.0.1:11434: bind: Only one usage of each socket address (protocol/network address/port) is normally permitted.
Error: listen tcp 127.0.0.1:11434: bind: Only one usage of each socket address (protocol/network address/port) is normally permitted.
Error: listen tcp 127.0.0.1:11434: bind: Only one usage of each socket address (protocol/network address/port) is normally permitted.
Error: listen tcp 127.0.0.1:11434: bind: Only one usage of each socket address (protocol/network address/port) is normally permitted.
Error: listen tcp 127.0.0.1:11434: bind: Only one usage of each socket address (protocol/network address/port) is normally permitted.
Error: listen tcp 127.0.0.1:11434: bind: Only one usage of each socket address (protocol/network address/port) is normally permitted.
Error: listen tcp 127.0.0.1:11434: bind: Only one usage of each socket address (protocol/network address/port) is normally permitted.
Error: listen tcp 127.0.0.1:11434: bind: Only one usage of each socket address (protocol/network address/port) is normally permitted.
Error: listen tcp 127.0.0.1:11434: bind: Only one usage of each socket address (protocol/network address/port) is normally permitted.
Error: listen tcp 127.0.0.1:11434: bind: Only one usage of each socket address (protocol/network address/port) is normally permitted.
Error: listen tcp 127.0.0.1:11434: bind: Only one usage of each socket address (protocol/network address/port) is normally permitted.
Error: listen tcp 127.0.0.1:11434: bind: Only one usage of each socket address (protocol/network address/port) is normally permitted.
Error: listen tcp 127.0.0.1:11434: bind: Only one usage of each socket address (protocol/network address/port) is normally permitted.
Error: listen tcp 127.0.0.1:11434: bind: Only one usage of each socket address (protocol/network address/port) is normally permitted.
Error: listen tcp 127.0.0.1:11434: bind: Only one usage of each socket address (protocol/network address/port) is normally permitted.
Error: listen tcp 127.0.0.1:11434: bind: Only one usage of each socket address (protocol/network address/port) is normally permitted.
Error: listen tcp 127.0.0.1:11434: bind: Only one usage of each socket address (protocol/network address/port) is normally permitted.
Error: listen tcp 127.0.0.1:11434: bind: Only one usage of each socket address (protocol/network address/port) is normally permitted.
Error: listen tcp 127.0.0.1:11434: bind: Only one usage of each socket address (protocol/network address/port) is normally permitted.
Error: listen tcp 127.0.0.1:11434: bind: Only one usage of each socket address (protocol/network address/port) is normally permitted.
Error: listen tcp 127.0.0.1:11434: bind: Only one usage of each socket address (protocol/network address/port) is normally permitted.
Error: listen tcp 127.0.0.1:11434: bind: Only one usage of each socket address (protocol/network address/port) is normally permitted.
Error: listen tcp 127.0.0.1:11434: bind: Only one usage of each socket address (protocol/network address/port) is normally permitted.
Error: listen tcp 127.0.0.1:11434: bind: Only one usage of each socket address (protocol/network address/port) is normally permitted.
Error: listen tcp 127.0.0.1:11434: bind: Only one usage of each socket address (protocol/network address/port) is normally permitted.
Error: listen tcp 127.0.0.1:11434: bind: Only one usage of each socket address (protocol/network address/port) is normally permitted.
Error: listen tcp 127.0.0.1:11434: bind: Only one usage of each socket address (protocol/network address/port) is normally permitted.
Error: listen tcp 127.0.0.1:11434: bind: Only one usage of each socket address (protocol/network address/port) is normally permitted.
Error: listen tcp 127.0.0.1:11434: bind: Only one usage of each socket address (protocol/network address/port) is normally permitted.
Error: listen tcp 127.0.0.1:11434: bind: Only one usage of each socket address (protocol/network address/port) is normally permitted.
Error: listen tcp 127.0.0.1:11434: bind: Only one usage of each socket address (protocol/network address/port) is normally permitted.
Error: listen tcp 127.0.0.1:11434: bind: Only one usage of each socket address (protocol/network address/port) is normally permitted.
Error: listen tcp 127.0.0.1:11434: bind: Only one usage of each socket address (protocol/network address/port) is normally permitted.
Error: listen tcp 127.0.0.1:11434: bind: Only one usage of each socket address (protocol/network address/port) is normally permitted.
Error: listen tcp 127.0.0.1:11434: bind: Only one usage of each socket address (protocol/network address/port) is normally permitted.
Error: listen tcp 127.0.0.1:11434: bind: Only one usage of each socket address (protocol/network address/port) is normally permitted.
Error: listen tcp 127.0.0.1:11434: bind: Only one usage of each socket address (protocol/network address/port) is normally permitted.
Error: listen tcp 127.0.0.1:11434: bind: Only one usage of each socket address (protocol/network address/port) is normally permitted.
Error: listen tcp 127.0.0.1:11434: bind: Only one usage of each socket address (protocol/network address/port) is normally permitted.
Error: listen tcp 127.0.0.1:11434: bind: Only one usage of each socket address (protocol/network address/port) is normally permitted.
Error: listen tcp 127.0.0.1:11434: bind: Only one usage of each socket address (protocol/network address/port) is normally permitted.
Error: listen tcp 127.0.0.1:11434: bind: Only one usage of each socket address (protocol/network address/port) is normally permitted.
Error: listen tcp 127.0.0.1:11434: bind: Only one usage of each socket address (protocol/network address/port) is normally permitted.
Error: listen tcp 127.0.0.1:11434: bind: Only one usage of each socket address (protocol/network address/port) is normally permitted.
Error: listen tcp 127.0.0.1:11434: bind: Only one usage of each socket address (protocol/network address/port) is normally permitted.
Error: listen tcp 127.0.0.1:11434: bind: Only one usage of each socket address (protocol/network address/port) is normally permitted.
Error: listen tcp 127.0.0.1:11434: bind: Only one usage of each socket address (protocol/network address/port) is normally permitted.
Error: listen tcp 127.0.0.1:11434: bind: Only one usage of each socket address (protocol/network address/port) is normally permitted.
Error: listen tcp 127.0.0.1:11434: bind: Only one usage of each socket address (protocol/network address/port) is normally permitted.
Error: listen tcp 127.0.0.1:11434: bind: Only one usage of each socket address (protocol/network address/port) is normally permitted.
Error: listen tcp 127.0.0.1:11434: bind: Only one usage of each socket address (protocol/network address/port) is normally permitted.
Error: listen tcp 127.0.0.1:11434: bind: Only one usage of each socket address (protocol/network address/port) is normally permitted.
Error: listen tcp 127.0.0.1:11434: bind: Only one usage of each socket address (protocol/network address/port) is normally permitted.
Error: listen tcp 127.0.0.1:11434: bind: Only one usage of each socket address (protocol/network address/port) is normally permitted.
Error: listen tcp 127.0.0.1:11434: bind: Only one usage of each socket address (protocol/network address/port) is normally permitted.
Error: listen tcp 127.0.0.1:11434: bind: Only one usage of each socket address (protocol/network address/port) is normally permitted.
Error: listen tcp 127.0.0.1:11434: bind: Only one usage of each socket address (protocol/network address/port) is normally permitted.
Error: listen tcp 127.0.0.1:11434: bind: Only one usage of each socket address (protocol/network address/port) is normally permitted.
Error: listen tcp 127.0.0.1:11434: bind: Only one usage of each socket address (protocol/network address/port) is normally permitted.
Error: listen tcp 127.0.0.1:11434: bind: Only one usage of each socket address (protocol/network address/port) is normally permitted.
Error: listen tcp 127.0.0.1:11434: bind: Only one usage of each socket address (protocol/network address/port) is normally permitted.
Error: listen tcp 127.0.0.1:11434: bind: Only one usage of each socket address (protocol/network address/port) is normally permitted.
Error: listen tcp 127.0.0.1:11434: bind: Only one usage of each socket address (protocol/network address/port) is normally permitted.
Error: listen tcp 127.0.0.1:11434: bind: Only one usage of each socket address (protocol/network address/port) is normally permitted.
Error: listen tcp 127.0.0.1:11434: bind: Only one usage of each socket address (protocol/network address/port) is normally permitted.
Error: listen tcp 127.0.0.1:11434: bind: Only one usage of each socket address (protocol/network address/port) is normally permitted.
Error: listen tcp 127.0.0.1:11434: bind: Only one usage of each socket address (protocol/network address/port) is normally permitted.
Error: listen tcp 127.0.0.1:11434: bind: Only one usage of each socket address (protocol/network address/port) is normally permitted.
Error: listen tcp 127.0.0.1:11434: bind: Only one usage of each socket address (protocol/network address/port) is normally permitted.
Error: listen tcp 127.0.0.1:11434: bind: Only one usage of each socket address (protocol/network address/port) is normally permitted.
Error: listen tcp 127.0.0.1:11434: bind: Only one usage of each socket address (protocol/network address/port) is normally permitted.
Error: listen tcp 127.0.0.1:11434: bind: Only one usage of each socket address (protocol/network address/port) is normally permitted.
Error: listen tcp 127.0.0.1:11434: bind: Only one usage of each socket address (protocol/network address/port) is normally permitted.
Error: listen tcp 127.0.0.1:11434: bind: Only one usage of each socket address (protocol/network address/port) is normally permitted.
Error: listen tcp 127.0.0.1:11434: bind: Only one usage of each socket address (protocol/network address/port) is normally permitted.
Error: listen tcp 127.0.0.1:11434: bind: Only one usage of each socket address (protocol/network address/port) is normally permitted.
Error: listen tcp 127.0.0.1:11434: bind: Only one usage of each socket address (protocol/network address/port) is normally permitted.
Error: listen tcp 127.0.0.1:11434: bind: Only one usage of each socket address (protocol/network address/port) is normally permitted.
Error: listen tcp 127.0.0.1:11434: bind: Only one usage of each socket address (protocol/network address/port) is normally permitted.
Error: listen tcp 127.0.0.1:11434: bind: Only one usage of each socket address (protocol/network address/port) is normally permitted.
Error: listen tcp 127.0.0.1:11434: bind: Only one usage of each socket address (protocol/network address/port) is normally permitted.
Error: listen tcp 127.0.0.1:11434: bind: Only one usage of each socket address (protocol/network address/port) is normally permitted.
Error: listen tcp 127.0.0.1:11434: bind: Only one usage of each socket address (protocol/network address/port) is normally permitted.
Error: listen tcp 127.0.0.1:11434: bind: Only one usage of each socket address (protocol/network address/port) is normally permitted.
Error: listen tcp 127.0.0.1:11434: bind: Only one usage of each socket address (protocol/network address/port) is normally permitted.
Error: listen tcp 127.0.0.1:11434: bind: Only one usage of each socket address (protocol/network address/port) is normally permitted.
Error: listen tcp 127.0.0.1:11434: bind: Only one usage of each socket address (protocol/network address/port) is normally permitted.
Error: listen tcp 127.0.0.1:11434: bind: Only one usage of each socket address (protocol/network address/port) is normally permitted.
Error: listen tcp 127.0.0.1:11434: bind: Only one usage of each socket address (protocol/network address/port) is normally permitted.
Error: listen tcp 127.0.0.1:11434: bind: Only one usage of each socket address (protocol/network address/port) is normally permitted.
Error: listen tcp 127.0.0.1:11434: bind: Only one usage of each socket address (protocol/network address/port) is normally permitted.
Error: listen tcp 127.0.0.1:11434: bind: Only one usage of each socket address (protocol/network address/port) is normally permitted.
Error: listen tcp 127.0.0.1:11434: bind: Only one usage of each socket address (protocol/network address/port) is normally permitted.
Error: listen tcp 127.0.0.1:11434: bind: Only one usage of each socket address (protocol/network address/port) is normally permitted.
Error: listen tcp 127.0.0.1:11434: bind: Only one usage of each socket address (protocol/network address/port) is normally permitted.
Error: listen tcp 127.0.0.1:11434: bind: Only one usage of each socket address (protocol/network address/port) is normally permitted.
Error: listen tcp 127.0.0.1:11434: bind: Only one usage of each socket address (protocol/network address/port) is normally permitted.
Error: listen tcp 127.0.0.1:11434: bind: Only one usage of each socket address (protocol/network address/port) is normally permitted.
Error: listen tcp 127.0.0.1:11434: bind: Only one usage of each socket address (protocol/network address/port) is normally permitted.
Error: listen tcp 127.0.0.1:11434: bind: Only one usage of each socket address (protocol/network address/port) is normally permitted.
Error: listen tcp 127.0.0.1:11434: bind: Only one usage of each socket address (protocol/network address/port) is normally permitted.
Error: listen tcp 127.0.0.1:11434: bind: Only one usage of each socket address (protocol/network address/port) is normally permitted.
Error: listen tcp 127.0.0.1:11434: bind: Only one usage of each socket address (protocol/network address/port) is normally permitted.
Error: listen tcp 127.0.0.1:11434: bind: Only one usage of each socket address (protocol/network address/port) is normally permitted.
Error: listen tcp 127.0.0.1:11434: bind: Only one usage of each socket address (protocol/network address/port) is normally permitted.
Error: listen tcp 127.0.0.1:11434: bind: Only one usage of each socket address (protocol/network address/port) is normally permitted.
Error: listen tcp 127.0.0.1:11434: bind: Only one usage of each socket address (protocol/network address/port) is normally permitted.
Error: listen tcp 127.0.0.1:11434: bind: Only one usage of each socket address (protocol/network address/port) is normally permitted.
Error: listen tcp 127.0.0.1:11434: bind: Only one usage of each socket address (protocol/network address/port) is normally permitted.
Error: listen tcp 127.0.0.1:11434: bind: Only one usage of each socket address (protocol/network address/port) is normally permitted.
Error: listen tcp 127.0.0.1:11434: bind: Only one usage of each socket address (protocol/network address/port) is normally permitted.
Error: listen tcp 127.0.0.1:11434: bind: Only one usage of each socket address (protocol/network address/port) is normally permitted.
Error: listen tcp 127.0.0.1:11434: bind: Only one usage of each socket address (protocol/network address/port) is normally permitted.
Error: listen tcp 127.0.0.1:11434: bind: Only one usage of each socket address (protocol/network address/port) is normally permitted.
Error: listen tcp 127.0.0.1:11434: bind: Only one usage of each socket address (protocol/network address/port) is normally permitted.
Error: listen tcp 127.0.0.1:11434: bind: Only one usage of each socket address (protocol/network address/port) is normally permitted.
Error: listen tcp 127.0.0.1:11434: bind: Only one usage of each socket address (protocol/network address/port) is normally permitted.
Error: listen tcp 127.0.0.1:11434: bind: Only one usage of each socket address (protocol/network address/port) is normally permitted.
Error: listen tcp 127.0.0.1:11434: bind: Only one usage of each socket address (protocol/network address/port) is normally permitted.
Error: listen tcp 127.0.0.1:11434: bind: Only one usage of each socket address (protocol/network address/port) is normally permitted.
Error: listen tcp 127.0.0.1:11434: bind: Only one usage of each socket address (protocol/network address/port) is normally permitted.
Error: listen tcp 127.0.0.1:11434: bind: Only one usage of each socket address (protocol/network address/port) is normally permitted.
Error: listen tcp 127.0.0.1:11434: bind: Only one usage of each socket address (protocol/network address/port) is normally permitted.
Error: listen tcp 127.0.0.1:11434: bind: Only one usage of each socket address (protocol/network address/port) is normally permitted.
Error: listen tcp 127.0.0.1:11434: bind: Only one usage of each socket address (protocol/network address/port) is normally permitted.
Error: listen tcp 127.0.0.1:11434: bind: Only one usage of each socket address (protocol/network address/port) is normally permitted.
Error: listen tcp 127.0.0.1:11434: bind: Only one usage of each socket address (protocol/network address/port) is normally permitted.
Error: listen tcp 127.0.0.1:11434: bind: Only one usage of each socket address (protocol/network address/port) is normally permitted.
Error: listen tcp 127.0.0.1:11434: bind: Only one usage of each socket address (protocol/network address/port) is normally permitted.
Error: listen tcp 127.0.0.1:11434: bind: Only one usage of each socket address (protocol/network address/port) is normally permitted.
Error: listen tcp 127.0.0.1:11434: bind: Only one usage of each socket address (protocol/network address/port) is normally permitted.
Error: listen tcp 127.0.0.1:11434: bind: Only one usage of each socket address (protocol/network address/port) is normally permitted.
Error: listen tcp 127.0.0.1:11434: bind: Only one usage of each socket address (protocol/network address/port) is normally permitted.
Error: listen tcp 127.0.0.1:11434: bind: Only one usage of each socket address (protocol/network address/port) is normally permitted.
Error: listen tcp 127.0.0.1:11434: bind: Only one usage of each socket address (protocol/network address/port) is normally permitted.
Error: listen tcp 127.0.0.1:11434: bind: Only one usage of each socket address (protocol/network address/port) is normally permitted.
Error: listen tcp 127.0.0.1:11434: bind: Only one usage of each socket address (protocol/network address/port) is normally permitted.
Error: listen tcp 127.0.0.1:11434: bind: Only one usage of each socket address (protocol/network address/port) is normally permitted.
Error: listen tcp 127.0.0.1:11434: bind: Only one usage of each socket address (protocol/network address/port) is normally permitted.
Error: listen tcp 127.0.0.1:11434: bind: Only one usage of each socket address (protocol/network address/port) is normally permitted.
Error: listen tcp 127.0.0.1:11434: bind: Only one usage of each socket address (protocol/network address/port) is normally permitted.
Error: listen tcp 127.0.0.1:11434: bind: Only one usage of each socket address (protocol/network address/port) is normally permitted.
Error: listen tcp 127.0.0.1:11434: bind: Only one usage of each socket address (protocol/network address/port) is normally permitted.
Error: listen tcp 127.0.0.1:11434: bind: Only one usage of each socket address (protocol/network address/port) is normally permitted.
Error: listen tcp 127.0.0.1:11434: bind: Only one usage of each socket address (protocol/network address/port) is normally permitted.
Error: listen tcp 127.0.0.1:11434: bind: Only one usage of each socket address (protocol/network address/port) is normally permitted.
Error: listen tcp 127.0.0.1:11434: bind: Only one usage of each socket address (protocol/network address/port) is normally permitted.
Error: listen tcp 127.0.0.1:11434: bind: Only one usage of each socket address (protocol/network address/port) is normally permitted.
Error: listen tcp 127.0.0.1:11434: bind: Only one usage of each socket address (protocol/network address/port) is normally permitted.
Error: listen tcp 127.0.0.1:11434: bind: Only one usage of each socket address (protocol/network address/port) is normally permitted.
Error: listen tcp 127.0.0.1:11434: bind: Only one usage of each socket address (protocol/network address/port) is normally permitted.
Error: listen tcp 127.0.0.1:11434: bind: Only one usage of each socket address (protocol/network address/port) is normally permitted.
Error: listen tcp 127.0.0.1:11434: bind: Only one usage of each socket address (protocol/network address/port) is normally permitted.
Error: listen tcp 127.0.0.1:11434: bind: Only one usage of each socket address (protocol/network address/port) is normally permitted.
Error: listen tcp 127.0.0.1:11434: bind: Only one usage of each socket address (protocol/network address/port) is normally permitted.
Error: listen tcp 127.0.0.1:11434: bind: Only one usage of each socket address (protocol/network address/port) is normally permitted.
Error: listen tcp 127.0.0.1:11434: bind: Only one usage of each socket address (protocol/network address/port) is normally permitted.
Error: listen tcp 127.0.0.1:11434: bind: Only one usage of each socket address (protocol/network address/port) is normally permitted.
Error: listen tcp 127.0.0.1:11434: bind: Only one usage of each socket address (protocol/network address/port) is normally permitted.
Error: listen tcp 127.0.0.1:11434: bind: Only one usage of each socket address (protocol/network address/port) is normally permitted.
Error: listen tcp 127.0.0.1:11434: bind: Only one usage of each socket address (protocol/network address/port) is normally permitted.
Error: listen tcp 127.0.0.1:11434: bind: Only one usage of each socket address (protocol/network address/port) is normally permitted.
Error: listen tcp 127.0.0.1:11434: bind: Only one usage of each socket address (protocol/network address/port) is normally permitted.
Error: listen tcp 127.0.0.1:11434: bind: Only one usage of each socket address (protocol/network address/port) is normally permitted.
Error: listen tcp 127.0.0.1:11434: bind: Only one usage of each socket address (protocol/network address/port) is normally permitted.
Error: listen tcp 127.0.0.1:11434: bind: Only one usage of each socket address (protocol/network address/port) is normally permitted.
Error: listen tcp 127.0.0.1:11434: bind: Only one usage of each socket address (protocol/network address/port) is normally permitted.
Error: listen tcp 127.0.0.1:11434: bind: Only one usage of each socket address (protocol/network address/port) is normally permitted.
Error: listen tcp 127.0.0.1:11434: bind: Only one usage of each socket address (protocol/network address/port) is normally permitted.
Error: listen tcp 127.0.0.1:11434: bind: Only one usage of each socket address (protocol/network address/port) is normally permitted.
Error: listen tcp 127.0.0.1:11434: bind: Only one usage of each socket address (protocol/network address/port) is normally permitted.
Error: listen tcp 127.0.0.1:11434: bind: Only one usage of each socket address (protocol/network address/port) is normally permitted.
Error: listen tcp 127.0.0.1:11434: bind: Only one usage of each socket address (protocol/network address/port) is normally permitted.
Error: listen tcp 127.0.0.1:11434: bind: Only one usage of each socket address (protocol/network address/port) is normally permitted.
Error: listen tcp 127.0.0.1:11434: bind: Only one usage of each socket address (protocol/network address/port) is normally permitted.
Error: listen tcp 127.0.0.1:11434: bind: Only one usage of each socket address (protocol/network address/port) is normally permitted.
Error: listen tcp 127.0.0.1:11434: bind: Only one usage of each socket address (protocol/network address/port) is normally permitted.
Error: listen tcp 127.0.0.1:11434: bind: Only one usage of each socket address (protocol/network address/port) is normally permitted.
Error: listen tcp 127.0.0.1:11434: bind: Only one usage of each socket address (protocol/network address/port) is normally permitted.
Error: listen tcp 127.0.0.1:11434: bind: Only one usage of each socket address (protocol/network address/port) is normally permitted.
Error: listen tcp 127.0.0.1:11434: bind: Only one usage of each socket address (protocol/network address/port) is normally permitted.
Error: listen tcp 127.0.0.1:11434: bind: Only one usage of each socket address (protocol/network address/port) is normally permitted.
Error: listen tcp 127.0.0.1:11434: bind: Only one usage of each socket address (protocol/network address/port) is normally permitted.
Error: listen tcp 127.0.0.1:11434: bind: Only one usage of each socket address (protocol/network address/port) is normally permitted.
Error: listen tcp 127.0.0.1:11434: bind: Only one usage of each socket address (protocol/network address/port) is normally permitted.
Error: listen tcp 127.0.0.1:11434: bind: Only one usage of each socket address (protocol/network address/port) is normally permitted.
Error: listen tcp 127.0.0.1:11434: bind: Only one usage of each socket address (protocol/network address/port) is normally permitted.
Error: listen tcp 127.0.0.1:11434: bind: Only one usage of each socket address (protocol/network address/port) is normally permitted.
Error: listen tcp 127.0.0.1:11434: bind: Only one usage of each socket address (protocol/network address/port) is normally permitted.
Error: listen tcp 127.0.0.1:11434: bind: Only one usage of each socket address (protocol/network address/port) is normally permitted.
Error: listen tcp 127.0.0.1:11434: bind: Only one usage of each socket address (protocol/network address/port) is normally permitted.
Error: listen tcp 127.0.0.1:11434: bind: Only one usage of each socket address (protocol/network address/port) is normally permitted.
Error: listen tcp 127.0.0.1:11434: bind: Only one usage of each socket address (protocol/network address/port) is normally permitted.
Error: listen tcp 127.0.0.1:11434: bind: Only one usage of each socket address (protocol/network address/port) is normally permitted.
Error: listen tcp 127.0.0.1:11434: bind: Only one usage of each socket address (protocol/network address/port) is normally permitted.
Error: listen tcp 127.0.0.1:11434: bind: Only one usage of each socket address (protocol/network address/port) is normally permitted.
Error: listen tcp 127.0.0.1:11434: bind: Only one usage of each socket address (protocol/network address/port) is normally permitted.
Error: listen tcp 127.0.0.1:11434: bind: Only one usage of each socket address (protocol/network address/port) is normally permitted.
Error: listen tcp 127.0.0.1:11434: bind: Only one usage of each socket address (protocol/network address/port) is normally permitted.
Error: listen tcp 127.0.0.1:11434: bind: Only one usage of each socket address (protocol/network address/port) is normally permitted.
Error: listen tcp 127.0.0.1:11434: bind: Only one usage of each socket address (protocol/network address/port) is normally permitted.
Error: listen tcp 127.0.0.1:11434: bind: Only one usage of each socket address (protocol/network address/port) is normally permitted.
Error: listen tcp 127.0.0.1:11434: bind: Only one usage of each socket address (protocol/network address/port) is normally permitted.
Error: listen tcp 127.0.0.1:11434: bind: Only one usage of each socket address (protocol/network address/port) is normally permitted.
Error: listen tcp 127.0.0.1:11434: bind: Only one usage of each socket address (protocol/network address/port) is normally permitted.
Error: listen tcp 127.0.0.1:11434: bind: Only one usage of each socket address (protocol/network address/port) is normally permitted.
Error: listen tcp 127.0.0.1:11434: bind: Only one usage of each socket address (protocol/network address/port) is normally permitted.
Error: listen tcp 127.0.0.1:11434: bind: Only one usage of each socket address (protocol/network address/port) is normally permitted.
Error: listen tcp 127.0.0.1:11434: bind: Only one usage of each socket address (protocol/network address/port) is normally permitted.
Error: listen tcp 127.0.0.1:11434: bind: Only one usage of each socket address (protocol/network address/port) is normally permitted.
Error: listen tcp 127.0.0.1:11434: bind: Only one usage of each socket address (protocol/network address/port) is normally permitted.
Error: listen tcp 127.0.0.1:11434: bind: Only one usage of each socket address (protocol/network address/port) is normally permitted.
Error: listen tcp 127.0.0.1:11434: bind: Only one usage of each socket address (protocol/network address/port) is normally permitted.
Error: listen tcp 127.0.0.1:11434: bind: Only one usage of each socket address (protocol/network address/port) is normally permitted.
Error: listen tcp 127.0.0.1:11434: bind: Only one usage of each socket address (protocol/network address/port) is normally permitted.
Error: listen tcp 127.0.0.1:11434: bind: Only one usage of each socket address (protocol/network address/port) is normally permitted.
Error: listen tcp 127.0.0.1:11434: bind: Only one usage of each socket address (protocol/network address/port) is normally permitted.
Error: listen tcp 127.0.0.1:11434: bind: Only one usage of each socket address (protocol/network address/port) is normally permitted.
Error: listen tcp 127.0.0.1:11434: bind: Only one usage of each socket address (protocol/network address/port) is normally permitted.
Error: listen tcp 127.0.0.1:11434: bind: Only one usage of each socket address (protocol/network address/port) is normally permitted.
Error: listen tcp 127.0.0.1:11434: bind: Only one usage of each socket address (protocol/network address/port) is normally permitted.
Error: listen tcp 127.0.0.1:11434: bind: Only one usage of each socket address (protocol/network address/port) is normally permitted.
Error: listen tcp 127.0.0.1:11434: bind: Only one usage of each socket address (protocol/network address/port) is normally permitted.
Error: listen tcp 127.0.0.1:11434: bind: Only one usage of each socket address (protocol/network address/port) is normally permitted.
Error: listen tcp 127.0.0.1:11434: bind: Only one usage of each socket address (protocol/network address/port) is normally permitted.
Error: listen tcp 127.0.0.1:11434: bind: Only one usage of each socket address (protocol/network address/port) is normally permitted.
Error: listen tcp 127.0.0.1:11434: bind: Only one usage of each socket address (protocol/network address/port) is normally permitted.
Error: listen tcp 127.0.0.1:11434: bind: Only one usage of each socket address (protocol/network address/port) is normally permitted.
Error: listen tcp 127.0.0.1:11434: bind: Only one usage of each socket address (protocol/network address/port) is normally permitted.
Error: listen tcp 127.0.0.1:11434: bind: Only one usage of each socket address (protocol/network address/port) is normally permitted.
Error: listen tcp 127.0.0.1:11434: bind: Only one usage of each socket address (protocol/network address/port) is normally permitted.
Error: listen tcp 127.0.0.1:11434: bind: Only one usage of each socket address (protocol/network address/port) is normally permitted.
Error: listen tcp 127.0.0.1:11434: bind: Only one usage of each socket address (protocol/network address/port) is normally permitted.
Error: listen tcp 127.0.0.1:11434: bind: Only one usage of each socket address (protocol/network address/port) is normally permitted.
Error: listen tcp 127.0.0.1:11434: bind: Only one usage of each socket address (protocol/network address/port) is normally permitted.
Error: listen tcp 127.0.0.1:11434: bind: Only one usage of each socket address (protocol/network address/port) is normally permitted.
Error: listen tcp 127.0.0.1:11434: bind: Only one usage of each socket address (protocol/network address/port) is normally permitted.
Error: listen tcp 127.0.0.1:11434: bind: Only one usage of each socket address (protocol/network address/port) is normally permitted.
Error: listen tcp 127.0.0.1:11434: bind: Only one usage of each socket address (protocol/network address/port) is normally permitted.
Error: listen tcp 127.0.0.1:11434: bind: Only one usage of each socket address (protocol/network address/port) is normally permitted.
Error: listen tcp 127.0.0.1:11434: bind: Only one usage of each socket address (protocol/network address/port) is normally permitted.
Error: listen tcp 127.0.0.1:11434: bind: Only one usage of each socket address (protocol/network address/port) is normally permitted.
Error: listen tcp 127.0.0.1:11434: bind: Only one usage of each socket address (protocol/network address/port) is normally permitted.
Error: listen tcp 127.0.0.1:11434: bind: Only one usage of each socket address (protocol/network address/port) is normally permitted.
Error: listen tcp 127.0.0.1:11434: bind: Only one usage of each socket address (protocol/network address/port) is normally permitted.
Error: listen tcp 127.0.0.1:11434: bind: Only one usage of each socket address (protocol/network address/port) is normally permitted.
Error: listen tcp 127.0.0.1:11434: bind: Only one usage of each socket address (protocol/network address/port) is normally permitted.
Error: listen tcp 127.0.0.1:11434: bind: Only one usage of each socket address (protocol/network address/port) is normally permitted.
Error: listen tcp 127.0.0.1:11434: bind: Only one usage of each socket address (protocol/network address/port) is normally permitted.
Error: listen tcp 127.0.0.1:11434: bind: Only one usage of each socket address (protocol/network address/port) is normally permitted.
Error: listen tcp 127.0.0.1:11434: bind: Only one usage of each socket address (protocol/network address/port) is normally permitted.
Error: listen tcp 127.0.0.1:11434: bind: Only one usage of each socket address (protocol/network address/port) is normally permitted.
Error: listen tcp 127.0.0.1:11434: bind: Only one usage of each socket address (protocol/network address/port) is normally permitted.
Error: listen tcp 127.0.0.1:11434: bind: Only one usage of each socket address (protocol/network address/port) is normally permitted.
Error: listen tcp 127.0.0.1:11434: bind: Only one usage of each socket address (protocol/network address/port) is normally permitted.
Error: listen tcp 127.0.0.1:11434: bind: Only one usage of each socket address (protocol/network address/port) is normally permitted.
Error: listen tcp 127.0.0.1:11434: bind: Only one usage of each socket address (protocol/network address/port) is normally permitted.
Error: listen tcp 127.0.0.1:11434: bind: Only one usage of each socket address (protocol/network address/port) is normally permitted.
Error: listen tcp 127.0.0.1:11434: bind: Only one usage of each socket address (protocol/network address/port) is normally permitted.
Error: listen tcp 127.0.0.1:11434: bind: Only one usage of each socket address (protocol/network address/port) is normally permitted.
Error: listen tcp 127.0.0.1:11434: bind: Only one usage of each socket address (protocol/network address/port) is normally permitted.
Error: listen tcp 127.0.0.1:11434: bind: Only one usage of each socket address (protocol/network address/port) is normally permitted.
Error: listen tcp 127.0.0.1:11434: bind: Only one usage of each socket address (protocol/network address/port) is normally permitted.
Error: listen tcp 127.0.0.1:11434: bind: Only one usage of each socket address (protocol/network address/port) is normally permitted.
Error: listen tcp 127.0.0.1:11434: bind: Only one usage of each socket address (protocol/network address/port) is normally permitted.
Error: listen tcp 127.0.0.1:11434: bind: Only one usage of each socket address (protocol/network address/port) is normally permitted.
Error: listen tcp 127.0.0.1:11434: bind: Only one usage of each socket address (protocol/network address/port) is normally permitted.
Error: listen tcp 127.0.0.1:11434: bind: Only one usage of each socket address (protocol/network address/port) is normally permitted.
Error: listen tcp 127.0.0.1:11434: bind: Only one usage of each socket address (protocol/network address/port) is normally permitted.
Error: listen tcp 127.0.0.1:11434: bind: Only one usage of each socket address (protocol/network address/port) is normally permitted.
Error: listen tcp 127.0.0.1:11434: bind: Only one usage of each socket address (protocol/network address/port) is normally permitted.
Error: listen tcp 127.0.0.1:11434: bind: Only one usage of each socket address (protocol/network address/port) is normally permitted.
Error: listen tcp 127.0.0.1:11434: bind: Only one usage of each socket address (protocol/network address/port) is normally permitted.
Error: listen tcp 127.0.0.1:11434: bind: Only one usage of each socket address (protocol/network address/port) is normally permitted.
Error: listen tcp 127.0.0.1:11434: bind: Only one usage of each socket address (protocol/network address/port) is normally permitted.
Error: listen tcp 127.0.0.1:11434: bind: Only one usage of each socket address (protocol/network address/port) is normally permitted.
Error: listen tcp 127.0.0.1:11434: bind: Only one usage of each socket address (protocol/network address/port) is normally permitted.
Error: listen tcp 127.0.0.1:11434: bind: Only one usage of each socket address (protocol/network address/port) is normally permitted.
Error: listen tcp 127.0.0.1:11434: bind: Only one usage of each socket address (protocol/network address/port) is normally permitted.
Error: listen tcp 127.0.0.1:11434: bind: Only one usage of each socket address (protocol/network address/port) is normally permitted.
Error: listen tcp 127.0.0.1:11434: bind: Only one usage of each socket address (protocol/network address/port) is normally permitted.
Error: listen tcp 127.0.0.1:11434: bind: Only one usage of each socket address (protocol/network address/port) is normally permitted.
Error: listen tcp 127.0.0.1:11434: bind: Only one usage of each socket address (protocol/network address/port) is normally permitted.
Error: listen tcp 127.0.0.1:11434: bind: Only one usage of each socket address (protocol/network address/port) is normally permitted.
Error: listen tcp 127.0.0.1:11434: bind: Only one usage of each socket address (protocol/network address/port) is normally permitted.
Error: listen tcp 127.0.0.1:11434: bind: Only one usage of each socket address (protocol/network address/port) is normally permitted.
Error: listen tcp 127.0.0.1:11434: bind: Only one usage of each socket address (protocol/network address/port) is normally permitted.
Error: listen tcp 127.0.0.1:11434: bind: Only one usage of each socket address (protocol/network address/port) is normally permitted.
Error: listen tcp 127.0.0.1:11434: bind: Only one usage of each socket address (protocol/network address/port) is normally permitted.
Error: listen tcp 127.0.0.1:11434: bind: Only one usage of each socket address (protocol/network address/port) is normally permitted.
Error: listen tcp 127.0.0.1:11434: bind: Only one usage of each socket address (protocol/network address/port) is normally permitted.
Error: listen tcp 127.0.0.1:11434: bind: Only one usage of each socket address (protocol/network address/port) is normally permitted.
Error: listen tcp 127.0.0.1:11434: bind: Only one usage of each socket address (protocol/network address/port) is normally permitted.
Error: listen tcp 127.0.0.1:11434: bind: Only one usage of each socket address (protocol/network address/port) is normally permitted.
Error: listen tcp 127.0.0.1:11434: bind: Only one usage of each socket address (protocol/network address/port) is normally permitted.
Error: listen tcp 127.0.0.1:11434: bind: Only one usage of each socket address (protocol/network address/port) is normally permitted.
Error: listen tcp 127.0.0.1:11434: bind: Only one usage of each socket address (protocol/network address/port) is normally permitted.
Error: listen tcp 127.0.0.1:11434: bind: Only one usage of each socket address (protocol/network address/port) is normally permitted.
Error: listen tcp 127.0.0.1:11434: bind: Only one usage of each socket address (protocol/network address/port) is normally permitted.
Error: listen tcp 127.0.0.1:11434: bind: Only one usage of each socket address (protocol/network address/port) is normally permitted.
Error: listen tcp 127.0.0.1:11434: bind: Only one usage of each socket address (protocol/network address/port) is normally permitted.
Error: listen tcp 127.0.0.1:11434: bind: Only one usage of each socket address (protocol/network address/port) is normally permitted.
Error: listen tcp 127.0.0.1:11434: bind: Only one usage of each socket address (protocol/network address/port) is normally permitted.
Error: listen tcp 127.0.0.1:11434: bind: Only one usage of each socket address (protocol/network address/port) is normally permitted.
Error: listen tcp 127.0.0.1:11434: bind: Only one usage of each socket address (protocol/network address/port) is normally permitted.
Error: listen tcp 127.0.0.1:11434: bind: Only one usage of each socket address (protocol/network address/port) is normally permitted.
Error: listen tcp 127.0.0.1:11434: bind: Only one usage of each socket address (protocol/network address/port) is normally permitted.
Error: listen tcp 127.0.0.1:11434: bind: Only one usage of each socket address (protocol/network address/port) is normally permitted.
Error: listen tcp 127.0.0.1:11434: bind: Only one usage of each socket address (protocol/network address/port) is normally permitted.
Error: listen tcp 127.0.0.1:11434: bind: Only one usage of each socket address (protocol/network address/port) is normally permitted.
Error: listen tcp 127.0.0.1:11434: bind: Only one usage of each socket address (protocol/network address/port) is normally permitted.
Error: listen tcp 127.0.0.1:11434: bind: Only one usage of each socket address (protocol/network address/port) is normally permitted.
Error: listen tcp 127.0.0.1:11434: bind: Only one usage of each socket address (protocol/network address/port) is normally permitted.
Error: listen tcp 127.0.0.1:11434: bind: Only one usage of each socket address (protocol/network address/port) is normally permitted.
Error: listen tcp 127.0.0.1:11434: bind: Only one usage of each socket address (protocol/network address/port) is normally permitted.
Error: listen tcp 127.0.0.1:11434: bind: Only one usage of each socket address (protocol/network address/port) is normally permitted.
Error: listen tcp 127.0.0.1:11434: bind: Only one usage of each socket address (protocol/network address/port) is normally permitted.
Error: listen tcp 127.0.0.1:11434: bind: Only one usage of each socket address (protocol/network address/port) is normally permitted.
Error: listen tcp 127.0.0.1:11434: bind: Only one usage of each socket address (protocol/network address/port) is normally permitted.
Error: listen tcp 127.0.0.1:11434: bind: Only one usage of each socket address (protocol/network address/port) is normally permitted.
Error: listen tcp 127.0.0.1:11434: bind: Only one usage of each socket address (protocol/network address/port) is normally permitted.
Error: listen tcp 127.0.0.1:11434: bind: Only one usage of each socket address (protocol/network address/port) is normally permitted.
Error: listen tcp 127.0.0.1:11434: bind: Only one usage of each socket address (protocol/network address/port) is normally permitted.
Error: listen tcp 127.0.0.1:11434: bind: Only one usage of each socket address (protocol/network address/port) is normally permitted.
Error: listen tcp 127.0.0.1:11434: bind: Only one usage of each socket address (protocol/network address/port) is normally permitted.
Error: listen tcp 127.0.0.1:11434: bind: Only one usage of each socket address (protocol/network address/port) is normally permitted.
Error: listen tcp 127.0.0.1:11434: bind: Only one usage of each socket address (protocol/network address/port) is normally permitted.
Error: listen tcp 127.0.0.1:11434: bind: Only one usage of each socket address (protocol/network address/port) is normally permitted.
Error: listen tcp 127.0.0.1:11434: bind: Only one usage of each socket address (protocol/network address/port) is normally permitted.
Error: listen tcp 127.0.0.1:11434: bind: Only one usage of each socket address (protocol/network address/port) is normally permitted.
Error: listen tcp 127.0.0.1:11434: bind: Only one usage of each socket address (protocol/network address/port) is normally permitted.
Error: listen tcp 127.0.0.1:11434: bind: Only one usage of each socket address (protocol/network address/port) is normally permitted.
Error: listen tcp 127.0.0.1:11434: bind: Only one usage of each socket address (protocol/network address/port) is normally permitted.
Error: listen tcp 127.0.0.1:11434: bind: Only one usage of each socket address (protocol/network address/port) is normally permitted.
Error: listen tcp 127.0.0.1:11434: bind: Only one usage of each socket address (protocol/network address/port) is normally permitted.
Error: listen tcp 127.0.0.1:11434: bind: Only one usage of each socket address (protocol/network address/port) is normally permitted.
Error: listen tcp 127.0.0.1:11434: bind: Only one usage of each socket address (protocol/network address/port) is normally permitted.
Error: listen tcp 127.0.0.1:11434: bind: Only one usage of each socket address (protocol/network address/port) is normally permitted.
Error: listen tcp 127.0.0.1:11434: bind: Only one usage of each socket address (protocol/network address/port) is normally permitted.
time=2025-10-23T12:09:41.462-04:00 level=INFO source=routes.go:1511 msg="server config" env="map[CUDA_VISIBLE_DEVICES: GGML_VK_VISIBLE_DEVICES: GPU_DEVICE_ORDINAL: HIP_VISIBLE_DEVICES: HSA_OVERRIDE_GFX_VERSION: HTTPS_PROXY: HTTP_PROXY: NO_PROXY: OLLAMA_CONTEXT_LENGTH:4096 OLLAMA_DEBUG:INFO OLLAMA_FLASH_ATTENTION:false OLLAMA_GPU_OVERHEAD:0 OLLAMA_HOST:http://127.0.0.1:11434 OLLAMA_INTEL_GPU:false OLLAMA_KEEP_ALIVE:5m0s OLLAMA_KV_CACHE_TYPE: OLLAMA_LLM_LIBRARY: OLLAMA_LOAD_TIMEOUT:5m0s OLLAMA_MAX_LOADED_MODELS:0 OLLAMA_MAX_QUEUE:512 OLLAMA_MODELS:C:\\Users\\frost\\.ollama\\models OLLAMA_MULTIUSER_CACHE:false OLLAMA_NEW_ENGINE:false OLLAMA_NOHISTORY:false OLLAMA_NOPRUNE:false OLLAMA_NUM_PARALLEL:1 OLLAMA_ORIGINS:[http://localhost https://localhost http://localhost:* https://localhost:* http://127.0.0.1 https://127.0.0.1 http://127.0.0.1:* https://127.0.0.1:* http://0.0.0.0 https://0.0.0.0 http://0.0.0.0:* https://0.0.0.0:* app://* file://* tauri://* vscode-webview://* vscode-file://*] OLLAMA_REMOTES:[ollama.com] OLLAMA_SCHED_SPREAD:false ROCR_VISIBLE_DEVICES:]"
time=2025-10-23T12:09:41.466-04:00 level=INFO source=images.go:522 msg="total blobs: 17"
time=2025-10-23T12:09:41.467-04:00 level=INFO source=images.go:529 msg="total unused blobs removed: 0"
time=2025-10-23T12:09:41.469-04:00 level=INFO source=routes.go:1564 msg="Listening on 127.0.0.1:11434 (version 0.12.6)"
time=2025-10-23T12:09:41.470-04:00 level=INFO source=runner.go:80 msg="discovering available GPUs..."
time=2025-10-23T12:09:42.286-04:00 level=INFO source=types.go:112 msg="inference compute" id=GPU-b28645fa-aaf6-86a1-ab1b-ef8d61df29f2 library=CUDA compute=12.0 name=CUDA0 description="NVIDIA GeForce RTX 5070" libdirs=ollama,cuda_v12 driver=12.9 pci_id=01:00.0 type=discrete total="11.9 GiB" available="10.5 GiB"
time=2025-10-23T12:09:42.286-04:00 level=INFO source=routes.go:1605 msg="entering low vram mode" "total vram"="11.9 GiB" threshold="20.0 GiB"
time=2025-10-23T12:11:14.441-04:00 level=INFO source=routes.go:1511 msg="server config" env="map[CUDA_VISIBLE_DEVICES: GGML_VK_VISIBLE_DEVICES: GPU_DEVICE_ORDINAL: HIP_VISIBLE_DEVICES: HSA_OVERRIDE_GFX_VERSION: HTTPS_PROXY: HTTP_PROXY: NO_PROXY: OLLAMA_CONTEXT_LENGTH:4096 OLLAMA_DEBUG:INFO OLLAMA_FLASH_ATTENTION:false OLLAMA_GPU_OVERHEAD:0 OLLAMA_HOST:http://127.0.0.1:11434 OLLAMA_INTEL_GPU:false OLLAMA_KEEP_ALIVE:5m0s OLLAMA_KV_CACHE_TYPE: OLLAMA_LLM_LIBRARY: OLLAMA_LOAD_TIMEOUT:5m0s OLLAMA_MAX_LOADED_MODELS:0 OLLAMA_MAX_QUEUE:512 OLLAMA_MODELS:C:\\Users\\frost\\.ollama\\models OLLAMA_MULTIUSER_CACHE:false OLLAMA_NEW_ENGINE:false OLLAMA_NOHISTORY:false OLLAMA_NOPRUNE:false OLLAMA_NUM_PARALLEL:1 OLLAMA_ORIGINS:[http://localhost https://localhost http://localhost:* https://localhost:* http://127.0.0.1 https://127.0.0.1 http://127.0.0.1:* https://127.0.0.1:* http://0.0.0.0 https://0.0.0.0 http://0.0.0.0:* https://0.0.0.0:* app://* file://* tauri://* vscode-webview://* vscode-file://*] OLLAMA_REMOTES:[ollama.com] OLLAMA_SCHED_SPREAD:false ROCR_VISIBLE_DEVICES:]"
time=2025-10-23T12:11:14.447-04:00 level=INFO source=images.go:522 msg="total blobs: 17"
time=2025-10-23T12:11:14.448-04:00 level=INFO source=images.go:529 msg="total unused blobs removed: 0"
time=2025-10-23T12:11:14.450-04:00 level=INFO source=routes.go:1564 msg="Listening on 127.0.0.1:11434 (version 0.12.6)"
time=2025-10-23T12:11:14.451-04:00 level=INFO source=runner.go:80 msg="discovering available GPUs..."
time=2025-10-23T12:11:15.187-04:00 level=INFO source=types.go:112 msg="inference compute" id=GPU-b28645fa-aaf6-86a1-ab1b-ef8d61df29f2 library=CUDA compute=12.0 name=CUDA0 description="NVIDIA GeForce RTX 5070" libdirs=ollama,cuda_v12 driver=12.9 pci_id=01:00.0 type=discrete total="11.9 GiB" available="10.5 GiB"
time=2025-10-23T12:11:15.187-04:00 level=INFO source=routes.go:1605 msg="entering low vram mode" "total vram"="11.9 GiB" threshold="20.0 GiB"
time=2025-10-23T12:13:19.624-04:00 level=INFO source=routes.go:1511 msg="server config" env="map[CUDA_VISIBLE_DEVICES: GGML_VK_VISIBLE_DEVICES: GPU_DEVICE_ORDINAL: HIP_VISIBLE_DEVICES: HSA_OVERRIDE_GFX_VERSION: HTTPS_PROXY: HTTP_PROXY: NO_PROXY: OLLAMA_CONTEXT_LENGTH:4096 OLLAMA_DEBUG:INFO OLLAMA_FLASH_ATTENTION:false OLLAMA_GPU_OVERHEAD:0 OLLAMA_HOST:http://127.0.0.1:11434 OLLAMA_INTEL_GPU:false OLLAMA_KEEP_ALIVE:5m0s OLLAMA_KV_CACHE_TYPE: OLLAMA_LLM_LIBRARY: OLLAMA_LOAD_TIMEOUT:5m0s OLLAMA_MAX_LOADED_MODELS:0 OLLAMA_MAX_QUEUE:512 OLLAMA_MODELS:C:\\Users\\frost\\.ollama\\models OLLAMA_MULTIUSER_CACHE:false OLLAMA_NEW_ENGINE:false OLLAMA_NOHISTORY:false OLLAMA_NOPRUNE:false OLLAMA_NUM_PARALLEL:1 OLLAMA_ORIGINS:[http://localhost https://localhost http://localhost:* https://localhost:* http://127.0.0.1 https://127.0.0.1 http://127.0.0.1:* https://127.0.0.1:* http://0.0.0.0 https://0.0.0.0 http://0.0.0.0:* https://0.0.0.0:* app://* file://* tauri://* vscode-webview://* vscode-file://*] OLLAMA_REMOTES:[ollama.com] OLLAMA_SCHED_SPREAD:false ROCR_VISIBLE_DEVICES:]"
time=2025-10-23T12:13:19.644-04:00 level=INFO source=images.go:522 msg="total blobs: 17"
time=2025-10-23T12:13:19.653-04:00 level=INFO source=images.go:529 msg="total unused blobs removed: 0"
time=2025-10-23T12:13:19.675-04:00 level=INFO source=routes.go:1564 msg="Listening on 127.0.0.1:11434 (version 0.12.6)"
time=2025-10-23T12:13:19.677-04:00 level=INFO source=runner.go:80 msg="discovering available GPUs..."
time=2025-10-23T12:13:20.768-04:00 level=INFO source=types.go:112 msg="inference compute" id=GPU-b28645fa-aaf6-86a1-ab1b-ef8d61df29f2 library=CUDA compute=12.0 name=CUDA0 description="NVIDIA GeForce RTX 5070" libdirs=ollama,cuda_v12 driver=12.9 pci_id=01:00.0 type=discrete total="11.9 GiB" available="10.5 GiB"
time=2025-10-23T12:13:20.768-04:00 level=INFO source=routes.go:1605 msg="entering low vram mode" "total vram"="11.9 GiB" threshold="20.0 GiB"
time=2025-10-23T12:13:57.628-04:00 level=INFO source=routes.go:1511 msg="server config" env="map[CUDA_VISIBLE_DEVICES: GGML_VK_VISIBLE_DEVICES: GPU_DEVICE_ORDINAL: HIP_VISIBLE_DEVICES: HSA_OVERRIDE_GFX_VERSION: HTTPS_PROXY: HTTP_PROXY: NO_PROXY: OLLAMA_CONTEXT_LENGTH:4096 OLLAMA_DEBUG:INFO OLLAMA_FLASH_ATTENTION:false OLLAMA_GPU_OVERHEAD:0 OLLAMA_HOST:http://127.0.0.1:11434 OLLAMA_INTEL_GPU:false OLLAMA_KEEP_ALIVE:5m0s OLLAMA_KV_CACHE_TYPE: OLLAMA_LLM_LIBRARY: OLLAMA_LOAD_TIMEOUT:5m0s OLLAMA_MAX_LOADED_MODELS:0 OLLAMA_MAX_QUEUE:512 OLLAMA_MODELS:C:\\Users\\frost\\.ollama\\models OLLAMA_MULTIUSER_CACHE:false OLLAMA_NEW_ENGINE:false OLLAMA_NOHISTORY:false OLLAMA_NOPRUNE:false OLLAMA_NUM_PARALLEL:1 OLLAMA_ORIGINS:[http://localhost https://localhost http://localhost:* https://localhost:* http://127.0.0.1 https://127.0.0.1 http://127.0.0.1:* https://127.0.0.1:* http://0.0.0.0 https://0.0.0.0 http://0.0.0.0:* https://0.0.0.0:* app://* file://* tauri://* vscode-webview://* vscode-file://*] OLLAMA_REMOTES:[ollama.com] OLLAMA_SCHED_SPREAD:false ROCR_VISIBLE_DEVICES:]"
time=2025-10-23T12:13:57.633-04:00 level=INFO source=images.go:522 msg="total blobs: 17"
time=2025-10-23T12:13:57.634-04:00 level=INFO source=images.go:529 msg="total unused blobs removed: 0"
time=2025-10-23T12:13:57.636-04:00 level=INFO source=routes.go:1564 msg="Listening on 127.0.0.1:11434 (version 0.12.6)"
time=2025-10-23T12:13:57.637-04:00 level=INFO source=runner.go:80 msg="discovering available GPUs..."
time=2025-10-23T12:13:58.504-04:00 level=INFO source=types.go:112 msg="inference compute" id=GPU-b28645fa-aaf6-86a1-ab1b-ef8d61df29f2 library=CUDA compute=12.0 name=CUDA0 description="NVIDIA GeForce RTX 5070" libdirs=ollama,cuda_v12 driver=12.9 pci_id=01:00.0 type=discrete total="11.9 GiB" available="10.5 GiB"
time=2025-10-23T12:13:58.504-04:00 level=INFO source=routes.go:1605 msg="entering low vram mode" "total vram"="11.9 GiB" threshold="20.0 GiB"
time=2025-10-23T12:14:32.481-04:00 level=INFO source=routes.go:1511 msg="server config" env="map[CUDA_VISIBLE_DEVICES: GGML_VK_VISIBLE_DEVICES: GPU_DEVICE_ORDINAL: HIP_VISIBLE_DEVICES: HSA_OVERRIDE_GFX_VERSION: HTTPS_PROXY: HTTP_PROXY: NO_PROXY: OLLAMA_CONTEXT_LENGTH:4096 OLLAMA_DEBUG:INFO OLLAMA_FLASH_ATTENTION:false OLLAMA_GPU_OVERHEAD:0 OLLAMA_HOST:http://127.0.0.1:11434 OLLAMA_INTEL_GPU:false OLLAMA_KEEP_ALIVE:5m0s OLLAMA_KV_CACHE_TYPE: OLLAMA_LLM_LIBRARY: OLLAMA_LOAD_TIMEOUT:5m0s OLLAMA_MAX_LOADED_MODELS:0 OLLAMA_MAX_QUEUE:512 OLLAMA_MODELS:C:\\Users\\frost\\.ollama\\models OLLAMA_MULTIUSER_CACHE:false OLLAMA_NEW_ENGINE:false OLLAMA_NOHISTORY:false OLLAMA_NOPRUNE:false OLLAMA_NUM_PARALLEL:1 OLLAMA_ORIGINS:[http://localhost https://localhost http://localhost:* https://localhost:* http://127.0.0.1 https://127.0.0.1 http://127.0.0.1:* https://127.0.0.1:* http://0.0.0.0 https://0.0.0.0 http://0.0.0.0:* https://0.0.0.0:* app://* file://* tauri://* vscode-webview://* vscode-file://*] OLLAMA_REMOTES:[ollama.com] OLLAMA_SCHED_SPREAD:false ROCR_VISIBLE_DEVICES:]"
time=2025-10-23T12:14:32.487-04:00 level=INFO source=images.go:522 msg="total blobs: 17"
time=2025-10-23T12:14:32.489-04:00 level=INFO source=images.go:529 msg="total unused blobs removed: 0"
time=2025-10-23T12:14:32.489-04:00 level=INFO source=routes.go:1564 msg="Listening on 127.0.0.1:11434 (version 0.12.6)"
time=2025-10-23T12:14:32.491-04:00 level=INFO source=runner.go:80 msg="discovering available GPUs..."
time=2025-10-23T12:14:33.248-04:00 level=INFO source=types.go:112 msg="inference compute" id=GPU-b28645fa-aaf6-86a1-ab1b-ef8d61df29f2 library=CUDA compute=12.0 name=CUDA0 description="NVIDIA GeForce RTX 5070" libdirs=ollama,cuda_v12 driver=12.9 pci_id=01:00.0 type=discrete total="11.9 GiB" available="10.5 GiB"
time=2025-10-23T12:14:33.248-04:00 level=INFO source=routes.go:1605 msg="entering low vram mode" "total vram"="11.9 GiB" threshold="20.0 GiB"
time=2025-10-23T12:16:06.510-04:00 level=INFO source=routes.go:1511 msg="server config" env="map[CUDA_VISIBLE_DEVICES: GGML_VK_VISIBLE_DEVICES: GPU_DEVICE_ORDINAL: HIP_VISIBLE_DEVICES: HSA_OVERRIDE_GFX_VERSION: HTTPS_PROXY: HTTP_PROXY: NO_PROXY: OLLAMA_CONTEXT_LENGTH:4096 OLLAMA_DEBUG:INFO OLLAMA_FLASH_ATTENTION:false OLLAMA_GPU_OVERHEAD:0 OLLAMA_HOST:http://127.0.0.1:11434 OLLAMA_INTEL_GPU:false OLLAMA_KEEP_ALIVE:5m0s OLLAMA_KV_CACHE_TYPE: OLLAMA_LLM_LIBRARY: OLLAMA_LOAD_TIMEOUT:5m0s OLLAMA_MAX_LOADED_MODELS:0 OLLAMA_MAX_QUEUE:512 OLLAMA_MODELS:C:\\Users\\frost\\.ollama\\models OLLAMA_MULTIUSER_CACHE:false OLLAMA_NEW_ENGINE:false OLLAMA_NOHISTORY:false OLLAMA_NOPRUNE:false OLLAMA_NUM_PARALLEL:1 OLLAMA_ORIGINS:[http://localhost https://localhost http://localhost:* https://localhost:* http://127.0.0.1 https://127.0.0.1 http://127.0.0.1:* https://127.0.0.1:* http://0.0.0.0 https://0.0.0.0 http://0.0.0.0:* https://0.0.0.0:* app://* file://* tauri://* vscode-webview://* vscode-file://*] OLLAMA_REMOTES:[ollama.com] OLLAMA_SCHED_SPREAD:false ROCR_VISIBLE_DEVICES:]"
time=2025-10-23T12:16:06.515-04:00 level=INFO source=images.go:522 msg="total blobs: 17"
time=2025-10-23T12:16:06.517-04:00 level=INFO source=images.go:529 msg="total unused blobs removed: 0"
time=2025-10-23T12:16:06.518-04:00 level=INFO source=routes.go:1564 msg="Listening on 127.0.0.1:11434 (version 0.12.6)"
time=2025-10-23T12:16:06.520-04:00 level=INFO source=runner.go:80 msg="discovering available GPUs..."
time=2025-10-23T12:16:07.277-04:00 level=INFO source=types.go:112 msg="inference compute" id=GPU-b28645fa-aaf6-86a1-ab1b-ef8d61df29f2 library=CUDA compute=12.0 name=CUDA0 description="NVIDIA GeForce RTX 5070" libdirs=ollama,cuda_v12 driver=12.9 pci_id=01:00.0 type=discrete total="11.9 GiB" available="10.5 GiB"
time=2025-10-23T12:16:07.277-04:00 level=INFO source=routes.go:1605 msg="entering low vram mode" "total vram"="11.9 GiB" threshold="20.0 GiB"
time=2025-10-23T12:18:33.967-04:00 level=INFO source=routes.go:1511 msg="server config" env="map[CUDA_VISIBLE_DEVICES: GGML_VK_VISIBLE_DEVICES: GPU_DEVICE_ORDINAL: HIP_VISIBLE_DEVICES: HSA_OVERRIDE_GFX_VERSION: HTTPS_PROXY: HTTP_PROXY: NO_PROXY: OLLAMA_CONTEXT_LENGTH:4096 OLLAMA_DEBUG:INFO OLLAMA_FLASH_ATTENTION:false OLLAMA_GPU_OVERHEAD:0 OLLAMA_HOST:http://127.0.0.1:11434 OLLAMA_INTEL_GPU:false OLLAMA_KEEP_ALIVE:5m0s OLLAMA_KV_CACHE_TYPE: OLLAMA_LLM_LIBRARY: OLLAMA_LOAD_TIMEOUT:5m0s OLLAMA_MAX_LOADED_MODELS:0 OLLAMA_MAX_QUEUE:512 OLLAMA_MODELS:C:\\Users\\frost\\.ollama\\models OLLAMA_MULTIUSER_CACHE:false OLLAMA_NEW_ENGINE:false OLLAMA_NOHISTORY:false OLLAMA_NOPRUNE:false OLLAMA_NUM_PARALLEL:1 OLLAMA_ORIGINS:[http://localhost https://localhost http://localhost:* https://localhost:* http://127.0.0.1 https://127.0.0.1 http://127.0.0.1:* https://127.0.0.1:* http://0.0.0.0 https://0.0.0.0 http://0.0.0.0:* https://0.0.0.0:* app://* file://* tauri://* vscode-webview://* vscode-file://*] OLLAMA_REMOTES:[ollama.com] OLLAMA_SCHED_SPREAD:false ROCR_VISIBLE_DEVICES:]"
time=2025-10-23T12:18:33.972-04:00 level=INFO source=images.go:522 msg="total blobs: 17"
time=2025-10-23T12:18:33.974-04:00 level=INFO source=images.go:529 msg="total unused blobs removed: 0"
time=2025-10-23T12:18:33.975-04:00 level=INFO source=routes.go:1564 msg="Listening on 127.0.0.1:11434 (version 0.12.6)"
time=2025-10-23T12:18:33.976-04:00 level=INFO source=runner.go:80 msg="discovering available GPUs..."
time=2025-10-23T12:18:34.702-04:00 level=INFO source=types.go:112 msg="inference compute" id=GPU-b28645fa-aaf6-86a1-ab1b-ef8d61df29f2 library=CUDA compute=12.0 name=CUDA0 description="NVIDIA GeForce RTX 5070" libdirs=ollama,cuda_v12 driver=12.9 pci_id=01:00.0 type=discrete total="11.9 GiB" available="10.5 GiB"
time=2025-10-23T12:18:34.702-04:00 level=INFO source=routes.go:1605 msg="entering low vram mode" "total vram"="11.9 GiB" threshold="20.0 GiB"
time=2025-10-23T12:22:37.568-04:00 level=INFO source=routes.go:1511 msg="server config" env="map[CUDA_VISIBLE_DEVICES: GGML_VK_VISIBLE_DEVICES: GPU_DEVICE_ORDINAL: HIP_VISIBLE_DEVICES: HSA_OVERRIDE_GFX_VERSION: HTTPS_PROXY: HTTP_PROXY: NO_PROXY: OLLAMA_CONTEXT_LENGTH:4096 OLLAMA_DEBUG:INFO OLLAMA_FLASH_ATTENTION:false OLLAMA_GPU_OVERHEAD:0 OLLAMA_HOST:http://127.0.0.1:11434 OLLAMA_INTEL_GPU:false OLLAMA_KEEP_ALIVE:5m0s OLLAMA_KV_CACHE_TYPE: OLLAMA_LLM_LIBRARY: OLLAMA_LOAD_TIMEOUT:5m0s OLLAMA_MAX_LOADED_MODELS:0 OLLAMA_MAX_QUEUE:512 OLLAMA_MODELS:C:\\Users\\frost\\.ollama\\models OLLAMA_MULTIUSER_CACHE:false OLLAMA_NEW_ENGINE:false OLLAMA_NOHISTORY:false OLLAMA_NOPRUNE:false OLLAMA_NUM_PARALLEL:1 OLLAMA_ORIGINS:[http://localhost https://localhost http://localhost:* https://localhost:* http://127.0.0.1 https://127.0.0.1 http://127.0.0.1:* https://127.0.0.1:* http://0.0.0.0 https://0.0.0.0 http://0.0.0.0:* https://0.0.0.0:* app://* file://* tauri://* vscode-webview://* vscode-file://*] OLLAMA_REMOTES:[ollama.com] OLLAMA_SCHED_SPREAD:false ROCR_VISIBLE_DEVICES:]"
time=2025-10-23T12:22:37.574-04:00 level=INFO source=images.go:522 msg="total blobs: 17"
time=2025-10-23T12:22:37.576-04:00 level=INFO source=images.go:529 msg="total unused blobs removed: 0"
time=2025-10-23T12:22:37.578-04:00 level=INFO source=routes.go:1564 msg="Listening on 127.0.0.1:11434 (version 0.12.6)"
time=2025-10-23T12:22:37.580-04:00 level=INFO source=runner.go:80 msg="discovering available GPUs..."
time=2025-10-23T12:22:38.464-04:00 level=INFO source=types.go:112 msg="inference compute" id=GPU-b28645fa-aaf6-86a1-ab1b-ef8d61df29f2 library=CUDA compute=12.0 name=CUDA0 description="NVIDIA GeForce RTX 5070" libdirs=ollama,cuda_v12 driver=12.9 pci_id=01:00.0 type=discrete total="11.9 GiB" available="10.5 GiB"
time=2025-10-23T12:22:38.464-04:00 level=INFO source=routes.go:1605 msg="entering low vram mode" "total vram"="11.9 GiB" threshold="20.0 GiB"
time=2025-10-23T12:22:54.699-04:00 level=INFO source=routes.go:1511 msg="server config" env="map[CUDA_VISIBLE_DEVICES: GGML_VK_VISIBLE_DEVICES: GPU_DEVICE_ORDINAL: HIP_VISIBLE_DEVICES: HSA_OVERRIDE_GFX_VERSION: HTTPS_PROXY: HTTP_PROXY: NO_PROXY: OLLAMA_CONTEXT_LENGTH:4096 OLLAMA_DEBUG:INFO OLLAMA_FLASH_ATTENTION:false OLLAMA_GPU_OVERHEAD:0 OLLAMA_HOST:http://127.0.0.1:11434 OLLAMA_INTEL_GPU:false OLLAMA_KEEP_ALIVE:5m0s OLLAMA_KV_CACHE_TYPE: OLLAMA_LLM_LIBRARY: OLLAMA_LOAD_TIMEOUT:5m0s OLLAMA_MAX_LOADED_MODELS:0 OLLAMA_MAX_QUEUE:512 OLLAMA_MODELS:C:\\Users\\frost\\.ollama\\models OLLAMA_MULTIUSER_CACHE:false OLLAMA_NEW_ENGINE:false OLLAMA_NOHISTORY:false OLLAMA_NOPRUNE:false OLLAMA_NUM_PARALLEL:1 OLLAMA_ORIGINS:[http://localhost https://localhost http://localhost:* https://localhost:* http://127.0.0.1 https://127.0.0.1 http://127.0.0.1:* https://127.0.0.1:* http://0.0.0.0 https://0.0.0.0 http://0.0.0.0:* https://0.0.0.0:* app://* file://* tauri://* vscode-webview://* vscode-file://*] OLLAMA_REMOTES:[ollama.com] OLLAMA_SCHED_SPREAD:false ROCR_VISIBLE_DEVICES:]"
time=2025-10-23T12:22:54.704-04:00 level=INFO source=images.go:522 msg="total blobs: 17"
time=2025-10-23T12:22:54.707-04:00 level=INFO source=images.go:529 msg="total unused blobs removed: 0"
time=2025-10-23T12:22:54.708-04:00 level=INFO source=routes.go:1564 msg="Listening on 127.0.0.1:11434 (version 0.12.6)"
time=2025-10-23T12:22:54.709-04:00 level=INFO source=runner.go:80 msg="discovering available GPUs..."
time=2025-10-23T12:22:55.557-04:00 level=INFO source=types.go:112 msg="inference compute" id=GPU-b28645fa-aaf6-86a1-ab1b-ef8d61df29f2 library=CUDA compute=12.0 name=CUDA0 description="NVIDIA GeForce RTX 5070" libdirs=ollama,cuda_v12 driver=12.9 pci_id=01:00.0 type=discrete total="11.9 GiB" available="10.5 GiB"
time=2025-10-23T12:22:55.557-04:00 level=INFO source=routes.go:1605 msg="entering low vram mode" "total vram"="11.9 GiB" threshold="20.0 GiB"
time=2025-10-23T12:29:06.984-04:00 level=INFO source=routes.go:1511 msg="server config" env="map[CUDA_VISIBLE_DEVICES: GGML_VK_VISIBLE_DEVICES: GPU_DEVICE_ORDINAL: HIP_VISIBLE_DEVICES: HSA_OVERRIDE_GFX_VERSION: HTTPS_PROXY: HTTP_PROXY: NO_PROXY: OLLAMA_CONTEXT_LENGTH:4096 OLLAMA_DEBUG:INFO OLLAMA_FLASH_ATTENTION:false OLLAMA_GPU_OVERHEAD:0 OLLAMA_HOST:http://127.0.0.1:11434 OLLAMA_INTEL_GPU:false OLLAMA_KEEP_ALIVE:5m0s OLLAMA_KV_CACHE_TYPE: OLLAMA_LLM_LIBRARY: OLLAMA_LOAD_TIMEOUT:5m0s OLLAMA_MAX_LOADED_MODELS:0 OLLAMA_MAX_QUEUE:512 OLLAMA_MODELS:C:\\Users\\frost\\.ollama\\models OLLAMA_MULTIUSER_CACHE:false OLLAMA_NEW_ENGINE:false OLLAMA_NOHISTORY:false OLLAMA_NOPRUNE:false OLLAMA_NUM_PARALLEL:1 OLLAMA_ORIGINS:[http://localhost https://localhost http://localhost:* https://localhost:* http://127.0.0.1 https://127.0.0.1 http://127.0.0.1:* https://127.0.0.1:* http://0.0.0.0 https://0.0.0.0 http://0.0.0.0:* https://0.0.0.0:* app://* file://* tauri://* vscode-webview://* vscode-file://*] OLLAMA_REMOTES:[ollama.com] OLLAMA_SCHED_SPREAD:false ROCR_VISIBLE_DEVICES:]"
time=2025-10-23T12:29:06.990-04:00 level=INFO source=images.go:522 msg="total blobs: 17"
time=2025-10-23T12:29:06.991-04:00 level=INFO source=images.go:529 msg="total unused blobs removed: 0"
time=2025-10-23T12:29:06.993-04:00 level=INFO source=routes.go:1564 msg="Listening on 127.0.0.1:11434 (version 0.12.6)"
time=2025-10-23T12:29:06.995-04:00 level=INFO source=runner.go:80 msg="discovering available GPUs..."
time=2025-10-23T12:29:07.831-04:00 level=INFO source=types.go:112 msg="inference compute" id=GPU-b28645fa-aaf6-86a1-ab1b-ef8d61df29f2 library=CUDA compute=12.0 name=CUDA0 description="NVIDIA GeForce RTX 5070" libdirs=ollama,cuda_v12 driver=12.9 pci_id=01:00.0 type=discrete total="11.9 GiB" available="10.5 GiB"
time=2025-10-23T12:29:07.831-04:00 level=INFO source=routes.go:1605 msg="entering low vram mode" "total vram"="11.9 GiB" threshold="20.0 GiB"
